{
  
    
        "post0": {
            "title": "Regression Model",
            "content": "import pandas as pd import numpy as np . df = pd.read_csv(&#39;/kaggle/input/titanic-survival-dataset/train.csv&#39;) . df . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . df.describe() . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 714.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 29.699118 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 14.526497 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 20.125000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 28.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 38.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . df = df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;Ticket&#39;, &#39;PassengerId&#39;]) . df[&#39;Male&#39;] = df[&#39;Sex&#39;] . df[&#39;Male&#39;] = df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) . df[&#39;Embarked_C&#39;] = df[&#39;Embarked&#39;] df[&#39;Embarked_C&#39;] = df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1}) df[&#39;Embarked_S&#39;] = df[&#39;Embarked&#39;] df[&#39;Embarked_S&#39;] = df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0}) df[&#39;Pclass1&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass2&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass3&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass1&#39;] = df[&#39;Pclass1&#39;].replace({2:0, 3:0}) df[&#39;Pclass2&#39;] = df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) df[&#39;Pclass3&#39;] = df[&#39;Pclass3&#39;].replace({1:0, 2:0, 3:1}) . df . Survived Pclass Sex Age SibSp Parch Fare Embarked Male Embarked_C Embarked_S Pclass1 Pclass2 Pclass3 . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | 1 | 0 | 1 | 0 | 0 | 1 | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | 0 | 1 | 0 | 1 | 0 | 0 | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | 0 | 0 | 1 | 0 | 0 | 1 | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | 0 | 0 | 1 | 1 | 0 | 0 | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | 1 | 0 | 1 | 0 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | male | 27.0 | 0 | 0 | 13.0000 | S | 1 | 0 | 1 | 0 | 1 | 0 | . 887 1 | 1 | female | 19.0 | 0 | 0 | 30.0000 | S | 0 | 0 | 1 | 1 | 0 | 0 | . 888 0 | 3 | female | NaN | 1 | 2 | 23.4500 | S | 0 | 0 | 1 | 0 | 0 | 1 | . 889 1 | 1 | male | 26.0 | 0 | 0 | 30.0000 | C | 1 | 1 | 0 | 1 | 0 | 0 | . 890 0 | 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | 1 | Q | Q | 0 | 0 | 1 | . 891 rows × 14 columns . random_parameters = np.random.rand(1,10) . Sibsp, Parch, Age, log_fare, Pclass1, Pclass2, EmbarkS, EmbarkC, Male, Const = [num for num in random_parameters[0]] . Sibsp . 0.6427942275076264 . Parch . 0.15150387891218575 . maxAge, maxFare = max(df[&#39;Age&#39;]), max(df[&#39;Fare&#39;]) . maxAge . 80.0 . df[&#39;Age_N&#39;] = df[&#39;Age&#39;] / maxAge . df[&#39;log_Fare&#39;] = df[&#39;Fare&#39;] / maxFare . df . Survived Pclass Sex Age SibSp Parch Fare Embarked Male Embarked_C Embarked_S Pclass1 Pclass2 Pclass3 Age_N log_Fare . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | 1 | 0 | 1 | 0 | 0 | 1 | 0.2750 | 0.014151 | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | 0 | 1 | 0 | 1 | 0 | 0 | 0.4750 | 0.139136 | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | 0 | 0 | 1 | 0 | 0 | 1 | 0.3250 | 0.015469 | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | 0 | 0 | 1 | 1 | 0 | 0 | 0.4375 | 0.103644 | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | 1 | 0 | 1 | 0 | 0 | 1 | 0.4375 | 0.015713 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | male | 27.0 | 0 | 0 | 13.0000 | S | 1 | 0 | 1 | 0 | 1 | 0 | 0.3375 | 0.025374 | . 887 1 | 1 | female | 19.0 | 0 | 0 | 30.0000 | S | 0 | 0 | 1 | 1 | 0 | 0 | 0.2375 | 0.058556 | . 888 0 | 3 | female | NaN | 1 | 2 | 23.4500 | S | 0 | 0 | 1 | 0 | 0 | 1 | NaN | 0.045771 | . 889 1 | 1 | male | 26.0 | 0 | 0 | 30.0000 | C | 1 | 1 | 0 | 1 | 0 | 0 | 0.3250 | 0.058556 | . 890 0 | 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | 1 | Q | Q | 0 | 0 | 1 | 0.4000 | 0.015127 | . 891 rows × 16 columns . df[&#39;log_Fare&#39;] = np.log10(df[&#39;log_Fare&#39;] + 1) . df[&#39;log_Fare&#39;] = df[&#39;log_Fare&#39;] +1 . df = df.drop(columns=[&#39;log_fare&#39;]) . df[&#39;log_Fare&#39;] = df[&#39;log_Fare&#39;] + 1 . df . Survived Pclass Sex Age SibSp Parch Fare Embarked Male Embarked_C Embarked_S Pclass1 Pclass2 Pclass3 Age_N log_Fare . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | 1 | 0 | 1 | 0 | 0 | 1 | 0.2750 | 0.006103 | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | 0 | 1 | 0 | 1 | 0 | 0 | 0.4750 | 0.056575 | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | 0 | 0 | 1 | 0 | 0 | 1 | 0.3250 | 0.006666 | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | 0 | 0 | 1 | 1 | 0 | 0 | 0.4375 | 0.042829 | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | 1 | 0 | 1 | 0 | 0 | 1 | 0.4375 | 0.006771 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | male | 27.0 | 0 | 0 | 13.0000 | S | 1 | 0 | 1 | 0 | 1 | 0 | 0.3375 | 0.010882 | . 887 1 | 1 | female | 19.0 | 0 | 0 | 30.0000 | S | 0 | 0 | 1 | 1 | 0 | 0 | 0.2375 | 0.024714 | . 888 0 | 3 | female | NaN | 1 | 2 | 23.4500 | S | 0 | 0 | 1 | 0 | 0 | 1 | NaN | 0.019437 | . 889 1 | 1 | male | 26.0 | 0 | 0 | 30.0000 | C | 1 | 1 | 0 | 1 | 0 | 0 | 0.3250 | 0.024714 | . 890 0 | 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | 1 | Q | Q | 0 | 0 | 1 | 0.4000 | 0.006520 | . 891 rows × 16 columns . parameters = { Sibsp, Parch, Age, log_fare, Pclass1, Pclass2, EmbarkS, EmbarkC, Male, Const } . df[&#39;Ones&#39;] = 1 . . Survived Pclass Sex Age SibSp Parch Fare Embarked Embarked_C Embarked_S Pclass1 Pclass2 Pclass3 Age_N log_Fare Ones . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | 0 | 1 | 0 | 0 | 1 | 0.2750 | 1.006103 | 1 | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | 1 | 0 | 1 | 0 | 0 | 0.4750 | 1.056575 | 1 | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | 0 | 1 | 0 | 0 | 1 | 0.3250 | 1.006666 | 1 | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | 0 | 1 | 1 | 0 | 0 | 0.4375 | 1.042829 | 1 | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | 0 | 1 | 0 | 0 | 1 | 0.4375 | 1.006771 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | male | 27.0 | 0 | 0 | 13.0000 | S | 0 | 1 | 0 | 1 | 0 | 0.3375 | 1.010882 | 1 | . 887 1 | 1 | female | 19.0 | 0 | 0 | 30.0000 | S | 0 | 1 | 1 | 0 | 0 | 0.2375 | 1.024714 | 1 | . 888 0 | 3 | female | NaN | 1 | 2 | 23.4500 | S | 0 | 1 | 0 | 0 | 1 | NaN | 1.019437 | 1 | . 889 1 | 1 | male | 26.0 | 0 | 0 | 30.0000 | C | 1 | 0 | 1 | 0 | 0 | 0.3250 | 1.024714 | 1 | . 890 0 | 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | Q | Q | 0 | 0 | 1 | 0.4000 | 1.006520 | 1 | . 891 rows × 16 columns . a = np.array([[1,2,1], [0,1,0], [2,3,4]]) . a . array([[1, 2, 1], [0, 1, 0], [2, 3, 4]]) . b = np.array([[2,5], [6,7], [1,8]]) . b . array([[2, 5], [6, 7], [1, 8]]) . c = np.dot(a,b) . c . array([[15, 27], [ 6, 7], [26, 63]]) . parameters . {0.023121551427445874, 0.13402581877095354, 0.1460823666152794, 0.22088891772092012, 0.594795670221624, 0.7961562768917094, 0.8530100056367039, 0.8613248152275507, 0.8901214376590094, 0.9785782868782011} . parameters = np.array([0.023121551427445874, 0.13402581877095354, 0.1460823666152794, 0.22088891772092012, 0.594795670221624, 0.7961562768917094, 0.8530100056367039, 0.8613248152275507, 0.8901214376590094, 0.9785782868782011]) . parameters . array([0.02312155, 0.13402582, 0.14608237, 0.22088892, 0.59479567, 0.79615628, 0.85301001, 0.86132482, 0.89012144, 0.97857829]) . type(df[&#39;log_Fare&#39;]) . pandas.core.series.Series . test_array = np.array([num for num in df[&#39;log_Fare&#39;]]) . test_array . array([1.00610265, 1.05657548, 1.00666649, 1.04282912, 1.00677082, 1.00711144, 1.04187758, 1.01750732, 1.00933647, 1.02477057, 1.01393053, 1.02194231, 1.00677082, 1.02573369, 1.00660738, 1.01335551, 1.02401262, 1.01088243, 1.01499643, 1.00608175, 1.02149883, 1.01088243, 1.00675346, 1.02909607, 1.01750732, 1.02582356, 1.00608175, 1.17993703, 1.00662825, 1.00664211, 1.02288486, 1.10923744, 1.00652038, 1.00881073, 1.06460275, 1.04198341, 1.00608526, 1.00677082, 1.01499643, 1.00942639, 1.00795846, 1.01744625, 1.00664211, 1.03388884, 1.00662825, 1.00677082, 1.0129443 , 1.00652038, 1.01799898, 1.01483262, 1.03240311, 1.00656213, 1.06060924, 1.02149883, 1.04959606, 1.02909607, 1.00881073, 1.00608526, 1.02290834, 1.03804073, 1.00608526, 1.06301403, 1.06555445, 1.02302895, 1.02288486, 1.0127351 , 1.00881073, 1.00686119, 1.00666649, 1.0072817 , 1.00881073, 1.03804073, 1.05822191, 1.01208297, 1.04542957, 1.00643686, 1.00664211, 1.00677082, 1.02391235, 1.01044819, 1.00756294, 1.00797927, 1.00655169, 1.03819602, 1.00881073, 1.01323219, 1.0282033 , 1.00677082, 1.17993703, 1.00677082, 1.00677082, 1.00660738, 1.04898749, 1.01710003, 1.00610265, 1.00677082, 1.02842504, 1.05063769, 1.01907182, 1.02149883, 1.00664211, 1.00664211, 1.06102067, 1.00727478, 1.00666649, 1.00664211, 1.00643686, 1.00654125, 1.00664211, 1.02000378, 1.04198341, 1.01208297, 1.00677082, 1.00824967, 1.01208635, 1.00666649, 1.00652038, 1.01744625, 1.17117876, 1.02573369, 1.05822191, 1.00677082, 1.02477057, 1.01088243, 1.06102067, 1.00942639, 1.00652038, 1.00601212, 1.01855092, 1.00587273, 1.00664211, 1.00593544, 1.01212073, 1.02149883, 1.01088243, 1.01257043, 1.02172732, 1.04282912, 1.00774343, 1.06242708, 1.0127351 , 1.00652038, 1.01323219, 1.00568452, 1.00964059, 1.03008588, 1.00655862, 1.0282033 , 1.02149883, 1.01088243, 1.01048957, 1.05307635, 1.00677082, 1.01212073, 1.00615489, 1.0491421 , 1.00650643, 1.00677082, 1.0072817 , 1.05528372, 1.0134377 , 1.01314995, 1.00654125, 1.0072817 , 1.03240311, 1.01705928, 1.04428603, 1.02302895, 1.02143832, 1.04542957, 1.02750766, 1.02401262, 1.00933647, 1.00666649, 1.02527072, 1.00660738, 1.02106838, 1.02368163, 1.01088243, 1. , 1.05528372, 1.01257389, 1.02582356, 1.03186189, 1.01828012, 1.04044153, 1.0129443 , 1.02194231, 1.0129443 , 1.00664211, 1.01088243, 1.01088243, 1.00660738, 1.02149883, 1.02288486, 1.10923744, 1.00652038, 1.00706633, 1.00652038, 1.01088243, 1.00797927, 1.05528372, 1.00547179, 1.00608175, 1.00677082, 1.00877957, 1.01323219, 1.01564085, 1.00652038, 1.02551394, 1.00593544, 1.01744625, 1.00610265, 1.01088243, 1.00652038, 1.08675055, 1.00666649, 1.02230483, 1.06028657, 1.00881073, 1.00677082, 1.01088243, 1.00677082, 1.00664211, 1.07028481, 1.00785441, 1.00881073, 1.00610265, 1.01088243, 1.02106838, 1.06555445, 1.00654125, 1.01129559, 1.02582356, 1.00881073, 1.00635333, 1.02149883, 1.02170047, 1.00881073, 1.01028265, 1.01208297, 1.0129443 , 1.00881073, 1.00599815, 1.00608175, 1.07028481, 1.00654125, 1.01212073, 1.0424097 , 1.02149883, 1.00610265, 1.00877957, 1.02194231, 1.0134377 , 1.01680451, 1.0127351 , 1.06242708, 1.06775386, 1.30103 , 1.02149883, 1.00652038, 1.02582356, 1.06275734, 1. , 1.00652038, 1.00881073, 1.03240311, 1.00654125, 1.11378927, 1.10200076, 1.02551394, 1. , 1.01622307, 1.02447357, 1.00652038, 1.06151448, 1.00652038, 1. , 1.02401262, 1.01683509, 1.00652038, 1.00660738, 1.00797927, 1.00677082, 1.02149883, 1.0072817 , 1.00797927, 1.00664211, 1.01088243, 1.00652038, 1.06217004, 1.07106224, 1.01077908, 1.00743797, 1.00664211, 1.02288486, 1.00608526, 1.11253995, 1.02511409, 1.17117876, 1.00652038, 1.01927459, 1. , 1.01034474, 1.00677082, 1.11253995, 1.08508705, 1.08370275, 1.01988233, 1.04576034, 1.06532354, 1.1795868 , 1.02149883, 1.00664211, 1.02170047, 1.00660738, 1.02149883, 1.01170836, 1.12116521, 1.10124051, 1.00610265, 1.00664211, 1.01034474, 1.02391235, 1.05528372, 1.10200076, 1.00525552, 1.01088243, 1.01705928, 1.04656066, 1.01927459, 1.02351102, 1.11378927, 1.01499643, 1.10066943, 1.00664211, 1.05307635, 1.10124051, 1.00677082, 1.02909607, 1.02149883, 1.17993703, 1.01088243, 1.01088243, 1.01088243, 1.01088243, 1.01088243, 1.0134377 , 1.0132733 , 1.0072817 , 1.00775034, 1.02869951, 1.00608526, 1.01483262, 1.00608175, 1.00797927, 1.04428603, 1.01088243, 1.00662825, 1.00662825, 1.02302895, 1.02288486, 1.01208297, 1.00593544, 1.0129443 , 1.00610265, 1.0595173 , 1.00608526, 1.00652038, 1.05509709, 1.04462402, 1.00547179, 1.00677082, 1.10200076, 1.01750732, 1.06460275, 1.00610265, 1.15008699, 1.00338809, 1.00654125, 1.15959703, 1.01314313, 1.00666649, 1.04198341, 1.00664211, 1.05822191, 1.03804073, 1.01088243, 1.00650301, 1.01005494, 1.09139413, 1.00655862, 1.00666649, 1.08675055, 1.01393053, 1.00655862, 1.00660738, 1.02149883, 1.00881073, 1.01059299, 1.00666649, 1.00677082, 1.00824967, 1.01323219, 1.0072817 , 1.01744625, 1.00652038, 1.01561018, 1.00654125, 1.02106838, 1.00664211, 1.00577512, 1.07028481, 1. , 1.00666649, 1.00677082, 1.02671127, 1.01088243, 1.01088243, 1.02000378, 1.00664211, 1.00650643, 1.00662475, 1.01203829, 1.01680451, 1.00610265, 1.02149883, 1.02149883, 1.00652038, 1.00677082, 1.02194231, 1.0134377 , 1.02149883, 1.00599815, 1.04497444, 1.09139413, 1.0282033 , 1.01561018, 1.17993703, 1.00881073, 1.02170047, 1.00797927, 1.00654125, 1.01088243, 1.00682298, 1.0643744 , 1.01622307, 1.02194231, 1.01602565, 1.02511409, 1.02290834, 1.01660401, 1.02290834, 1.06963843, 1.00677082, 1.00664211, 1.02194231, 1.04187758, 1.00881073, 1.00652038, 1.02194231, 1.00677082, 1.03146785, 1.01088243, 1.00677082, 1.00593544, 1. , 1.02194231, 1.0064995 , 1.01602565, 1.00610265, 1.0072817 , 1.02290834, 1.01153645, 1.00826006, 1.04198341, 1.01744625, 1.00593193, 1.00632894, 1.010293 , 1.03804073, 1. , 1.00677082, 1.00805208, 1.07106224, 1.02106838, 1.07028481, 1.02447357, 1.00677082, 1.0132733 , 1.01660401, 1.00610265, 1.02511409, 1.04005844, 1.00677082, 1.01208635, 1.06174132, 1.01261506, 1.11253995, 1.00655862, 1.0072817 , 1.00652038, 1.00641949, 1.00805208, 1.06775386, 1.08370275, 1.02149883, 1.02194231, 1.0186863 , 1.04542957, 1.00652038, 1.00677082, 1.02173071, 1.04764126, 1.00630805, 1.02792184, 1.00881073, 1.02000378, 1.02149883, 1.00664211, 1.07280109, 1.00664211, 1.00608175, 1.04656066, 1.00608526, 1.00652038, 1.00881073, 1.15621108, 1.00666649, 1.00964059, 1.02149883, 1.00608526, 1.00608526, 1.01855092, 1.0072817 , 1.02170047, 1.02194231, 1.08196905, 1.01212073, 1.0400552 , 1.05636461, 1.02573369, 1.02573369, 1.02149883, 1.08196905, 1.02149883, 1.02149883, 1.01159488, 1.01705928, 1.03008588, 1.08508705, 1.02149883, 1.00658651, 1.00608175, 1.00654125, 1.02194231, 1.03233426, 1.15959703, 1.06275734, 1.0145048 , 1.00652038, 1.00664211, 1.01129559, 1.00677082, 1.00677082, 1.02000378, 1.00664211, 1.01750732, 1.00608526, 1.00660738, 1.00881073, 1.04158243, 1.02181133, 1.00652038, 1.00677082, 1.01212073, 1.01088243, 1.04497444, 1.01208635, 1.00666649, 1.02471388, 1.08508705, 1.02149883, 1.03274717, 1.00732337, 1.06275734, 1.01253271, 1.06242708, 1.00677082, 1.00677082, 1.00599815, 1.06174132, 1.00610265, 1.00652038, 1.02149883, 1.02000378, 1.02710964, 1. , 1.00608175, 1.04576034, 1.02230483, 1.00664211, 1.03453192, 1.00677082, 1.02194231, 1.01298544, 1.00664211, 1.02511409, 1.03388884, 1.11378927, 1.02573369, 1.00593544, 1.0129443 , 1.00652038, 1.00677082, 1.05187441, 1.01203829, 1.0134377 , 1.03186189, 1.00881073, 1.01208297, 1.0424097 , 1.01314313, 1.00660738, 1.0134377 , 1.0265684 , 1.01034474, 1.06151448, 1.00664211, 1.00650643, 1.02471388, 1.00593896, 1.02511409, 1. , 1.02302895, 1.01088243, 1.00666649, 1.02170047, 1.03240311, 1.0134377 , 1.00660738, 1.05509709, 1.02302895, 1.04542957, 1.01602565, 1.06060924, 1.00664211, 1.02909607, 1.00635333, 1.00635333, 1.00664211, 1.01907182, 1.00709059, 1.00658651, 1.00568452, 1.05822191, 1.00664211, 1.0129443 , 1.01088243, 1.08675055, 1.10066943, 1.00608175, 1.02116592, 1.00630805, 1.00666649, 1.05822191, 1.01088243, 1.00654125, 1.00677082, 1.04198341, 1.03186189, 1.04198341, 1.00881073, 1.01088243, 1. , 1.00654125, 1.00677082, 1.00826356, 1.03804073, 1.30103 , 1.00684384, 1.06060924, 1.00775034, 1.03804073, 1.03186189, 1.03388884, 1.03240311, 1.00853719, 1.00655862, 1.14998948, 1.04581435, 1.01122679, 1.04542957, 1.00608175, 1.02194231, 1.01129559, 1.00677082, 1.00650643, 1.08508705, 1.00643686, 1.15959703, 1.02173071, 1.01208297, 1.00651344, 1.00660738, 1.02149883, 1.01129559, 1.02173071, 1.11253995, 1.0127351 , 1.04005844, 1.02194231, 1.04198341, 1.00796537, 1.01088243, 1.00643686, 1.15959703, 1.00881073, 1.0129443 , 1.00654125, 1.02710964, 1.00593896, 1.01088243, 1.01088243, 1.04282912, 1.0072817 , 1.01744625, 1.00650994, 1.02149883, 1.00666649, 1.14998948, 1.01564085, 1. , 1.01088243, 1.01088243, 1.0134377 , 1.0282033 , 1.30103 , 1.00664211, 1.00664211, 1.02471388, 1.06217004, 1.1795868 , 1.0134377 , 1.00666649, 1.05636461, 1.01683509, 1.01088243, 1.04282912, 1.00652038, 1.01907182, 1.01044819, 1.00797927, 1.00664211, 1.05187441, 1.01212073, 1.00655862, 1.00964059, 1.00677082, 1.06775386, 1.01212073, 1.00599815, 1.00608526, 1.09139413, 1.00654125, 1.06151448, 1.03233426, 1.00652038, 1.02000378, 1.00703155, 1.00797927, 1.00660738, 1.00881073, 1.00608175, 1.01907182, 1.00652038, 1.00652038, 1.01044819, 1.00650994, 1.14998948, 1.00608526, 1.04581435, 1.02471388, 1.01943674, 1.00593544, 1.00610265, 1.00630805, 1.02401262, 1.01710003, 1.06242708, 1.00652038, 1.02149883, 1.05528372, 1.02527072, 1.00664211, 1.01088243, 1.02144171, 1.00729903, 1.00608526, 1.02000378, 1.01088243, 1.02170047, 1.09139413, 1.00716014, 1.00587273, 1.00654125, 1. , 1.00654125, 1.01088243, 1.04282912, 1.00663518, 1.02000378, 1.00881073, 1.02573369, 1.00677082, 1. , 1.00666649, 1.0302869 , 1.00543345, 1.02302895, 1.07280109, 1.0072817 , 1. , 1.01044819, 1.03240311, 1.00585182, 1.04542957, 1.0302869 , 1.00652038, 1.06301403, 1.01208297, 1.01561018, 1.00608526, 1.00660738, 1.00697941, 1.06532354, 1.0072817 , 1.00677082, 1.04542957, 1.02447357, 1.00666649, 1.00881073, 1.02551394, 1.00542298, 1.0072817 , 1.00635333, 1.05528372, 1.00664211, 1.02710964, 1.06963843, 1.02573369, 1.00654125, 1.0127351 , 1.03217686, 1.02149883, 1.00785441, 1.12116521, 1.02194231, 1.01602565, 1.00608526, 1.01179771, 1.00964059, 1.02144171, 1.05528372, 1.01088243, 1.01088243, 1.01159142, 1.04082427, 1.00797927, 1.00933647, 1.00664211, 1.0424097 , 1.00421788, 1.00756294, 1.01988233, 1.00608175, 1.00826697, 1.00664211, 1.00664211, 1.06532354, 1.02149883, 1.00664211, 1.0088246 , 1.00881073, 1.00593544, 1.02401262, 1.01088243, 1.02471388, 1.01943674, 1.02471388, 1.00652038]) . . model_df = df . model_df = model_df.drop(columns=[&#39;Survived&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]) . model_df = model_df.drop(columns=[&#39;Pclass&#39;]) . model_df[&#39;Age_N&#39;] = model_df[&#39;Age_N&#39;].fillna(0) . model_df.isnull().values.any() . True . model_df . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Pclass3 Age_N log_Fare Ones . 0 1 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 0.2750 | 0.006103 | 1 | . 1 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0.4750 | 0.056575 | 1 | . 2 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0.3250 | 0.006666 | 1 | . 3 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0.4375 | 0.042829 | 1 | . 4 0 | 0 | 1 | 0 | 1 | 0 | 0 | 1 | 0.4375 | 0.006771 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0 | 0.3375 | 0.010882 | 1 | . 887 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0.2375 | 0.024714 | 1 | . 888 1 | 2 | 0 | 0 | 1 | 0 | 0 | 1 | 0.0000 | 0.019437 | 1 | . 889 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0 | 0.3250 | 0.024714 | 1 | . 890 0 | 0 | 1 | Q | Q | 0 | 0 | 1 | 0.4000 | 0.006520 | 1 | . 891 rows × 11 columns . model_df = model_df.drop(columns=[&#39;Pclass3&#39;]) . model_df . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones . 0 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0.2750 | 0.006103 | 1 | . 1 1 | 0 | 0 | 1 | 0 | 1 | 0 | 0.4750 | 0.056575 | 1 | . 2 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0.3250 | 0.006666 | 1 | . 3 1 | 0 | 0 | 0 | 1 | 1 | 0 | 0.4375 | 0.042829 | 1 | . 4 0 | 0 | 1 | 0 | 1 | 0 | 0 | 0.4375 | 0.006771 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0 | 1 | 0 | 1 | 0.3375 | 0.010882 | 1 | . 887 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0.2375 | 0.024714 | 1 | . 888 1 | 2 | 0 | 0 | 1 | 0 | 0 | 0.0000 | 0.019437 | 1 | . 889 0 | 0 | 1 | 1 | 0 | 1 | 0 | 0.3250 | 0.024714 | 1 | . 890 0 | 0 | 1 | Q | Q | 0 | 0 | 0.4000 | 0.006520 | 1 | . 891 rows × 10 columns . model_df[&#39;Linear&#39;] = model_df.dot(parameters) . model_df[&#39;Embarked_S&#39;] = model_df[&#39;Embarked_S&#39;].replace({&#39;Q&#39;: 0}) model_df[&#39;Embarked_C&#39;] = model_df[&#39;Embarked_C&#39;].replace({&#39;Q&#39;: 0}) . model_df . SibSp Parch Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Male . 0 1 | 0 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 1.006103 | 1 | 1 | . 1 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 1.056575 | 1 | 0 | . 2 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 1.006666 | 1 | 0 | . 3 1 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 1.042829 | 1 | 0 | . 4 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 1.006771 | 1 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 1.010882 | 1 | 1 | . 887 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 1.024714 | 1 | 0 | . 888 1 | 2 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 1.019437 | 1 | 0 | . 889 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 1.024714 | 1 | 1 | . 890 0 | 0 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 1.006520 | 1 | 1 | . 891 rows × 10 columns . model_df . SibSp Parch Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Male Linear . 0 1 | 0 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 1.006103 | 1 | 1 | 3.213869 | . 1 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 1.056575 | 1 | 0 | 2.969355 | . 2 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 1.006666 | 1 | 0 | 2.255305 | . 3 1 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 1.042829 | 1 | 0 | 3.000334 | . 4 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 1.006771 | 1 | 1 | 3.329937 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 1.010882 | 1 | 1 | 4.044334 | . 887 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 1.024714 | 1 | 0 | 2.791007 | . 888 1 | 2 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 1.019437 | 1 | 0 | 2.280250 | . 889 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 1.024714 | 1 | 1 | 3.769418 | . 890 0 | 0 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 1.006520 | 1 | 1 | 3.076845 | . 891 rows × 11 columns . df . Survived Pclass Sex Age SibSp Parch Fare Embarked Embarked_C Embarked_S Pclass1 Pclass2 Pclass3 Age_N log_Fare Ones Male . 0 0 | 3 | male | 22.0 | 1 | 0 | 7.2500 | S | 0 | 1 | 0 | 0 | 1 | 0.2750 | 1.006103 | 1 | 1 | . 1 1 | 1 | female | 38.0 | 1 | 0 | 71.2833 | C | 1 | 0 | 1 | 0 | 0 | 0.4750 | 1.056575 | 1 | 0 | . 2 1 | 3 | female | 26.0 | 0 | 0 | 7.9250 | S | 0 | 1 | 0 | 0 | 1 | 0.3250 | 1.006666 | 1 | 0 | . 3 1 | 1 | female | 35.0 | 1 | 0 | 53.1000 | S | 0 | 1 | 1 | 0 | 0 | 0.4375 | 1.042829 | 1 | 0 | . 4 0 | 3 | male | 35.0 | 0 | 0 | 8.0500 | S | 0 | 1 | 0 | 0 | 1 | 0.4375 | 1.006771 | 1 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 2 | male | 27.0 | 0 | 0 | 13.0000 | S | 0 | 1 | 0 | 1 | 0 | 0.3375 | 1.010882 | 1 | 1 | . 887 1 | 1 | female | 19.0 | 0 | 0 | 30.0000 | S | 0 | 1 | 1 | 0 | 0 | 0.2375 | 1.024714 | 1 | 0 | . 888 0 | 3 | female | NaN | 1 | 2 | 23.4500 | S | 0 | 1 | 0 | 0 | 1 | NaN | 1.019437 | 1 | 0 | . 889 1 | 1 | male | 26.0 | 0 | 0 | 30.0000 | C | 1 | 0 | 1 | 0 | 0 | 0.3250 | 1.024714 | 1 | 1 | . 890 0 | 3 | male | 32.0 | 0 | 0 | 7.7500 | Q | Q | Q | 0 | 0 | 1 | 0.4000 | 1.006520 | 1 | 1 | . 891 rows × 17 columns . model_df[&#39;Survived&#39;] = df[&#39;Survived&#39;] . model_df . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 0.006103 | 1 | 1.984874 | 0 | . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 0.006771 | 1 | 2.102313 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 0.010882 | 1 | 2.872850 | 0 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 0.019437 | 1 | 1.881848 | 0 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 0.006520 | 1 | 1.474995 | 0 | . 891 rows × 12 columns . model_df[&#39;Loss&#39;] = (model_df[&#39;Linear&#39;] - 1)**2 . model_df . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 0.006103 | 1 | 1.984874 | 0 | 0.969977 | . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 0.006771 | 1 | 2.102313 | 0 | 1.215093 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 0.010882 | 1 | 2.872850 | 0 | 3.507568 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 0.019437 | 1 | 1.881848 | 0 | 0.777656 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 0.006520 | 1 | 1.474995 | 0 | 0.225620 | . 891 rows × 13 columns . model_df[&#39;Linear&#39;].mean() . 2.255390616421957 . survived_label = model_df[&#39;Survived&#39;] == 1 . survived_label . 0 False 1 True 2 True 3 True 4 False ... 886 False 887 True 888 False 889 True 890 False Name: Survived, Length: 891, dtype: bool . death_label = model_df[&#39;Survived&#39;] == 0 . survived = model_df.loc[survived_label] . death = model_df.loc[death_label] . import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . death . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 0.006103 | 1 | 1.984874 | 0 | 0.969977 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 0.006771 | 1 | 2.102313 | 0 | 1.215093 | . 5 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.0000 | 0.007111 | 1 | 1.130991 | 0 | 0.017159 | . 6 0 | 0 | 1 | 0.0 | 1.0 | 1 | 0 | 0.6750 | 0.041878 | 1 | 3.134283 | 0 | 4.555164 | . 7 3 | 1 | 1 | 0.0 | 1.0 | 0 | 0 | 0.0250 | 0.017507 | 1 | 1.959964 | 0 | 0.921530 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 884 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.3125 | 0.005935 | 1 | 1.993904 | 0 | 0.987844 | . 885 0 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0.4875 | 0.024013 | 1 | 2.089977 | 0 | 1.188051 | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 0.010882 | 1 | 2.872850 | 0 | 3.507568 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 0.019437 | 1 | 1.881848 | 0 | 0.777656 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 0.006520 | 1 | 1.474995 | 0 | 0.225620 | . 549 rows × 13 columns . survived . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 8 0 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3375 | 0.009336 | 1 | 2.140433 | 1 | 1.300588 | . 9 1 | 0 | 0 | 1.0 | 0.0 | 0 | 1 | 0.1750 | 0.024771 | 1 | 2.248379 | 1 | 1.558451 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 875 0 | 0 | 0 | 1.0 | 0.0 | 0 | 0 | 0.1875 | 0.006082 | 1 | 1.366379 | 1 | 0.134234 | . 879 0 | 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.7000 | 0.065324 | 1 | 2.790723 | 1 | 3.206687 | . 880 0 | 1 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3125 | 0.021499 | 1 | 2.848710 | 1 | 3.417730 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 342 rows × 13 columns . survived_tensor = tensor(survived) death_tensor = tensor(death) . survived_tensor . tensor([[1.0000, 0.0000, 0.0000, ..., 2.4782, 1.0000, 2.1852], [0.0000, 0.0000, 0.0000, ..., 1.8592, 1.0000, 0.7383], [1.0000, 0.0000, 0.0000, ..., 2.8076, 1.0000, 3.2674], ..., [0.0000, 1.0000, 0.0000, ..., 2.8487, 1.0000, 3.4177], [0.0000, 0.0000, 0.0000, ..., 2.5961, 1.0000, 2.5475], [0.0000, 0.0000, 1.0000, ..., 2.4436, 1.0000, 2.0841]]) . len(survived_tensor), len(death_tensor) . (342, 549) . stacked_survived = torch.stack(survived_tensor).float() stacked_death = torch.stack(death_tensor).float() . TypeError Traceback (most recent call last) /tmp/ipykernel_17/772444025.py in &lt;module&gt; -&gt; 1 stacked_survived = torch.stack(survived_tensor).float() 2 stacked_death = torch.stack(death_tensor).float() TypeError: stack(): argument &#39;tensors&#39; (position 1) must be tuple of Tensors, not Tensor . num = range(len(survived)) stacked_survived = [tensor(survived.iloc[num]) for num in num] . num = range(len(survived)) . num . range(0, 342) . copydf = model_df . copydf . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 0.006103 | 1 | 1.984874 | 0 | 0.969977 | . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 0.006771 | 1 | 2.102313 | 0 | 1.215093 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 0.010882 | 1 | 2.872850 | 0 | 3.507568 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 0.019437 | 1 | 1.881848 | 0 | 0.777656 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 0.006520 | 1 | 1.474995 | 0 | 0.225620 | . 891 rows × 13 columns . print(&#39;test&#39;) . test . copydf = copydf.drop(columns=[&#39;Age_N&#39;, &#39;log_Fare&#39;, &#39;Ones&#39;, &#39;Linear&#39;, &#39;Loss&#39;]) . copydf . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Survived . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 1 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 1 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 1 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 1 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 1 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | . 891 rows × 8 columns . survived_label = copydf[&#39;Survived&#39;] == 1 death_label = copydf[&#39;Survived&#39;] == 0 survived = copydf.loc[survived_label] death = copydf.loc[death_label] num = range(len(survived)) nums = range(len(death)) stacked_survived = [tensor(survived.iloc[num]) for num in num] stacked_death = [tensor(death.iloc[nums]) for nums in nums] . survive_tensors_stacked = torch.stack(stacked_survived).float() death_tensors_stacked = torch.stack(stacked_death).float() # mean_survived = stacked_death.mean(0) # mean_death = stacked_survived.mean(0) . survive_tensors_stacked.shape, death_tensors_stacked.shape . (torch.Size([342, 8]), torch.Size([549, 8])) . mean_survived = survive_tensors_stacked.mean(0) mean_death = death_tensors_stacked.mean(0) . mean_survived . tensor([0.4737, 0.4649, 0.3187, nan, nan, 0.3977, 0.2544, 1.0000]) . single_survivor = survive_tensors_stacked[1] single_death = death_tensors_stacked[1] . import torch.nn.functional as F . F.l1_loss(single_survivor.float(),mean_death), F.mse_loss(single_survivor,mean_death).sqrt() . (tensor(0.4271), tensor(0.5318)) . F.l1_loss(single_survivor.float(),mean_survived), F.mse_loss(single_survivor,mean_survived).sqrt() # need RElu given that data contains many 0? -- &gt; nan output . (tensor(nan), tensor(nan)) . def survive_distance(a,b): return (a-b).abs().mean((-1,-2)) distance_all_survived = survive_distance(survive_tensors_stacked, mean_survived) . distance_all_survived . tensor(nan) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/16/regression-model.html",
            "relUrl": "/fastpages/jupyter/2022/09/16/regression-model.html",
            "date": " • Sep 16, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Neural Network Digit Classifier",
            "content": "Neural network image classifier using fast.ai and Pytorch modules . Install required modules and get the training and validation data from MNIST. | pip install fastbook . Collecting fastbook Downloading fastbook-0.0.28-py3-none-any.whl (719 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 719.8/719.8 kB 927.7 kB/s eta 0:00:00a 0:00:01 Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastbook) (22.1.2) Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastbook) (21.3) Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.1.0) Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from fastbook) (4.20.1) Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.8.4) Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastbook) (1.3.5) Requirement already satisfied: fastai&gt;=2.6 in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.7.9) Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.1.97) Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.28.1) Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.7.3) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.5.3) Requirement already satisfied: spacy&lt;4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.3.1) Requirement already satisfied: torch&lt;1.14,&gt;=1.7 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.11.0) Requirement already satisfied: fastcore&lt;1.6,&gt;=1.4.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.21) Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.2) Requirement already satisfied: torchvision&gt;=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.12.0) Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (6.0) Requirement already satisfied: pillow&gt;6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (9.1.1) Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7) Requirement already satisfied: fastprogress&gt;=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: numpy&gt;=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (1.21.6) Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (2022.7.1) Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.0.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.64.0) Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.8.1) Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.70.13) Requirement already satisfied: responses&lt;0.19 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.18.0) Requirement already satisfied: pyarrow&gt;=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (5.0.0) Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.8.1) Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.12.0) Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.3.5.1) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2.1.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2022.6.15) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (1.26.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (3.3) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging-&gt;fastbook) (3.0.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2022.1) Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (2021.11.10) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (0.12.1) Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (3.7.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets-&gt;fastbook) (4.3.0) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;fastbook) (1.15.0) Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.10) Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (59.8.0) Requirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.4.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.6) Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.8) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.17) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.8.2) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.10.1) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.8) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.4) Requirement already satisfied: pathy&gt;=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.2) Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0) Collecting typing-extensions&gt;=3.7.4.3 Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.7) Requirement already satisfied: frozenlist&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2) Requirement already satisfied: aiosignal&gt;=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.2.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.7.2) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (21.4.0) Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (0.13.0) Requirement already satisfied: zipp&gt;=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata-&gt;datasets-&gt;fastbook) (3.8.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.3) Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.33.3) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.1) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (5.2.1) Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.4) Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.1) Installing collected packages: typing-extensions, fastbook Attempting uninstall: typing-extensions Found existing installation: typing_extensions 4.3.0 Uninstalling typing_extensions-4.3.0: Successfully uninstalled typing_extensions-4.3.0 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed. tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible. tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible. tensorflow 2.6.4 requires tensorboard&lt;2.7,&gt;=2.6.0, but you have tensorboard 2.10.0 which is incompatible. tensorflow 2.6.4 requires typing-extensions&lt;3.11,&gt;=3.7, but you have typing-extensions 4.1.1 which is incompatible. tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.6.4 which is incompatible. tensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.6.4 which is incompatible. pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible. flax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible. flake8 4.0.1 requires importlib-metadata&lt;4.3; python_version &lt; &#34;3.8&#34;, but you have importlib-metadata 4.12.0 which is incompatible. apache-beam 2.40.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.5.1 which is incompatible. allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible. aiobotocore 2.3.4 requires botocore&lt;1.24.22,&gt;=1.24.21, but you have botocore 1.27.56 which is incompatible. Successfully installed fastbook-0.0.28 typing-extensions-4.1.1 WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv Note: you may need to restart the kernel to use updated packages. . . import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . . 100.14% [3219456/3214948 00:01&lt;00:00] path.ls() . (#3) [Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;),Path(&#39;train&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . im3_path = threes[1] im3 = Image.open(im3_path) im3 . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . . im3_t = tensor(im3) df = pd.DataFrame(im3_t[:]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 51 | 119 | 253 | 253 | 253 | 76 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 183 | 253 | 253 | 139 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 182 | 253 | 253 | 104 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 249 | 253 | 253 | 36 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 214 | 253 | 253 | 173 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 247 | 253 | 253 | 226 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 150 | 252 | 253 | 253 | 233 | 53 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 42 | 115 | 42 | 60 | 115 | 159 | 240 | 253 | 253 | 250 | 175 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 187 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 197 | 86 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 103 | 253 | 253 | 253 | 253 | 253 | 232 | 67 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . # Check one of the images created show_image(three_tensors[1]); . . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean(0) show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . # Check a random image and see how far its distance is from the ideal three a_3 = stacked_threes[1] show_image(a_3); . . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . Compute Metrics using Broadcasting . Start by getting validation labels from the MNIST dataset for both digits . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1270, 0.1632, 0.1676, ..., 0.1228, 0.1210, 0.1287]), torch.Size([1010])) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . is_3(valid_3_tens) . tensor([ True, False, False, ..., True, True, False]) . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Use Stochastic Gradient Descent to optimize our prediction model . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until we stop the training process | def f(x): return x**2 . xt = tensor(3.).requires_grad_() . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . yt.backward() . xt.grad . tensor(6.) . xt = tensor([3.,4.,10.]).requires_grad_() xt # Add sum to the quadratic function so it can take a vector (rank-1 tensor) and return a scalar (rank-0 tensor) def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad . tensor([ 6., 8., 20.]) . # w -= gradient(w) * lr . def mse(preds, targets): return ((preds-targets)**2).mean() def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . # start by by concatenating all of our images (independant x variable) into a single tensor and change them from a list of matrices (rank-3 tensor) to a list of vectors (a rank-2 tensor) -- using Pytorch&#39;s # view method. train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . bias = init_params(1) . (train_x[0]*weights.T).sum() + bias . tensor([-6.2330], grad_fn=&lt;AddBackward0&gt;) . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ -6.2330], [-10.6388], [-20.8865], ..., [-15.9176], [ -1.6866], [-11.3568]], grad_fn=&lt;AddBackward0&gt;) . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [ True], [ True]]) . corrects.float().mean().item() . 0.5379961133003235 . with torch.no_grad(): weights[0] *= 1.0001 . preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.5379961133003235 . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . # Re-initialize parameters weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . preds = linear1(batch) preds . tensor([[14.0882], [13.9915], [16.0442], [17.7304]], grad_fn=&lt;AddBackward0&gt;) . loss = mnist_loss(preds, train_y[:4]) loss . tensor(4.1723e-07, grad_fn=&lt;MeanBackward0&gt;) . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-5.9512e-08), tensor([-4.1723e-07])) . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-1.1902e-07), tensor([-8.3446e-07])) . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-1.7854e-07), tensor([-1.2517e-06])) . weights.grad.zero_() bias.grad.zero_() . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . (preds&gt;0.0).float() == train_y[:4] . tensor([[True], [True], [True], [True]]) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(batch), train_y[:4]) . tensor(1.) . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5748 . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.7251 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8569 0.9096 0.9296 0.9399 0.9467 0.9545 0.9569 0.9628 0.9647 0.9662 0.9672 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.975 . Creating an optimizer . replace linear1 function with Pytorch&#39;s nn.linear module reminder : nn.linear accomplishes the same thing as init_params and linear together - it contains both the weights and biases in a single class | linear_model = nn.Linear(28*28,1) . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.6381 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.7724 0.8559 0.916 0.935 0.9472 0.9579 0.9628 0.9658 0.9677 0.9697 0.9716 0.9741 0.975 0.976 0.9765 0.9775 0.978 0.978 0.978 . # fast ai SGD class is the same as our BasicOptim class, therefore: linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.831 0.8398 0.9116 0.934 0.9477 0.956 0.9623 0.9658 0.9667 0.9697 0.9726 0.9741 0.975 0.9755 0.9765 0.9775 0.9785 0.9785 0.9785 . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637040 | 0.503638 | 0.495584 | 00:00 | . 1 | 0.596475 | 0.159199 | 0.878312 | 00:00 | . 2 | 0.216541 | 0.197214 | 0.819431 | 00:00 | . 3 | 0.093282 | 0.111199 | 0.908243 | 00:00 | . 4 | 0.047910 | 0.080145 | 0.931305 | 00:00 | . 5 | 0.030276 | 0.063814 | 0.946025 | 00:00 | . 6 | 0.023095 | 0.053701 | 0.955348 | 00:00 | . 7 | 0.019960 | 0.046993 | 0.961727 | 00:00 | . 8 | 0.018413 | 0.042308 | 0.965162 | 00:00 | . 9 | 0.017511 | 0.038881 | 0.967125 | 00:00 | . Adding Nonlinearity . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.385122 | 0.388649 | 0.520118 | 00:00 | . 1 | 0.170687 | 0.256767 | 0.771835 | 00:00 | . 2 | 0.090929 | 0.123868 | 0.908734 | 00:00 | . 3 | 0.057405 | 0.081251 | 0.938665 | 00:00 | . 4 | 0.042229 | 0.062670 | 0.952895 | 00:00 | . 5 | 0.034708 | 0.052418 | 0.963690 | 00:00 | . 6 | 0.030526 | 0.046020 | 0.965653 | 00:00 | . 7 | 0.027888 | 0.041687 | 0.966634 | 00:00 | . 8 | 0.026028 | 0.038563 | 0.968106 | 00:00 | . 9 | 0.024608 | 0.036192 | 0.968597 | 00:00 | . 10 | 0.023467 | 0.034323 | 0.971050 | 00:00 | . 11 | 0.022520 | 0.032800 | 0.973013 | 00:00 | . 12 | 0.021717 | 0.031524 | 0.973503 | 00:00 | . 13 | 0.021023 | 0.030433 | 0.974975 | 00:00 | . 14 | 0.020417 | 0.029485 | 0.974975 | 00:00 | . 15 | 0.019881 | 0.028649 | 0.975466 | 00:00 | . 16 | 0.019402 | 0.027905 | 0.975957 | 00:00 | . 17 | 0.018971 | 0.027236 | 0.976938 | 00:00 | . 18 | 0.018580 | 0.026633 | 0.977429 | 00:00 | . 19 | 0.018223 | 0.026085 | 0.978410 | 00:00 | . 20 | 0.017895 | 0.025584 | 0.978410 | 00:00 | . 21 | 0.017592 | 0.025126 | 0.978901 | 00:00 | . 22 | 0.017310 | 0.024705 | 0.978901 | 00:00 | . 23 | 0.017048 | 0.024316 | 0.979882 | 00:00 | . 24 | 0.016803 | 0.023957 | 0.980373 | 00:00 | . 25 | 0.016572 | 0.023623 | 0.980373 | 00:00 | . 26 | 0.016355 | 0.023313 | 0.980864 | 00:00 | . 27 | 0.016150 | 0.023025 | 0.980864 | 00:00 | . 28 | 0.015955 | 0.022756 | 0.981354 | 00:00 | . 29 | 0.015771 | 0.022505 | 0.981354 | 00:00 | . 30 | 0.015595 | 0.022270 | 0.981354 | 00:00 | . 31 | 0.015427 | 0.022051 | 0.981845 | 00:00 | . 32 | 0.015267 | 0.021844 | 0.981845 | 00:00 | . 33 | 0.015114 | 0.021651 | 0.982826 | 00:00 | . 34 | 0.014967 | 0.021469 | 0.982826 | 00:00 | . 35 | 0.014827 | 0.021297 | 0.982826 | 00:00 | . 36 | 0.014692 | 0.021135 | 0.982826 | 00:00 | . 37 | 0.014562 | 0.020982 | 0.982826 | 00:00 | . 38 | 0.014436 | 0.020837 | 0.982826 | 00:00 | . 39 | 0.014315 | 0.020700 | 0.982826 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)); . learn.recorder.values[-1][2] . 0.982826292514801 . dls = ImageDataLoaders.from_folder(path) learn = vision_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.074170 | 0.031738 | 0.994112 | 00:23 | .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/15/digit-classifier-ipynb.html",
            "relUrl": "/fastpages/jupyter/2022/09/15/digit-classifier-ipynb.html",
            "date": " • Sep 15, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Aspiring SWE/data scientist with a strong background in sports management. . Current tech stack : . Python | SQL | Postgres | Docker | Kubernetes | RabbitMQ | . linkedin: https://www.linkedin.com/in/eric-vincent-706078137/ .",
          "url": "https://ericvincent18.github.io/fastaiMLmodel/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ericvincent18.github.io/fastaiMLmodel/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}