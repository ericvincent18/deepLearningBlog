{
  
    
        "post0": {
            "title": "CNN with Pytorch",
            "content": "import numpy as np import pandas as pd . import torch import torchvision import torchvision.transforms as transforms . transform = transforms.Compose( [transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) batch_size = 4 trainset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=True, download=True, transform=transform) trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2) testset = torchvision.datasets.CIFAR10(root=&#39;./data&#39;, train=False, download=True, transform=transform) testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2) classes = (&#39;plane&#39;, &#39;car&#39;, &#39;bird&#39;, &#39;cat&#39;, &#39;deer&#39;, &#39;dog&#39;, &#39;frog&#39;, &#39;horse&#39;, &#39;ship&#39;, &#39;truck&#39;) . Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified . import matplotlib.pyplot as plt # functions to show an image def imshow(img): img = img / 2 + 0.5 # unnormalize npimg = img.numpy() plt.imshow(np.transpose(npimg, (1, 2, 0))) plt.show() # get some random training images dataiter = iter(trainloader) images, labels = next(dataiter) # show images imshow(torchvision.utils.make_grid(images)) # print labels print(&#39; &#39;.join(f&#39;{classes[labels[j]]:5s}&#39; for j in range(batch_size))) . dog dog car ship . import torch.nn as nn import torch.nn.functional as F class Net(nn.Module): def __init__(self): super().__init__() self.conv1 = nn.Conv2d(3, 18, 5) self.pool = nn.MaxPool2d(2, 2) self.conv2 = nn.Conv2d(18, 16, 5) self.fc1 = nn.Linear(16 * 5 * 5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = torch.flatten(x, 1) # flatten all dimensions except batch x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x net = Net() . import torch.optim as optim criterion = nn.CrossEntropyLoss() optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) . for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] inputs, labels = data # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.item() if i % 2000 == 1999: # print every 2000 mini-batches print(f&#39;[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}&#39;) running_loss = 0.0 print(&#39;Finished Training&#39;) . [1, 2000] loss: 2.184 [1, 4000] loss: 1.806 [1, 6000] loss: 1.639 [1, 8000] loss: 1.523 [1, 10000] loss: 1.464 [1, 12000] loss: 1.405 [2, 2000] loss: 1.321 [2, 4000] loss: 1.285 [2, 6000] loss: 1.276 [2, 8000] loss: 1.239 [2, 10000] loss: 1.173 [2, 12000] loss: 1.170 Finished Training . PATH = &#39;./cifar_net.pth&#39; torch.save(net.state_dict(), PATH) . dataiter = iter(testloader) images, labels = next(dataiter) # print images imshow(torchvision.utils.make_grid(images)) print(&#39;GroundTruth: &#39;, &#39; &#39;.join(f&#39;{classes[labels[j]]:5s}&#39; for j in range(4))) . GroundTruth: cat ship ship plane . net = Net() net.load_state_dict(torch.load(PATH)) . &lt;All keys matched successfully&gt; . outputs = net(images) . _, predicted = torch.max(outputs, 1) print(&#39;Predicted: &#39;, &#39; &#39;.join(f&#39;{classes[predicted[j]]:5s}&#39; for j in range(4))) . Predicted: cat ship ship ship . correct = 0 total = 0 # since we&#39;re not training, we don&#39;t need to calculate the gradients for our outputs with torch.no_grad(): for data in testloader: images, labels = data # calculate outputs by running images through the network outputs = net(images) # the class with the highest energy is what we choose as prediction _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum().item() print(f&#39;Accuracy of the network on the 10000 test images: {100 * correct // total} %&#39;) . Accuracy of the network on the 10000 test images: 59 % . correct_pred = {classname: 0 for classname in classes} total_pred = {classname: 0 for classname in classes} # again no gradients needed with torch.no_grad(): for data in testloader: images, labels = data outputs = net(images) _, predictions = torch.max(outputs, 1) # collect the correct predictions for each class for label, prediction in zip(labels, predictions): if label == prediction: correct_pred[classes[label]] += 1 total_pred[classes[label]] += 1 # print accuracy for each class for classname, correct_count in correct_pred.items(): accuracy = 100 * float(correct_count) / total_pred[classname] print(f&#39;Accuracy for class: {classname:5s} is {accuracy:.1f} %&#39;) . Accuracy for class: plane is 45.7 % Accuracy for class: car is 78.1 % Accuracy for class: bird is 28.4 % Accuracy for class: cat is 37.0 % Accuracy for class: deer is 56.6 % Accuracy for class: dog is 56.5 % Accuracy for class: frog is 77.1 % Accuracy for class: horse is 64.0 % Accuracy for class: ship is 78.8 % Accuracy for class: truck is 68.0 % . device = torch.device(&#39;cuda:0&#39; if torch.cuda.is_available() else &#39;cpu&#39;) # Assuming that we are on a CUDA machine, this should print a CUDA device: print(device) . cuda:0 . net.to(device) inputs, labels = data[0].to(device), data[1].to(device) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/10/21/CNN-Pytorch-Image-Class.html",
            "relUrl": "/fastpages/jupyter/2022/10/21/CNN-Pytorch-Image-Class.html",
            "date": " • Oct 21, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Pytorch neural network",
            "content": "import numpy as np import pandas as pd . import torch from torch.utils.data import Dataset from torchvision import datasets from torchvision.transforms import ToTensor import matplotlib.pyplot as plt training_data = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor() ) test_data = datasets.FashionMNIST( root=&quot;data&quot;, train=False, download=True, transform=ToTensor() ) . Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw . labels_map = { 0: &quot;T-Shirt&quot;, 1: &quot;Trouser&quot;, 2: &quot;Pullover&quot;, 3: &quot;Dress&quot;, 4: &quot;Coat&quot;, 5: &quot;Sandal&quot;, 6: &quot;Shirt&quot;, 7: &quot;Sneaker&quot;, 8: &quot;Bag&quot;, 9: &quot;Ankle Boot&quot;, } figure = plt.figure(figsize=(8, 8)) cols, rows = 3, 3 for i in range(1, cols * rows + 1): sample_idx = torch.randint(len(training_data), size=(1,)).item() img, label = training_data[sample_idx] figure.add_subplot(rows, cols, i) plt.title(labels_map[label]) plt.axis(&quot;off&quot;) plt.imshow(img.squeeze(), cmap=&quot;gray&quot;) plt.show() . import os import pandas as pd from torchvision.io import read_image class CustomImageDataset(Dataset): def __init__(self, annotations_file, img_dir, transform=None, target_transform=None): self.img_labels = pd.read_csv(annotations_file) self.img_dir = img_dir self.transform = transform self.target_transform = target_transform def __len__(self): return len(self.img_labels) def __getitem__(self, idx): img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0]) image = read_image(img_path) label = self.img_labels.iloc[idx, 1] if self.transform: image = self.transform(image) if self.target_transform: label = self.target_transform(label) return image, label . from torch.utils.data import DataLoader train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True) test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True) . train_features, train_labels = next(iter(train_dataloader)) print(f&quot;Feature batch shape: {train_features.size()}&quot;) print(f&quot;Labels batch shape: {train_labels.size()}&quot;) img = train_features[0].squeeze() label = train_labels[0] plt.imshow(img, cmap=&quot;gray&quot;) plt.show() print(f&quot;Label: {label}&quot;) . Feature batch shape: torch.Size([64, 1, 28, 28]) Labels batch shape: torch.Size([64]) . Label: 1 . import torch from torchvision import datasets from torchvision.transforms import ToTensor, Lambda ds = datasets.FashionMNIST( root=&quot;data&quot;, train=True, download=True, transform=ToTensor(), target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) ) . import os import torch from torch import nn from torch.utils.data import DataLoader from torchvision import datasets, transforms . device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot; print(f&quot;Using {device} device&quot;) . Using cuda device . class NeuralNetwork(nn.Module): def __init__(self): super(NeuralNetwork, self).__init__() self.flatten = nn.Flatten() self.linear_relu_stack = nn.Sequential( nn.Linear(28*28, 512), nn.ReLU(), nn.Linear(512, 512), nn.ReLU(), nn.Linear(512, 10), ) def forward(self, x): x = self.flatten(x) logits = self.linear_relu_stack(x) return logits . model = NeuralNetwork().to(device) print(model) . NeuralNetwork( (flatten): Flatten(start_dim=1, end_dim=-1) (linear_relu_stack): Sequential( (0): Linear(in_features=784, out_features=512, bias=True) (1): ReLU() (2): Linear(in_features=512, out_features=512, bias=True) (3): ReLU() (4): Linear(in_features=512, out_features=10, bias=True) ) ) . X = torch.rand(1, 28, 28, device=device) logits = model(X) pred_probab = nn.Softmax(dim=1)(logits) y_pred = pred_probab.argmax(1) print(f&quot;Predicted class: {y_pred}&quot;) . Predicted class: tensor([8], device=&#39;cuda:0&#39;) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/10/20/Pytorch-lowlevel-NN.html",
            "relUrl": "/fastpages/jupyter/2022/10/20/Pytorch-lowlevel-NN.html",
            "date": " • Oct 20, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Kaggle Competition - Data exploration",
            "content": "import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) import os for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . /kaggle/input/kaggle-survey-2022/kaggle_survey_2022_responses.csv /kaggle/input/kaggle-survey-2022/Supplementary Data/kaggle_survey_2022_methodology.pdf /kaggle/input/kaggle-survey-2022/Supplementary Data/kaggle_survey_2022_answer_choices.pdf . df = pd.read_csv(&#39;/kaggle/input/kaggle-survey-2022/kaggle_survey_2022_responses.csv&#39;, low_memory=False) df.head() . Duration (in seconds) Q2 Q3 Q4 Q5 Q6_1 Q6_2 Q6_3 Q6_4 Q6_5 ... Q44_3 Q44_4 Q44_5 Q44_6 Q44_7 Q44_8 Q44_9 Q44_10 Q44_11 Q44_12 . 0 Duration (in seconds) | What is your age (# years)? | What is your gender? - Selected Choice | In which country do you currently reside? | Are you currently a student? (high school, uni... | On which platforms have you begun or completed... | On which platforms have you begun or completed... | On which platforms have you begun or completed... | On which platforms have you begun or completed... | On which platforms have you begun or completed... | ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | Who/what are your favorite media sources that ... | . 1 121 | 30-34 | Man | India | No | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 2 462 | 30-34 | Man | Algeria | No | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . 3 293 | 18-21 | Man | Egypt | Yes | Coursera | edX | NaN | DataCamp | NaN | ... | NaN | Kaggle (notebooks, forums, etc) | NaN | YouTube (Kaggle YouTube, Cloud AI Adventures, ... | Podcasts (Chai Time Data Science, O’Reilly Dat... | NaN | NaN | NaN | NaN | NaN | . 4 851 | 55-59 | Man | France | No | Coursera | NaN | Kaggle Learn Courses | NaN | NaN | ... | NaN | Kaggle (notebooks, forums, etc) | Course Forums (forums.fast.ai, Coursera forums... | NaN | NaN | Blogs (Towards Data Science, Analytics Vidhya,... | NaN | NaN | NaN | NaN | . 5 rows × 296 columns . df.describe() . Duration (in seconds) Q2 Q3 Q4 Q5 Q6_1 Q6_2 Q6_3 Q6_4 Q6_5 ... Q44_3 Q44_4 Q44_5 Q44_6 Q44_7 Q44_8 Q44_9 Q44_10 Q44_11 Q44_12 . count 23998 | 23998 | 23998 | 23998 | 23998 | 9700 | 2475 | 6629 | 3719 | 945 | ... | 2679 | 11182 | 4007 | 11958 | 2121 | 7767 | 3805 | 1727 | 1269 | 836 | . unique 3530 | 12 | 6 | 59 | 3 | 2 | 2 | 2 | 2 | 2 | ... | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | 2 | . top 272 | 18-21 | Man | India | No | Coursera | edX | Kaggle Learn Courses | DataCamp | Fast.ai | ... | Reddit (r/machinelearning, etc) | Kaggle (notebooks, forums, etc) | Course Forums (forums.fast.ai, Coursera forums... | YouTube (Kaggle YouTube, Cloud AI Adventures, ... | Podcasts (Chai Time Data Science, O’Reilly Dat... | Blogs (Towards Data Science, Analytics Vidhya,... | Journal Publications (peer-reviewed journals, ... | Slack Communities (ods.ai, kagglenoobs, etc) | None | Other | . freq 63 | 4559 | 18266 | 8792 | 12036 | 9699 | 2474 | 6628 | 3718 | 944 | ... | 2678 | 11181 | 4006 | 11957 | 2120 | 7766 | 3804 | 1726 | 1268 | 835 | . 4 rows × 296 columns . questions = [question for question in df.iloc[0]] questions . [&#39;Duration (in seconds)&#39;, &#39;What is your age (# years)?&#39;, &#39;What is your gender? - Selected Choice&#39;, &#39;In which country do you currently reside?&#39;, &#39;Are you currently a student? (high school, university, or graduate)&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Coursera&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - edX&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Kaggle Learn Courses&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - DataCamp&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Fast.ai&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udacity&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Udemy&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - LinkedIn Learning&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Cloud-certification programs (direct from AWS, Azure, GCP, or similar)&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - University Courses (resulting in a university degree)&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - None&#39;, &#39;On which platforms have you begun or completed data science courses? (Select all that apply) - Selected Choice - Other&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - University courses&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - Online courses (Coursera, EdX, etc)&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - Social media platforms (Reddit, Twitter, etc)&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - Video platforms (YouTube, Twitch, etc)&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - Kaggle (notebooks, competitions, etc)&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - None / I do not study data science&#39;, &#39;What products or platforms did you find to be most helpful when you first started studying data science? (Select all that apply) - Selected Choice - Other&#39;, &#39;What is the highest level of formal education that you have attained or plan to attain within the next 2 years?&#39;, &#39;Have you ever published any academic research (papers, preprints, conference proceedings, etc)?&#39;, &#39;Did your research make use of machine learning? - Yes, the research made advances related to some novel machine learning method (theoretical research)&#39;, &#39;Did your research make use of machine learning? - Yes, the research made use of machine learning as a tool (applied research)&#39;, &#39;Did your research make use of machine learning? - No&#39;, &#39;For how many years have you been writing code and/or programming?&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Python&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SQL&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C#&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C++&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Java&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Javascript&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - PHP&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Julia&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Go&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - None&#39;, &#39;What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Other&#39;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - JupyterLab &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - RStudio &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio Code (VSCode) &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - PyCharm &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Spyder &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Notepad++ &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Sublime Text &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Vim / Emacs &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB &#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Jupyter Notebook&#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - IntelliJ&#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - None&#34;, &#34;Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Other&#34;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Kaggle Notebooks&#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Colab Notebooks&#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Azure Notebooks&#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Code Ocean &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - IBM Watson Studio &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Amazon Sagemaker Studio &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Amazon Sagemaker Studio Lab &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Amazon EMR Notebooks &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Google Cloud Vertex AI Workbench &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Hex Workspaces&#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Noteable Notebooks &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Databricks Collaborative Notebooks &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Deepnote Notebooks &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Gradient Notebooks &#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following hosted notebook products? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Matplotlib &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Seaborn &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Plotly / Plotly Express &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Ggplot / ggplot2 &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Shiny &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - D3 js &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Altair &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Bokeh &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Geoplotlib &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Leaflet / Folium &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Pygal &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Dygraphs &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Highcharter &#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following data visualization libraries on a regular basis? (Select all that apply) - Selected Choice - Other&#39;, &#39;For how many years have you used machine learning methods?&#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Scikit-learn &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - TensorFlow &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Keras &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - PyTorch &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Fast.ai &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Xgboost &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - LightGBM &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - CatBoost &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Caret &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Tidymodels &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - JAX &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - PyTorch Lightning &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Huggingface &#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - None&#39;, &#39;Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Other&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Linear or Logistic Regression&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Decision Trees or Random Forests&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Gradient Boosting Machines (xgboost, lightgbm, etc)&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Bayesian Approaches&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Evolutionary Approaches&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Dense Neural Networks (MLPs, etc)&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Convolutional Neural Networks&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Generative Adversarial Networks&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Recurrent Neural Networks&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Transformer Networks (BERT, gpt-3, etc)&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Autoencoder Networks (DAE, VAE, etc)&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Graph Neural Networks&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - None&#39;, &#39;Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Other&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - General purpose image/video tools (PIL, cv2, skimage, etc)&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - Image segmentation methods (U-Net, Mask R-CNN, etc)&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - Object detection methods (YOLOv6, RetinaNet, etc)&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - Image classification and other general purpose networks (VGG, Inception, ResNet, ResNeXt, NASNet, EfficientNet, etc)&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - Generative Networks (GAN, VAE, etc)&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - Vision transformer networks (ViT, DeiT, BiT, BEiT, Swin, etc)&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - None&#39;, &#39;Which categories of computer vision methods do you use on a regular basis? (Select all that apply) - Selected Choice - Other&#39;, &#39;Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply) - Selected Choice - Word embeddings/vectors (GLoVe, fastText, word2vec)&#39;, &#39;Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply) - Selected Choice - Encoder-decoder models (seq2seq, vanilla transformers)&#39;, &#39;Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply) - Selected Choice - Contextualized embeddings (ELMo, CoVe)&#39;, &#39;Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply) - Selected Choice - Transformer language models (GPT-3, BERT, XLnet, etc)&#39;, &#39;Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply) - Selected Choice - None&#39;, &#39;Which of the following natural language processing (NLP) methods do you use on a regular basis? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - TensorFlow Hub &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - PyTorch Hub &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - Huggingface Models &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - Timm &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - Jumpstart &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - ONNX models &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - NVIDIA NGC models &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - Kaggle datasets &#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - No, I do not download pre-trained model weights on a regular basis&#39;, &#39;Do you download pre-trained model weights from any of the following services? (Select all that apply) - Selected Choice - Other storage services (i.e. google drive)&#39;, &#39;Which of the following ML model hubs/repositories do you use most often? - Selected Choice&#39;, &#39;Select the title most similar to your current role (or most recent title if retired): - Selected Choice&#39;, &#39;In what industry is your current employer/contract (or your most recent employer if retired)? - Selected Choice&#39;, &#39;What is the size of the company where you are employed?&#39;, &#39;Approximately how many individuals are responsible for data science workloads at your place of business?&#39;, &#39;Does your current employer incorporate machine learning methods into their business?&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Analyze and understand data to influence product or business decisions&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Build and/or run the data infrastructure that my business uses for storing, analyzing, and operationalizing data&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Build prototypes to explore applying machine learning to new areas&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Build and/or run a machine learning service that operationally improves my product or workflows&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Experimentation and iteration to improve existing ML models&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Do research that advances the state of the art of machine learning&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - None of these activities are an important part of my role at work&#39;, &#39;Select any activities that make up an important part of your role at work: (Select all that apply) - Other&#39;, &#39;What is your current yearly compensation (approximate $USD)?&#39;, &#39;Approximately how much money have you spent on machine learning and/or cloud computing services at home or at work in the past 5 years (approximate $USD)? n (approximate $USD)?&#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Amazon Web Services (AWS) &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Microsoft Azure &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Google Cloud Platform (GCP) &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - IBM Cloud / Red Hat &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Oracle Cloud &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - SAP Cloud &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - VMware Cloud &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Alibaba Cloud &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Tencent Cloud &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Huawei Cloud &#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - None&#39;, &#39;Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Other&#39;, &#39;Of the cloud platforms that you are familiar with, which has the best developer experience (most enjoyable to use)? - Selected Choice&#39;, &#39;Do you use any of the following cloud computing products? (Select all that apply) - Selected Choice - Amazon Elastic Compute Cloud (EC2) &#39;, &#39;Do you use any of the following cloud computing products? (Select all that apply) - Selected Choice - Microsoft Azure Virtual Machines &#39;, &#39;Do you use any of the following cloud computing products? (Select all that apply) - Selected Choice - Google Cloud Compute Engine &#39;, &#39;Do you use any of the following cloud computing products? (Select all that apply) - Selected Choice - No / None&#39;, &#39;Do you use any of the following cloud computing products? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Microsoft Azure Blob Storage&#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Microsoft Azure Files &#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Amazon Simple Storage Service (S3) &#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Amazon Elastic File System (EFS) &#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Google Cloud Storage (GCS) &#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Google Cloud Filestore &#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - No / None&#39;, &#39;Do you use any of the following data storage products? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - MySQL &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - PostgreSQL &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - SQLite &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Oracle Database &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - MongoDB &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Snowflake &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - IBM Db2 &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Microsoft SQL Server &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Microsoft Azure SQL Database &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Amazon Redshift &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Amazon RDS &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Amazon DynamoDB &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Google Cloud BigQuery &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Google Cloud SQL &#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Amazon QuickSight&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Microsoft Power BI&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Google Data Studio&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Looker&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Tableau&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Qlik Sense&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Domo&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - TIBCO Spotfire&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Alteryx &#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Sisense &#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - SAP Analytics Cloud &#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Microsoft Azure Synapse &#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Thoughtspot &#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Amazon SageMaker &#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Azure Machine Learning Studio &#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Google Cloud Vertex AI&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - DataRobot&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Databricks&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Dataiku&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Alteryx&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Rapidminer&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - C3.ai&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Domino Data Lab &#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - H2O AI Cloud &#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - No / None&#39;, &#39;Do you use any of the following managed machine learning products on a regular basis? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Google Cloud AutoML &#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - H2O Driverless AI &#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Databricks AutoML &#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - DataRobot AutoML &#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Amazon Sagemaker Autopilot &#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Azure Automated Machine Learning &#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - No / None&#39;, &#39;Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - TensorFlow Extended (TFX) &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - TorchServe &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - ONNX Runtime &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - Triton Inference Server &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - OpenVINO Model Server &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - KServe &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - BentoML &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - Multi Model Server (MMS) &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - Seldon Core &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - MLflow &#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following products to serve your machine learning models? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Neptune.ai &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Weights &amp; Biases &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Comet.ml &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - TensorBoard &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Guild.ai &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - ClearML &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - MLflow &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Aporia &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Evidently AI &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Arize &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - WhyLabs &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Fiddler &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - DVC &#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - No / None&#39;, &#39;Do you use any tools to help monitor your machine learning models and/or experiments? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - Google Responsible AI Toolkit (LIT, What-if, Fairness Indicator, etc) &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - Microsoft Responsible AI Resources (Fairlearn, Counterfit, InterpretML, etc) &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - IBM AI Ethics tools (AI Fairness 360, Adversarial Robustness Toolbox, etc &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - Amazon AI Ethics Tools (Clarify, A2I, etc) &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - The LinkedIn Fairness Toolkit (LiFT) &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - Audit-AI &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - Aequitas &#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following responsible or ethical AI products in your machine learning practices? (Select all that apply) - Selected Choice - Other&#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - GPUs &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - TPUs &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - IPUs &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - RDUs &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - WSEs &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - Trainium Chips &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - Inferentia Chips &#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - None&#39;, &#39;Do you use any of the following types of specialized hardware when training machine learning models? (Select all that apply) - Selected Choice - Other&#39;, &#39;Approximately how many times have you used a TPU (tensor processing unit)?&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Twitter (data science influencers)&#39;, &#34;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Email newsletters (Data Elixir, O&#39;Reilly Data &amp; AI, etc)&#34;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Reddit (r/machinelearning, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Kaggle (notebooks, forums, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Course Forums (forums.fast.ai, Coursera forums, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - YouTube (Kaggle YouTube, Cloud AI Adventures, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Podcasts (Chai Time Data Science, O’Reilly Data Show, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Blogs (Towards Data Science, Analytics Vidhya, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Journal Publications (peer-reviewed journals, conference proceedings, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Slack Communities (ods.ai, kagglenoobs, etc)&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - None&#39;, &#39;Who/what are your favorite media sources that report on data science topics? (Select all that apply) - Selected Choice - Other&#39;] . df.stack() . 0 Duration (in seconds) Duration (in seconds) Q2 What is your age (# years)? Q3 What is your gender? - Selected Choice Q4 In which country do you currently reside? Q5 Are you currently a student? (high school, uni... ... 23997 Q17_2 TensorFlow Q18_1 Linear or Logistic Regression Q18_2 Decision Trees or Random Forests Q21_9 No, I do not download pre-trained model weight... Q44_12 Other Length: 858725, dtype: object . dft = df.copy() dft.T . 0 1 2 3 4 5 6 7 8 9 ... 23988 23989 23990 23991 23992 23993 23994 23995 23996 23997 . Duration (in seconds) Duration (in seconds) | 121 | 462 | 293 | 851 | 232 | 277 | 1550 | 501 | 787 | ... | 245 | 402 | 603 | 557 | 153 | 331 | 330 | 860 | 597 | 303 | . Q2 What is your age (# years)? | 30-34 | 30-34 | 18-21 | 55-59 | 45-49 | 18-21 | 18-21 | 30-34 | 70+ | ... | 18-21 | 55-59 | 35-39 | 40-44 | 22-24 | 22-24 | 60-69 | 25-29 | 35-39 | 18-21 | . Q3 What is your gender? - Selected Choice | Man | Man | Man | Man | Man | Woman | Man | Man | Man | ... | Man | Man | Man | Man | Man | Man | Man | Man | Woman | Man | . Q4 In which country do you currently reside? | India | Algeria | Egypt | France | India | India | India | Germany | Australia | ... | India | Ukraine | India | India | India | United States of America | United States of America | Turkey | Israel | India | . Q5 Are you currently a student? (high school, uni... | No | No | Yes | No | Yes | Yes | Yes | No | No | ... | Yes | Yes | No | No | Yes | Yes | Yes | No | No | Yes | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . Q44_8 Who/what are your favorite media sources that ... | NaN | NaN | NaN | Blogs (Towards Data Science, Analytics Vidhya,... | Blogs (Towards Data Science, Analytics Vidhya,... | Blogs (Towards Data Science, Analytics Vidhya,... | Blogs (Towards Data Science, Analytics Vidhya,... | Blogs (Towards Data Science, Analytics Vidhya,... | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Q44_9 Who/what are your favorite media sources that ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | Journal Publications (peer-reviewed journals, ... | NaN | NaN | NaN | NaN | . Q44_10 Who/what are your favorite media sources that ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | . Q44_11 Who/what are your favorite media sources that ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | NaN | NaN | NaN | None | NaN | NaN | NaN | NaN | NaN | . Q44_12 Who/what are your favorite media sources that ... | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | NaN | ... | NaN | Other | NaN | NaN | NaN | NaN | NaN | NaN | NaN | Other | . 296 rows × 23998 columns . df.isna().sum() . Duration (in seconds) 0 Q2 0 Q3 0 Q4 0 Q5 0 ... Q44_9 20193 Q44_10 22271 Q44_11 22729 Q44_12 23162 aggpro 20072 Length: 297, dtype: int64 . dfid.groupby([&#39;Q2&#39;])[[&#39;student?_Yes&#39;,&#39;student?_No&#39;]].sum() . student?_Yes student?_No . Q2 . 18-21 4386.0 | 173.0 | . 22-24 2981.0 | 1302.0 | . 25-29 1976.0 | 2496.0 | . 30-34 887.0 | 2085.0 | . 35-39 627.0 | 1726.0 | . 40-44 464.0 | 1463.0 | . 45-49 290.0 | 963.0 | . 50-54 163.0 | 751.0 | . 55-59 99.0 | 512.0 | . 60-69 64.0 | 462.0 | . 70+ 24.0 | 103.0 | . dfid.groupby([&#39;Q4&#39;])[[&#39;student?_Yes&#39;,&#39;student?_No&#39;]].sum() . student?_Yes student?_No . Q4 . Algeria 46.0 | 16.0 | . Argentina 110.0 | 94.0 | . Australia 56.0 | 86.0 | . Bangladesh 182.0 | 69.0 | . Belgium 8.0 | 43.0 | . Brazil 437.0 | 396.0 | . Cameroon 50.0 | 18.0 | . Canada 87.0 | 170.0 | . Chile 50.0 | 65.0 | . China 290.0 | 163.0 | . Colombia 137.0 | 119.0 | . Czech Republic 10.0 | 39.0 | . Ecuador 40.0 | 14.0 | . Egypt 240.0 | 143.0 | . Ethiopia 57.0 | 41.0 | . France 74.0 | 188.0 | . Germany 32.0 | 67.0 | . Ghana 68.0 | 39.0 | . Hong Kong (S.A.R.) 17.0 | 41.0 | . I do not wish to disclose my location 23.0 | 19.0 | . India 4967.0 | 3825.0 | . Indonesia 228.0 | 148.0 | . Iran, Islamic Republic of... 74.0 | 46.0 | . Ireland 17.0 | 36.0 | . Israel 33.0 | 69.0 | . Italy 54.0 | 128.0 | . Japan 86.0 | 470.0 | . Kenya 134.0 | 67.0 | . Malaysia 33.0 | 41.0 | . Mexico 192.0 | 188.0 | . Morocco 119.0 | 58.0 | . Nepal 54.0 | 21.0 | . Netherlands 13.0 | 95.0 | . Nigeria 482.0 | 249.0 | . Other 708.0 | 722.0 | . Pakistan 418.0 | 202.0 | . Peru 73.0 | 48.0 | . Philippines 47.0 | 61.0 | . Poland 29.0 | 84.0 | . Portugal 34.0 | 53.0 | . Romania 21.0 | 29.0 | . Russia 160.0 | 164.0 | . Saudi Arabia 32.0 | 52.0 | . Singapore 22.0 | 46.0 | . South Africa 58.0 | 51.0 | . South Korea 104.0 | 213.0 | . Spain 80.0 | 177.0 | . Sri Lanka 43.0 | 34.0 | . Taiwan 70.0 | 172.0 | . Thailand 53.0 | 79.0 | . Tunisia 96.0 | 29.0 | . Turkey 175.0 | 170.0 | . Ukraine 40.0 | 39.0 | . United Arab Emirates 20.0 | 74.0 | . United Kingdom of Great Britain and Northern Ireland 87.0 | 171.0 | . United States of America 924.0 | 1996.0 | . Viet Nam 135.0 | 77.0 | . Zimbabwe 32.0 | 22.0 | . dfid[&#39;Q3&#39;].value_counts() . Man 18266 Woman 5286 Prefer not to say 334 Nonbinary 78 Prefer to self-describe 33 Name: Q3, dtype: int64 . # Q12_1 dfpro[&#39;Q12_1&#39;].value_counts() dfpro[&#39;Q12_2&#39;].value_counts() . R 4571 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R 1 Name: Q12_2, dtype: int64 . for q in range(1,14): print(dfpro[f&quot;Q12_{q}&quot;].value_counts()) . Python 18653 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Python 1 Name: Q12_1, dtype: int64 R 4571 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - R 1 Name: Q12_2, dtype: int64 SQL 9620 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - SQL 1 Name: Q12_3, dtype: int64 C 3801 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C 1 Name: Q12_4, dtype: int64 C# 1473 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C# 1 Name: Q12_5, dtype: int64 C++ 4549 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - C++ 1 Name: Q12_6, dtype: int64 Java 3862 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Java 1 Name: Q12_7, dtype: int64 Javascript 3489 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Javascript 1 Name: Q12_8, dtype: int64 Bash 1674 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Bash 1 Name: Q12_9, dtype: int64 PHP 1443 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - PHP 1 Name: Q12_10, dtype: int64 MATLAB 2441 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB 1 Name: Q12_11, dtype: int64 Julia 296 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Julia 1 Name: Q12_12, dtype: int64 Go 322 What programming languages do you use on a regular basis? (Select all that apply) - Selected Choice - Go 1 Name: Q12_13, dtype: int64 . dfpro[&#39;Q12_1&#39;].isna().sum() . 5344 . for q in range(1,14): print(dfpro[f&quot;Q13_{q}&quot;].value_counts()) . JupyterLab 4887 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - JupyterLab 1 Name: Q13_1, dtype: int64 RStudio 3824 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - RStudio 1 Name: Q13_2, dtype: int64 Visual Studio 4416 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio 1 Name: Q13_3, dtype: int64 Visual Studio Code (VSCode) 9976 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Visual Studio Code (VSCode) 1 Name: Q13_4, dtype: int64 PyCharm 6099 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - PyCharm 1 Name: Q13_5, dtype: int64 Spyder 2880 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Spyder 1 Name: Q13_6, dtype: int64 Notepad++ 3891 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Notepad++ 1 Name: Q13_7, dtype: int64 Sublime Text 2218 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Sublime Text 1 Name: Q13_8, dtype: int64 Vim / Emacs 1448 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Vim / Emacs 1 Name: Q13_9, dtype: int64 MATLAB 2302 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - MATLAB 1 Name: Q13_10, dtype: int64 Jupyter Notebook 13684 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Jupyter Notebook 1 Name: Q13_11, dtype: int64 IntelliJ 1612 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - IntelliJ 1 Name: Q13_12, dtype: int64 None 409 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - None 1 Name: Q13_13, dtype: int64 . dfpro[&#39;Q13_11&#39;].value_counts() # most used is python, best ide is Jupyter, then Vs code . Jupyter Notebook 13684 Which of the following integrated development environments (IDE&#39;s) do you use on a regular basis? (Select all that apply) - Selected Choice - Jupyter Notebook 1 Name: Q13_11, dtype: int64 . # 648 responses and pytorch 5191 for q in range(1,16): print(dfpro[f&quot;Q17_{q}&quot;].value_counts()) . Scikit-learn 11403 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Scikit-learn 1 Name: Q17_1, dtype: int64 TensorFlow 7953 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - TensorFlow 1 Name: Q17_2, dtype: int64 Keras 6575 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Keras 1 Name: Q17_3, dtype: int64 PyTorch 5191 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - PyTorch 1 Name: Q17_4, dtype: int64 Fast.ai 648 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Fast.ai 1 Name: Q17_5, dtype: int64 Xgboost 4477 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Xgboost 1 Name: Q17_6, dtype: int64 LightGBM 1940 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - LightGBM 1 Name: Q17_7, dtype: int64 CatBoost 1165 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - CatBoost 1 Name: Q17_8, dtype: int64 Caret 821 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Caret 1 Name: Q17_9, dtype: int64 Tidymodels 547 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Tidymodels 1 Name: Q17_10, dtype: int64 JAX 252 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - JAX 1 Name: Q17_11, dtype: int64 PyTorch Lightning 1013 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - PyTorch Lightning 1 Name: Q17_12, dtype: int64 Huggingface 1332 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Huggingface 1 Name: Q17_13, dtype: int64 None 1709 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - None 1 Name: Q17_14, dtype: int64 Other 620 Which of the following machine learning frameworks do you use on a regular basis? (Select all that apply) - Selected Choice - Other 1 Name: Q17_15, dtype: int64 . for q in range(1,15): print(dfpro[f&quot;Q18_{q}&quot;].value_counts()) . Linear or Logistic Regression 11338 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Linear or Logistic Regression 1 Name: Q18_1, dtype: int64 Decision Trees or Random Forests 9373 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Decision Trees or Random Forests 1 Name: Q18_2, dtype: int64 Gradient Boosting Machines (xgboost, lightgbm, etc) 5506 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Gradient Boosting Machines (xgboost, lightgbm, etc) 1 Name: Q18_3, dtype: int64 Bayesian Approaches 3661 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Bayesian Approaches 1 Name: Q18_4, dtype: int64 Evolutionary Approaches 823 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Evolutionary Approaches 1 Name: Q18_5, dtype: int64 Dense Neural Networks (MLPs, etc) 3476 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Dense Neural Networks (MLPs, etc) 1 Name: Q18_6, dtype: int64 Convolutional Neural Networks 6006 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Convolutional Neural Networks 1 Name: Q18_7, dtype: int64 Generative Adversarial Networks 1166 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Generative Adversarial Networks 1 Name: Q18_8, dtype: int64 Recurrent Neural Networks 3451 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Recurrent Neural Networks 1 Name: Q18_9, dtype: int64 Transformer Networks (BERT, gpt-3, etc) 2196 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Transformer Networks (BERT, gpt-3, etc) 1 Name: Q18_10, dtype: int64 Autoencoder Networks (DAE, VAE, etc) 1234 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Autoencoder Networks (DAE, VAE, etc) 1 Name: Q18_11, dtype: int64 Graph Neural Networks 1422 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Graph Neural Networks 1 Name: Q18_12, dtype: int64 None 1326 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - None 1 Name: Q18_13, dtype: int64 Other 538 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Other 1 Name: Q18_14, dtype: int64 . dfpro[&#39;Q18_7&#39;].value_counts() . Convolutional Neural Networks 6006 Which of the following ML algorithms do you use on a regular basis? (Select all that apply): - Selected Choice - Convolutional Neural Networks 1 Name: Q18_7, dtype: int64 . dfpro[&#39;Q29&#39;].value_counts() . $0-999 1112 10,000-14,999 493 30,000-39,999 464 1,000-1,999 444 40,000-49,999 421 100,000-124,999 404 5,000-7,499 391 50,000-59,999 366 7,500-9,999 362 150,000-199,999 342 20,000-24,999 337 60,000-69,999 318 15,000-19,999 299 70,000-79,999 289 25,000-29,999 277 2,000-2,999 271 125,000-149,999 269 3,000-3,999 244 4,000-4,999 234 80,000-89,999 222 90,000-99,999 197 200,000-249,999 155 250,000-299,999 78 300,000-499,999 76 $500,000-999,999 48 &gt;$1,000,000 23 What is your current yearly compensation (approximate $USD)? 1 Name: Q29, dtype: int64 . dfpro[&#39;Q30&#39;].value_counts() # majority dont spend anything or max between 0-999$, people who put down more is prob for education . $0 ($USD) 2822 $100-$999 2078 $1000-$9,999 1469 $1-$99 1449 $10,000-$99,999 480 $100,000 or more ($USD) 186 Approximately how much money have you spent on machine learning and/or cloud computing services at home or at work in the past 5 years (approximate $USD)? n (approximate $USD)? 1 Name: Q30, dtype: int64 . for q in range(1,13): print(dfpro[f&quot;Q31_{q}&quot;].value_counts()) . Amazon Web Services (AWS) 2346 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Amazon Web Services (AWS) 1 Name: Q31_1, dtype: int64 Microsoft Azure 1416 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Microsoft Azure 1 Name: Q31_2, dtype: int64 Google Cloud Platform (GCP) 2056 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Google Cloud Platform (GCP) 1 Name: Q31_3, dtype: int64 IBM Cloud / Red Hat 287 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - IBM Cloud / Red Hat 1 Name: Q31_4, dtype: int64 Oracle Cloud 230 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Oracle Cloud 1 Name: Q31_5, dtype: int64 SAP Cloud 107 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - SAP Cloud 1 Name: Q31_6, dtype: int64 VMware Cloud 155 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - VMware Cloud 1 Name: Q31_7, dtype: int64 Alibaba Cloud 76 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Alibaba Cloud 1 Name: Q31_8, dtype: int64 Tencent Cloud 56 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Tencent Cloud 1 Name: Q31_9, dtype: int64 Huawei Cloud 47 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Huawei Cloud 1 Name: Q31_10, dtype: int64 None 1167 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - None 1 Name: Q31_11, dtype: int64 Other 217 Which of the following cloud computing platforms do you use? (Select all that apply) - Selected Choice - Other 1 Name: Q31_12, dtype: int64 . for q in range(1,17): print(dfpro[f&quot;Q35_{q}&quot;].value_counts()) . MySQL 2233 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - MySQL 1 Name: Q35_1, dtype: int64 PostgreSQL 1516 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - PostgreSQL 1 Name: Q35_2, dtype: int64 SQLite 1159 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - SQLite 1 Name: Q35_3, dtype: int64 Oracle Database 688 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Oracle Database 1 Name: Q35_4, dtype: int64 MongoDB 1031 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - MongoDB 1 Name: Q35_5, dtype: int64 Snowflake 399 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Snowflake 1 Name: Q35_6, dtype: int64 IBM Db2 192 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - IBM Db2 1 Name: Q35_7, dtype: int64 Microsoft SQL Server 1203 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Microsoft SQL Server 1 Name: Q35_8, dtype: int64 Microsoft Azure SQL Database 520 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Microsoft Azure SQL Database 1 Name: Q35_9, dtype: int64 Amazon Redshift 380 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Amazon Redshift 1 Name: Q35_10, dtype: int64 Amazon RDS 505 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Amazon RDS 1 Name: Q35_11, dtype: int64 Amazon DynamoDB 356 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Amazon DynamoDB 1 Name: Q35_12, dtype: int64 Google Cloud BigQuery 690 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Google Cloud BigQuery 1 Name: Q35_13, dtype: int64 Google Cloud SQL 439 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Google Cloud SQL 1 Name: Q35_14, dtype: int64 None 955 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - None 1 Name: Q35_15, dtype: int64 Other 217 Do you use any of the following data products (relational databases, data warehouses, data lakes, or similar)? (Select all that apply) - Selected Choice - Other 1 Name: Q35_16, dtype: int64 . for q in range(1,16): print(dfpro[f&quot;Q36_{q}&quot;].value_counts()) . Amazon QuickSight 224 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Amazon QuickSight 1 Name: Q36_1, dtype: int64 Microsoft Power BI 1658 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Microsoft Power BI 1 Name: Q36_2, dtype: int64 Google Data Studio 643 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Google Data Studio 1 Name: Q36_3, dtype: int64 Looker 166 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Looker 1 Name: Q36_4, dtype: int64 Tableau 1732 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Tableau 1 Name: Q36_5, dtype: int64 Qlik Sense 207 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Qlik Sense 1 Name: Q36_6, dtype: int64 Domo 44 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Domo 1 Name: Q36_7, dtype: int64 TIBCO Spotfire 86 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - TIBCO Spotfire 1 Name: Q36_8, dtype: int64 Alteryx 132 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Alteryx 1 Name: Q36_9, dtype: int64 Sisense 38 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Sisense 1 Name: Q36_10, dtype: int64 SAP Analytics Cloud 106 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - SAP Analytics Cloud 1 Name: Q36_11, dtype: int64 Microsoft Azure Synapse 167 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Microsoft Azure Synapse 1 Name: Q36_12, dtype: int64 Thoughtspot 22 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Thoughtspot 1 Name: Q36_13, dtype: int64 None 2050 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - None 1 Name: Q36_14, dtype: int64 Other 191 Do you use any of the following business intelligence tools? (Select all that apply) - Selected Choice - Other 1 Name: Q36_15, dtype: int64 . for q in range(1,8): print(dfpro[f&quot;Q38_{q}&quot;].value_counts()) . Google Cloud AutoML 463 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Google Cloud AutoML 1 Name: Q38_1, dtype: int64 H2O Driverless AI 122 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - H2O Driverless AI 1 Name: Q38_2, dtype: int64 Databricks AutoML 193 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Databricks AutoML 1 Name: Q38_3, dtype: int64 DataRobot AutoML 125 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - DataRobot AutoML 1 Name: Q38_4, dtype: int64 Amazon Sagemaker Autopilot 261 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Amazon Sagemaker Autopilot 1 Name: Q38_5, dtype: int64 Azure Automated Machine Learning 323 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - Azure Automated Machine Learning 1 Name: Q38_6, dtype: int64 No / None 3536 Do you use any of the following automated machine learning tools? (Select all that apply) - Selected Choice - No / None 1 Name: Q38_7, dtype: int64 . (dfpro[&#39;Q11&#39;].value_counts()/dfpro[&#39;Q11&#39;].value_counts().sum()) *100 . 1-3 years 27.787816 &lt; 1 years 23.464120 3-5 years 14.623129 5-10 years 10.996386 I have never written code 8.763552 10-20 years 7.748236 20+ years 6.612459 For how many years have you been writing code and/or programming? 0.004302 Name: Q11, dtype: float64 . a = pd.Series(dfpro[&#39;Q11&#39;]) . a.pop(0) . &#39;For how many years have you been writing code and/or programming?&#39; . b = a.value_counts() . b.plot.bar() . &lt;AxesSubplot:&gt; .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/10/19/data-exploration-kaggle-ml-data.html",
            "relUrl": "/fastpages/jupyter/2022/10/19/data-exploration-kaggle-ml-data.html",
            "date": " • Oct 19, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "TabularPandas on Titanic dataset.",
            "content": "iskaggle = os.environ.get(&#39;KAGGLE_KERNEL_RUN_TYPE&#39;, &#39;&#39;) if iskaggle: path = Path(&#39;../input/titanic&#39;) else: path = Path(&#39;titanic&#39;) if not path.exists(): import zipfile,kaggle kaggle.api.competition_download_cli(str(path)) zipfile.ZipFile(f&#39;{path}.zip&#39;).extractall(path) . import torch, numpy as np, pandas as pd np.set_printoptions(linewidth=140) torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7) pd.set_option(&#39;display.width&#39;, 140) . !pip install fastbook import fastbook fastbook.setup_book() from fastbook import * import torch.nn.functional as F . Collecting fastbook Downloading fastbook-0.0.28-py3-none-any.whl (719 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 719.8/719.8 kB 30.6 MB/s eta 0:00:00 Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from fastbook) (0.20.1) Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from fastbook) (21.3) Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (from fastbook) (22.2.2) Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from fastbook) (2.3.2) Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from fastbook) (0.1.96) Collecting fastai&gt;=2.6 Downloading fastai-2.7.9-py3-none-any.whl (225 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 225.5/225.5 kB 28.8 MB/s eta 0:00:00 Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from fastbook) (1.4.3) Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from fastbook) (4.20.1) Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fastbook) (2.28.1) Requirement already satisfied: spacy&lt;4 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (3.4.0) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.1.1) Requirement already satisfied: torchvision&gt;=0.8.2 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (0.13.0+cu116) Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.8.1) Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (5.4.1) Collecting fastprogress&gt;=0.2.4 Downloading fastprogress-1.0.3-py3-none-any.whl (12 kB) Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (3.5.2) Requirement already satisfied: pillow&gt;6.0.0 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (9.2.0) Collecting fastdownload&lt;2,&gt;=0.0.5 Downloading fastdownload-0.0.7-py3-none-any.whl (12 kB) Requirement already satisfied: torch&lt;1.14,&gt;=1.7 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.12.0+cu116) Collecting fastcore&lt;1.6,&gt;=1.4.5 Downloading fastcore-1.5.27-py3-none-any.whl (67 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 67.1/67.1 kB 12.2 MB/s eta 0:00:00 Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (3.8.1) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (1.23.1) Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (3.0.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (4.64.0) Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (2022.5.0) Requirement already satisfied: dill&lt;0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.3.5.1) Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.70.13) Requirement already satisfied: responses&lt;0.19 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.18.0) Requirement already satisfied: pyarrow&gt;=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (8.0.0) Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.8.1) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python3/dist-packages (from requests-&gt;fastbook) (2019.11.28) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;fastbook) (1.26.10) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;fastbook) (2.1.0) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests-&gt;fastbook) (2.8) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging-&gt;fastbook) (3.0.9) Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas-&gt;fastbook) (2022.1) Requirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas-&gt;fastbook) (2.8.2) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers-&gt;fastbook) (0.12.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers-&gt;fastbook) (2022.7.9) Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers-&gt;fastbook) (3.7.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets-&gt;fastbook) (4.3.0) Requirement already satisfied: six&gt;=1.5 in /usr/lib/python3/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;fastbook) (1.14.0) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.9.1) Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.7) Requirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.1.0) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.6) Requirement already satisfied: pathy&gt;=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.2) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.6) Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (63.1.0) Requirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.4.2) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.3) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.10.0,&gt;=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.9.1) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.7) Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2) Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.2) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.9) Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.2.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2) Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.0) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (18.2.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.7.2) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.3) Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.34.4) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.0) Requirement already satisfied: joblib&gt;=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.0) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (5.2.1) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.8) Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.1.3) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.1) Installing collected packages: fastprogress, fastcore, fastdownload, fastai, fastbook Successfully installed fastai-2.7.9 fastbook-0.0.28 fastcore-1.5.27 fastdownload-0.0.7 fastprogress-1.0.3 WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv . from fastai.tabular.all import * pd.options.display.float_format = &#39;{:.2f}&#39;.format set_seed(42) . df = pd.read_csv(path/&#39;train.csv&#39;) . def add_features(df): df[&#39;LogFare&#39;] = np.log1p(df[&#39;Fare&#39;]) df[&#39;Deck&#39;] = df.Cabin.str[0].map(dict(A=&quot;ABC&quot;, B=&quot;ABC&quot;, C=&quot;ABC&quot;, D=&quot;DE&quot;, E=&quot;DE&quot;, F=&quot;FG&quot;, G=&quot;FG&quot;)) df[&#39;Family&#39;] = df.SibSp+df.Parch df[&#39;Alone&#39;] = df.Family==0 df[&#39;TicketFreq&#39;] = df.groupby(&#39;Ticket&#39;)[&#39;Ticket&#39;].transform(&#39;count&#39;) df[&#39;Title&#39;] = df.Name.str.split(&#39;, &#39;, expand=True)[1].str.split(&#39;.&#39;, expand=True)[0] df[&#39;Title&#39;] = df.Title.map(dict(Mr=&quot;Mr&quot;,Miss=&quot;Miss&quot;,Mrs=&quot;Mrs&quot;,Master=&quot;Master&quot;)) add_features(df) . df.groupby(&#39;Sex&#39;)[&#39;Fare&#39;].mean() . Sex female 44.48 male 25.52 Name: Fare, dtype: float64 . splits = RandomSplitter(seed=42)(df) . dls = TabularPandas( df, splits=splits, procs = [Categorify, FillMissing, Normalize], cat_names=[&quot;Sex&quot;,&quot;Pclass&quot;,&quot;Embarked&quot;,&quot;Deck&quot;, &quot;Title&quot;], cont_names=[&#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;LogFare&#39;, &#39;Alone&#39;, &#39;TicketFreq&#39;, &#39;Family&#39;], y_names=&quot;Survived&quot;, y_block = CategoryBlock(), ).dataloaders(path=&quot;.&quot;) . learn = tabular_learner(dls, metrics=accuracy, layers=[10,10]) . learn.lr_find(suggest_funcs=(slide, valley)) . SuggestedLRs(slide=0.05754399299621582, valley=0.013182567432522774) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/30/tabular-data.html",
            "relUrl": "/fastpages/jupyter/2022/09/30/tabular-data.html",
            "date": " • Sep 30, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Decision tree and random forest on Titanic dataset",
            "content": "Setup using a paperspace gradient notebook . iskaggle = os.environ.get(&#39;KAGGLE_KERNEL_RUN_TYPE&#39;, &#39;&#39;) if iskaggle: path = Path(&#39;../input/titanic&#39;) else: path = Path(&#39;titanic&#39;) if not path.exists(): import zipfile,kaggle kaggle.api.competition_download_cli(str(path)) zipfile.ZipFile(f&#39;{path}.zip&#39;).extractall(path) . import torch, numpy as np, pandas as pd np.set_printoptions(linewidth=140) torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7) pd.set_option(&#39;display.width&#39;, 140) . df = pd.read_csv(path/&#39;train.csv&#39;) tst_df = pd.read_csv(path/&#39;test.csv&#39;) modes = df.mode().iloc[0] . def proc_data(df): df[&#39;Fare&#39;] = df.Fare.fillna(0) df.fillna(modes, inplace=True) df[&#39;LogFare&#39;] = np.log1p(df[&#39;Fare&#39;]) df[&#39;Embarked&#39;] = pd.Categorical(df.Embarked) df[&#39;Sex&#39;] = pd.Categorical(df.Sex) proc_data(df) proc_data(tst_df) . cats=[&quot;Sex&quot;,&quot;Embarked&quot;] conts=[&#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;LogFare&#39;,&quot;Pclass&quot;] dep=&quot;Survived&quot; . from numpy import random from sklearn.model_selection import train_test_split # set random seed to reproduce results random.seed(42) trn_df,val_df = train_test_split(df, test_size=0.25) trn_df[cats] = trn_df[cats].apply(lambda x: x.cat.codes) val_df[cats] = val_df[cats].apply(lambda x: x.cat.codes) . ModuleNotFoundError Traceback (most recent call last) /Users/ericvincent/gitblog/fastaiMLmodel/_notebooks/2022-09-30-random-forest.ipynb Cell 10 in &lt;cell line: 1&gt;() -&gt; &lt;a href=&#39;vscode-notebook-cell:/Users/ericvincent/gitblog/fastaiMLmodel/_notebooks/2022-09-30-random-forest.ipynb#X11sZmlsZQ%3D%3D?line=0&#39;&gt;1&lt;/a&gt; from numpy import random &lt;a href=&#39;vscode-notebook-cell:/Users/ericvincent/gitblog/fastaiMLmodel/_notebooks/2022-09-30-random-forest.ipynb#X11sZmlsZQ%3D%3D?line=1&#39;&gt;2&lt;/a&gt; from sklearn.model_selection import train_test_split &lt;a href=&#39;vscode-notebook-cell:/Users/ericvincent/gitblog/fastaiMLmodel/_notebooks/2022-09-30-random-forest.ipynb#X11sZmlsZQ%3D%3D?line=3&#39;&gt;4&lt;/a&gt; # set random seed to reproduce results ModuleNotFoundError: No module named &#39;numpy&#39; . def xs_y(df): xs = df[cats+conts].copy() return xs,df[dep] if dep in df else None trn_xs,trn_y = xs_y(trn_df) val_xs,val_y = xs_y(val_df) . from sklearn.metrics import mean_absolute_error . from sklearn.tree import DecisionTreeClassifier, export_graphviz m = DecisionTreeClassifier(max_leaf_nodes=4).fit(trn_xs, trn_y); . from sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier(100, min_samples_leaf=5) rf.fit(trn_xs, trn_y); mean_absolute_error(val_y, rf.predict(val_xs)) . 0.18834080717488788 . m.feature_importances_ . array([0.71053851, 0. , 0.09383065, 0. , 0. , 0. , 0.19563083]) . pd.DataFrame(dict(cols=trn_xs.columns, imp=m.feature_importances_)).plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;); .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/30/random-forest.html",
            "relUrl": "/fastpages/jupyter/2022/09/30/random-forest.html",
            "date": " • Sep 30, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Revisiting Linear Regression & Neural Networks",
            "content": "iskaggle = os.environ.get(&#39;KAGGLE_KERNEL_RUN_TYPE&#39;, &#39;&#39;) if iskaggle: path = Path(&#39;../input/titanic&#39;) else: path = Path(&#39;titanic&#39;) if not path.exists(): import zipfile,kaggle kaggle.api.competition_download_cli(str(path)) zipfile.ZipFile(f&#39;{path}.zip&#39;).extractall(path) . import torch, numpy as np, pandas as pd np.set_printoptions(linewidth=140) torch.set_printoptions(linewidth=140, sci_mode=False, edgeitems=7) pd.set_option(&#39;display.width&#39;, 140) . df = pd.read_csv(path/&#39;train.csv&#39;) df . PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked . 0 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN | S | . 1 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C | . 2 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | NaN | S | . 3 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | S | . 4 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500 | NaN | S | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | 2 | Montvila, Rev. Juozas | male | 27.0 | 0 | 0 | 211536 | 13.0000 | NaN | S | . 887 888 | 1 | 1 | Graham, Miss. Margaret Edith | female | 19.0 | 0 | 0 | 112053 | 30.0000 | B42 | S | . 888 889 | 0 | 3 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | female | NaN | 1 | 2 | W./C. 6607 | 23.4500 | NaN | S | . 889 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.0000 | C148 | C | . 890 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.7500 | NaN | Q | . 891 rows × 12 columns . df.isna().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 177 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 687 Embarked 2 dtype: int64 . modes = df.mode().iloc[0] . df.fillna(modes, inplace=True) . df.isna().sum() . PassengerId 0 Survived 0 Pclass 0 Name 0 Sex 0 Age 0 SibSp 0 Parch 0 Ticket 0 Fare 0 Cabin 0 Embarked 0 dtype: int64 . import numpy as np df.describe(include=(np.number)) . PassengerId Survived Pclass Age SibSp Parch Fare . count 891.000000 | 891.000000 | 891.000000 | 891.000000 | 891.000000 | 891.000000 | 891.000000 | . mean 446.000000 | 0.383838 | 2.308642 | 28.566970 | 0.523008 | 0.381594 | 32.204208 | . std 257.353842 | 0.486592 | 0.836071 | 13.199572 | 1.102743 | 0.806057 | 49.693429 | . min 1.000000 | 0.000000 | 1.000000 | 0.420000 | 0.000000 | 0.000000 | 0.000000 | . 25% 223.500000 | 0.000000 | 2.000000 | 22.000000 | 0.000000 | 0.000000 | 7.910400 | . 50% 446.000000 | 0.000000 | 3.000000 | 24.000000 | 0.000000 | 0.000000 | 14.454200 | . 75% 668.500000 | 1.000000 | 3.000000 | 35.000000 | 1.000000 | 0.000000 | 31.000000 | . max 891.000000 | 1.000000 | 3.000000 | 80.000000 | 8.000000 | 6.000000 | 512.329200 | . df[&#39;Fare&#39;].hist(); . df[&#39;LogFare&#39;] = np.log1p(df[&#39;Fare&#39;]) . df[&#39;LogFare&#39;].hist(); . pclasses = sorted(df.Pclass.unique()) pclasses . [1, 2, 3] . df = pd.get_dummies(df, columns=[&quot;Sex&quot;,&quot;Pclass&quot;,&quot;Embarked&quot;]) df.columns . Index([&#39;PassengerId&#39;, &#39;Survived&#39;, &#39;Name&#39;, &#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;Ticket&#39;, &#39;Fare&#39;, &#39;Cabin&#39;, &#39;LogFare&#39;, &#39;Sex_female&#39;, &#39;Sex_male&#39;, &#39;Pclass_1&#39;, &#39;Pclass_2&#39;, &#39;Pclass_3&#39;, &#39;Embarked_C&#39;, &#39;Embarked_Q&#39;, &#39;Embarked_S&#39;], dtype=&#39;object&#39;) . added_cols = [&#39;Sex_male&#39;, &#39;Sex_female&#39;, &#39;Pclass_1&#39;, &#39;Pclass_2&#39;, &#39;Pclass_3&#39;, &#39;Embarked_C&#39;, &#39;Embarked_Q&#39;, &#39;Embarked_S&#39;] df[added_cols].head() . Sex_male Sex_female Pclass_1 Pclass_2 Pclass_3 Embarked_C Embarked_Q Embarked_S . 0 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 1 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . 2 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | . 3 0 | 1 | 1 | 0 | 0 | 0 | 0 | 1 | . 4 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . df.Survived . 0 0 1 1 2 1 3 1 4 0 .. 886 0 887 1 888 0 889 1 890 0 Name: Survived, Length: 891, dtype: int64 . from torch import tensor t_dep = tensor(df.Survived) . df . PassengerId Survived Name Age SibSp Parch Ticket Fare Cabin LogFare Sex_female Sex_male Pclass_1 Pclass_2 Pclass_3 Embarked_C Embarked_Q Embarked_S . 0 1 | 0 | Braund, Mr. Owen Harris | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | B96 B98 | 2.110213 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | . 1 2 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | 38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | 4.280593 | 1 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | . 2 3 | 1 | Heikkinen, Miss. Laina | 26.0 | 0 | 0 | STON/O2. 3101282 | 7.9250 | B96 B98 | 2.188856 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 3 4 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | 35.0 | 1 | 0 | 113803 | 53.1000 | C123 | 3.990834 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 4 5 | 0 | Allen, Mr. William Henry | 35.0 | 0 | 0 | 373450 | 8.0500 | B96 B98 | 2.202765 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 1 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 886 887 | 0 | Montvila, Rev. Juozas | 27.0 | 0 | 0 | 211536 | 13.0000 | B96 B98 | 2.639057 | 0 | 1 | 0 | 1 | 0 | 0 | 0 | 1 | . 887 888 | 1 | Graham, Miss. Margaret Edith | 19.0 | 0 | 0 | 112053 | 30.0000 | B42 | 3.433987 | 1 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | . 888 889 | 0 | Johnston, Miss. Catherine Helen &quot;Carrie&quot; | 24.0 | 1 | 2 | W./C. 6607 | 23.4500 | B96 B98 | 3.196630 | 1 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | . 889 890 | 1 | Behr, Mr. Karl Howell | 26.0 | 0 | 0 | 111369 | 30.0000 | C148 | 3.433987 | 0 | 1 | 1 | 0 | 0 | 1 | 0 | 0 | . 890 891 | 0 | Dooley, Mr. Patrick | 32.0 | 0 | 0 | 370376 | 7.7500 | B96 B98 | 2.169054 | 0 | 1 | 0 | 0 | 1 | 0 | 1 | 0 | . 891 rows × 18 columns . indep_cols = [&#39;Age&#39;, &#39;SibSp&#39;, &#39;Parch&#39;, &#39;LogFare&#39;] + added_cols df[indep_cols].values . array([[22., 1., 0., ..., 0., 0., 1.], [38., 1., 0., ..., 1., 0., 0.], [26., 0., 0., ..., 0., 0., 1.], ..., [24., 1., 2., ..., 0., 0., 1.], [26., 0., 0., ..., 1., 0., 0.], [32., 0., 0., ..., 0., 1., 0.]]) . t_indep = tensor(df[indep_cols].values, dtype=torch.float) t_indep . tensor([[22.0000, 1.0000, 0.0000, 2.1102, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [38.0000, 1.0000, 0.0000, 4.2806, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000], [26.0000, 0.0000, 0.0000, 2.1889, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [35.0000, 1.0000, 0.0000, 3.9908, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [35.0000, 0.0000, 0.0000, 2.2028, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [24.0000, 0.0000, 0.0000, 2.2469, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000], [54.0000, 0.0000, 0.0000, 3.9677, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], ..., [25.0000, 0.0000, 0.0000, 2.0857, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [39.0000, 0.0000, 5.0000, 3.4054, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000], [27.0000, 0.0000, 0.0000, 2.6391, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000], [19.0000, 0.0000, 0.0000, 3.4340, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [24.0000, 1.0000, 2.0000, 3.1966, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [26.0000, 0.0000, 0.0000, 3.4340, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000], [32.0000, 0.0000, 0.0000, 2.1691, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]]) . t_indep.shape . torch.Size([891, 12]) . Linear model . torch.manual_seed(442) n_coeff = t_indep.shape[1] coeffs = torch.rand(n_coeff)-0.5 coeffs . tensor([-0.4629, 0.1386, 0.2409, -0.2262, -0.2632, -0.3147, 0.4876, 0.3136, 0.2799, -0.4392, 0.2103, 0.3625]) . vals,indices = t_indep.max(dim=0) t_indep = t_indep / vals . vals . tensor([80.0000, 8.0000, 6.0000, 6.2409, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000]) . indices . tensor([630, 159, 678, 258, 0, 1, 1, 9, 0, 1, 5, 0]) . t_indep[0] . tensor([0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000]) . t_indep*coeffs . tensor([[-0.1273, 0.0173, 0.0000, -0.0765, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.2199, 0.0173, 0.0000, -0.1551, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.4392, 0.0000, 0.0000], [-0.1504, 0.0000, 0.0000, -0.0793, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.2025, 0.0173, 0.0000, -0.1446, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], [-0.2025, 0.0000, 0.0000, -0.0798, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.1389, 0.0000, 0.0000, -0.0814, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000], [-0.3125, 0.0000, 0.0000, -0.1438, -0.2632, -0.0000, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], ..., [-0.1447, 0.0000, 0.0000, -0.0756, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.2257, 0.0000, 0.2008, -0.1234, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000], [-0.1562, 0.0000, 0.0000, -0.0956, -0.2632, -0.0000, 0.0000, 0.3136, 0.0000, -0.0000, 0.0000, 0.3625], [-0.1099, 0.0000, 0.0000, -0.1244, -0.0000, -0.3147, 0.4876, 0.0000, 0.0000, -0.0000, 0.0000, 0.3625], [-0.1389, 0.0173, 0.0803, -0.1158, -0.0000, -0.3147, 0.0000, 0.0000, 0.2799, -0.0000, 0.0000, 0.3625], [-0.1504, 0.0000, 0.0000, -0.1244, -0.2632, -0.0000, 0.4876, 0.0000, 0.0000, -0.4392, 0.0000, 0.0000], [-0.1852, 0.0000, 0.0000, -0.0786, -0.2632, -0.0000, 0.0000, 0.0000, 0.2799, -0.0000, 0.2103, 0.0000]]) . preds = (t_indep*coeffs).sum(axis=1) . preds[:10] . tensor([ 0.1927, -0.6239, 0.0979, 0.2056, 0.0968, 0.0066, 0.1306, 0.3476, 0.1613, -0.6285]) . loss = torch.abs(preds-t_dep).mean() loss . tensor(0.5382) . t_dep . tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]) . t_dep.shape . torch.Size([891]) . t_indep.shape . torch.Size([891, 12]) . def calc_preds(coeffs, indeps): return (indeps*coeffs).sum(axis=1) def calc_loss(coeffs, indeps, deps): return torch.abs(calc_preds(coeffs, indeps)-deps).mean() . Implementing the gradient descent . coeffs.requires_grad_() . tensor([-0.4629, 0.1386, 0.2409, -0.2262, -0.2632, -0.3147, 0.4876, 0.3136, 0.2799, -0.4392, 0.2103, 0.3625], requires_grad=True) . loss = calc_loss(coeffs, t_indep, t_dep) loss . tensor(0.5382, grad_fn=&lt;MeanBackward0&gt;) . loss.backward() . coeffs.grad . tensor([-0.0106, 0.0129, -0.0041, -0.0484, 0.2099, -0.2132, -0.1212, -0.0247, 0.1425, -0.1886, -0.0191, 0.2043]) . each time we call backward, the gradients are actually added to whatever is in the .grad attribute. If we run it again: . loss = calc_loss(coeffs, t_indep, t_dep) loss.backward() coeffs.grad . tensor([-0.0212, 0.0258, -0.0082, -0.0969, 0.4198, -0.4265, -0.2424, -0.0494, 0.2851, -0.3771, -0.0382, 0.4085]) . our .grad values are have doubled. That&#39;s because it added the gradients a second time. For this reason, after we use the gradients to do a gradient descent step, we need to set them back to zero. . We can now do one gradient descent step, and check that our loss decreases: . loss = calc_loss(coeffs, t_indep, t_dep) loss.backward() with torch.no_grad(): coeffs.sub_(coeffs.grad * 0.1) coeffs.grad.zero_() print(calc_loss(coeffs, t_indep, t_dep)) . tensor(0.4945) . Training the linear model We could use ether fast.ai api or scikitlearns api to split the data set . !pip install fastbook import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * import torch.nn.functional as F . Requirement already satisfied: fastbook in /usr/local/lib/python3.9/dist-packages (0.0.28) Requirement already satisfied: datasets in /usr/local/lib/python3.9/dist-packages (from fastbook) (2.3.2) Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from fastbook) (0.20.1) Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from fastbook) (2.28.1) Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from fastbook) (1.4.3) Requirement already satisfied: sentencepiece in /usr/local/lib/python3.9/dist-packages (from fastbook) (0.1.96) Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (from fastbook) (22.2.2) Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from fastbook) (21.3) Requirement already satisfied: fastai&gt;=2.6 in /usr/local/lib/python3.9/dist-packages (from fastbook) (2.7.9) Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (from fastbook) (4.20.1) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.1.1) Requirement already satisfied: pillow&gt;6.0.0 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (9.2.0) Requirement already satisfied: torchvision&gt;=0.8.2 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (0.13.0+cu116) Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (5.4.1) Requirement already satisfied: spacy&lt;4 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (3.4.0) Requirement already satisfied: fastcore&lt;1.6,&gt;=1.4.5 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.27) Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7) Requirement already satisfied: torch&lt;1.14,&gt;=1.7 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.12.0+cu116) Requirement already satisfied: fastprogress&gt;=0.2.4 in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (3.5.2) Requirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from fastai&gt;=2.6-&gt;fastbook) (1.8.1) Requirement already satisfied: numpy&gt;=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (1.23.1) Requirement already satisfied: aiohttp in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (3.8.1) Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.1.0 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.8.1) Requirement already satisfied: xxhash in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (3.0.0) Requirement already satisfied: pyarrow&gt;=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (8.0.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (4.64.0) Requirement already satisfied: dill&lt;0.3.6 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.3.5.1) Requirement already satisfied: responses&lt;0.19 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.18.0) Requirement already satisfied: multiprocess in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (0.70.13) Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /usr/local/lib/python3.9/dist-packages (from datasets-&gt;fastbook) (2022.5.0) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /usr/lib/python3/dist-packages (from requests-&gt;fastbook) (2.8) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/lib/python3/dist-packages (from requests-&gt;fastbook) (2019.11.28) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;fastbook) (2.1.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests-&gt;fastbook) (1.26.10) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /usr/local/lib/python3.9/dist-packages (from packaging-&gt;fastbook) (3.0.9) Requirement already satisfied: pytz&gt;=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas-&gt;fastbook) (2022.1) Requirement already satisfied: python-dateutil&gt;=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas-&gt;fastbook) (2.8.2) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers-&gt;fastbook) (0.12.1) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers-&gt;fastbook) (2022.7.9) Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers-&gt;fastbook) (3.7.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets-&gt;fastbook) (4.3.0) Requirement already satisfied: six&gt;=1.5 in /usr/lib/python3/dist-packages (from python-dateutil&gt;=2.8.1-&gt;pandas-&gt;fastbook) (1.14.0) Requirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.1.0) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.6) Requirement already satisfied: pathy&gt;=0.3.5 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.2) Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.2) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.7) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.9 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.9) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.10.0,&gt;=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.9.1) Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.6) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.9.1) Requirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.4.2) Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (63.1.0) Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.3) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.7) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.7.2) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (18.2.0) Requirement already satisfied: frozenlist&gt;=1.1.1 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2) Requirement already satisfied: aiosignal&gt;=1.1.2 in /usr/local/lib/python3.9/dist-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.2.0) Requirement already satisfied: cycler&gt;=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0) Requirement already satisfied: fonttools&gt;=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.34.4) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.3) Requirement already satisfied: joblib&gt;=1.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.0) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.2.1 in /usr/local/lib/python3.9/dist-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (5.2.1) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.8) Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.1.3) Requirement already satisfied: MarkupSafe&gt;=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.1) WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv . from fastai.data.transforms import RandomSplitter trn_split,val_split=RandomSplitter(seed=42)(df) . t_indep . tensor([[0.2750, 0.1250, 0.0000, 0.3381, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [0.4750, 0.1250, 0.0000, 0.6859, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000], [0.3250, 0.0000, 0.0000, 0.3507, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [0.4375, 0.1250, 0.0000, 0.6395, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [0.4375, 0.0000, 0.0000, 0.3530, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [0.3000, 0.0000, 0.0000, 0.3600, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000], [0.6750, 0.0000, 0.0000, 0.6358, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], ..., [0.3125, 0.0000, 0.0000, 0.3342, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [0.4875, 0.0000, 0.8333, 0.5456, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000], [0.3375, 0.0000, 0.0000, 0.4229, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000], [0.2375, 0.0000, 0.0000, 0.5502, 0.0000, 1.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000], [0.3000, 0.1250, 0.3333, 0.5122, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000], [0.3250, 0.0000, 0.0000, 0.5502, 1.0000, 0.0000, 1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000], [0.4000, 0.0000, 0.0000, 0.3476, 1.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000]]) . trn_split . (#713) [788,525,821,253,374,98,215,313,281,305...] . trn_indep,val_indep = t_indep[trn_split],t_indep[val_split] trn_dep,val_dep = t_dep[trn_split],t_dep[val_split] len(trn_indep),len(val_indep) . (713, 178) . len(trn_dep), len(val_dep) . (713, 178) . def update_coeffs(coeffs, lr): coeffs.sub_(coeffs.grad * lr) coeffs.grad.zero_() . def one_epoch(coeffs, lr): loss = calc_loss(coeffs, trn_indep, trn_dep) loss.backward() with torch.no_grad(): update_coeffs(coeffs, lr) print(f&quot;{loss:.3f}&quot;, end=&quot;; &quot;) . def init_coeffs(): return (torch.rand(n_coeff)-0.5).requires_grad_() . def train_model(epochs=30, lr=0.01): torch.manual_seed(442) coeffs = init_coeffs() for i in range(epochs): one_epoch(coeffs, lr=lr) return coeffs . coeffs = train_model(18, lr=0.2) . 0.536; 0.502; 0.477; 0.454; 0.431; 0.409; 0.388; 0.367; 0.349; 0.336; 0.330; 0.326; 0.329; 0.304; 0.314; 0.296; 0.300; 0.289; . def show_coeffs(): return dict(zip(indep_cols, coeffs.requires_grad_(False))) show_coeffs() . {&#39;Age&#39;: tensor(-0.2694), &#39;SibSp&#39;: tensor(0.0901), &#39;Parch&#39;: tensor(0.2359), &#39;LogFare&#39;: tensor(0.0280), &#39;Sex_male&#39;: tensor(-0.3990), &#39;Sex_female&#39;: tensor(0.2345), &#39;Pclass_1&#39;: tensor(0.7232), &#39;Pclass_2&#39;: tensor(0.4112), &#39;Pclass_3&#39;: tensor(0.3601), &#39;Embarked_C&#39;: tensor(0.0955), &#39;Embarked_Q&#39;: tensor(0.2395), &#39;Embarked_S&#39;: tensor(0.2122)} . preds = calc_preds(coeffs, val_indep) . We&#39;ll assume that any passenger with a score of over 0.5 is predicted to survive. So that means we&#39;re correct for each row where preds&gt;0.5 is the same as the dependent variable: . results = val_dep.bool()==(preds&gt;0.5) results[:16] . tensor([ True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, False]) . results.float().mean() . tensor(0.7865) . val_dep.bool() . tensor([ True, False, False, False, False, False, True, True, False, True, True, True, True, True, False, True, False, True, False, True, False, False, True, True, False, False, True, False, False, True, True, False, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, True, False, False, False, False, False, False, False, True, False, False, True, True, False, False, True, False, False, True, True, False, False, False, True, False, True, False, False, True, True, False, False, True, False, False, True, True, True, True, True, False, True, False, True, True, False, True, False, False, False, True, False, True, False, True, False, False, False, True, False, False, True, False, True, True, True, True, False, False, False, True, False, True, False, False, False, True, False, True, True, False, False, False, True, False, True, False, False, False, True, False, True, False, False, True, True, True, True, True, False, False, False, False, True, False, True, True, True, True]) . val_dep . tensor([1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1]) . def acc(coeffs): return (val_dep.bool()==(calc_preds(coeffs, val_indep)&gt;0.5)).float().mean() acc(coeffs) . tensor(0.7865) . Using sigmoid to keep preds between 0 and 1 . def calc_preds(coeffs, indeps): return torch.sigmoid((indeps*coeffs).sum(axis=1)) . coeffs = train_model(lr=100) . 0.510; 0.327; 0.294; 0.207; 0.201; 0.199; 0.198; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; . check the accuracy . acc(coeffs) . tensor(0.8258) . show_coeffs() . {&#39;Age&#39;: tensor(-1.5061), &#39;SibSp&#39;: tensor(-1.1575), &#39;Parch&#39;: tensor(-0.4267), &#39;LogFare&#39;: tensor(0.2543), &#39;Sex_male&#39;: tensor(-10.3320), &#39;Sex_female&#39;: tensor(8.4185), &#39;Pclass_1&#39;: tensor(3.8389), &#39;Pclass_2&#39;: tensor(2.1398), &#39;Pclass_3&#39;: tensor(-6.2331), &#39;Embarked_C&#39;: tensor(1.4771), &#39;Embarked_Q&#39;: tensor(2.1168), &#39;Embarked_S&#39;: tensor(-4.7958)} . Putting it all together towards a submission to kaggle . tst_df = pd.read_csv(path/&#39;test.csv&#39;) tst_df[&#39;Fare&#39;] = tst_df.Fare.fillna(0) tst_df.fillna(modes, inplace=True) tst_df[&#39;LogFare&#39;] = np.log(tst_df[&#39;Fare&#39;]+1) tst_df = pd.get_dummies(tst_df, columns=[&quot;Sex&quot;,&quot;Pclass&quot;,&quot;Embarked&quot;]) tst_indep = tensor(tst_df[indep_cols].values, dtype=torch.float) tst_indep = tst_indep / vals . tst_df[&#39;Survived&#39;] = (calc_preds(tst_indep, coeffs)&gt;0.5).int() . sub_df = tst_df[[&#39;PassengerId&#39;,&#39;Survived&#39;]] sub_df.to_csv(&#39;sub.csv&#39;, index=False) . Using matrix product . val_indep@coeffs . tensor([ 12.3288, -14.8119, -15.4540, -13.1513, -13.3511, -13.6468, 3.6248, 5.3429, -22.0878, 3.1233, -21.8742, -15.6421, -21.5504, 3.9393, -21.9190, -12.0010, -12.3775, 5.3550, -13.5880, -3.1015, -21.7237, -12.2081, 12.9767, 4.7427, -21.6525, -14.9135, -2.7433, -12.3210, -21.5886, 3.9387, 5.3890, -3.6196, -21.6296, -21.8454, 12.2159, -3.2275, -12.0289, 13.4560, -21.7230, -3.1366, -13.2462, -21.7230, -13.6831, 13.3092, -21.6477, -3.5868, -21.6854, -21.8316, -14.8158, -2.9386, -5.3103, -22.2384, -22.1097, -21.7466, -13.3780, -13.4909, -14.8119, -22.0690, -21.6666, -21.7818, -5.4439, -21.7407, -12.6551, -21.6671, 4.9238, -11.5777, -13.3323, -21.9638, -15.3030, 5.0243, -21.7614, 3.1820, -13.4721, -21.7170, -11.6066, -21.5737, -21.7230, -11.9652, -13.2382, -13.7599, -13.2170, 13.1347, -21.7049, -21.7268, 4.9207, -7.3198, -5.3081, 7.1065, 11.4948, -13.3135, -21.8723, -21.7230, 13.3603, -15.5670, 3.4105, -7.2857, -13.7197, 3.6909, 3.9763, -14.7227, -21.8268, 3.9387, -21.8743, -21.8367, -11.8518, -13.6712, -21.8299, 4.9440, -5.4471, -21.9666, 5.1333, -3.2187, -11.6008, 13.7920, -21.7230, 12.6369, -3.7268, -14.8119, -22.0637, 12.9468, -22.1610, -6.1827, -14.8119, -3.2838, -15.4540, -11.6950, -2.9926, -3.0110, -21.5664, -13.8268, 7.3426, -21.8418, 5.0744, 5.2582, 13.3415, -21.6289, -13.9898, -21.8112, -7.3316, 5.2296, -13.4453, 12.7891, -22.1235, -14.9625, -3.4339, 6.3089, -21.9839, 3.1968, 7.2400, 2.8558, -3.1187, 3.7965, 5.4667, -15.1101, -15.0597, -22.9391, -21.7230, -3.0346, -13.5206, -21.7011, 13.4425, -7.2690, -21.8335, -12.0582, 13.0489, 6.7993, 5.2160, 5.0794, -12.6957, -12.1838, -3.0873, -21.6070, 7.0744, -21.7170, -22.1001, 6.8159, -11.6002, -21.6310]) . def calc_preds(coeffs, indeps): return torch.sigmoid(indeps@coeffs) . def init_coeffs(): return (torch.rand(n_coeff, 1)*0.1).requires_grad_() . trn_dep . tensor([1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0]) . trn_dep = trn_dep[:,None] val_dep = val_dep[:,None] . trn_dep.shape . torch.Size([713, 1]) . coeffs = train_model(lr=100) . 0.512; 0.323; 0.290; 0.205; 0.200; 0.198; 0.197; 0.197; 0.196; 0.196; 0.196; 0.195; 0.195; 0.195; 0.195; 0.195; 0.195; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; 0.194; . acc(coeffs) . tensor(0.8258) . Neural Network implementation . def init_coeffs(n_hidden=20): layer1 = (torch.rand(n_coeff, n_hidden)-0.5)/n_hidden layer2 = torch.rand(n_hidden, 1)-0.3 const = torch.rand(1)[0] return layer1.requires_grad_(),layer2.requires_grad_(),const.requires_grad_() . import torch.nn.functional as F def calc_preds(coeffs, indeps): l1,l2,const = coeffs res = F.relu(indeps@l1) res = res@l2 + const return torch.sigmoid(res) . def update_coeffs(coeffs, lr): for layer in coeffs: layer.sub_(layer.grad * lr) layer.grad.zero_() . coeffs = train_model(lr=1.4) . 0.543; 0.532; 0.520; 0.505; 0.487; 0.466; 0.439; 0.407; 0.373; 0.343; 0.319; 0.301; 0.286; 0.274; 0.264; 0.256; 0.250; 0.245; 0.240; 0.237; 0.234; 0.231; 0.229; 0.227; 0.226; 0.224; 0.223; 0.222; 0.221; 0.220; . coeffs = train_model(lr=20) . 0.543; 0.400; 0.260; 0.390; 0.221; 0.211; 0.197; 0.195; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.193; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; 0.192; . acc(coeffs) . tensor(0.8258) . Deep learning implementation . def init_coeffs(): hiddens = [10, 10] # &lt;-- set this to the size of each hidden layer you want sizes = [n_coeff] + hiddens + [1] n = len(sizes) layers = [(torch.rand(sizes[i], sizes[i+1])-0.3)/sizes[i+1]*4 for i in range(n-1)] consts = [(torch.rand(1)[0]-0.5)*0.1 for i in range(n-1)] for l in layers+consts: l.requires_grad_() return layers,consts . import torch.nn.functional as F def calc_preds(coeffs, indeps): layers,consts = coeffs n = len(layers) res = indeps for i,l in enumerate(layers): res = res@l + consts[i] if i!=n-1: res = F.relu(res) #activation fct return torch.sigmoid(res) #activation fct . def update_coeffs(coeffs, lr): layers,consts = coeffs for layer in layers+consts: layer.sub_(layer.grad * lr) layer.grad.zero_() . coeffs = train_model(lr=4) . 0.521; 0.483; 0.427; 0.379; 0.379; 0.379; 0.379; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.378; 0.377; 0.376; 0.371; 0.333; 0.239; 0.224; 0.208; 0.204; 0.203; 0.203; 0.207; 0.197; 0.196; 0.195; . acc(coeffs) . tensor(0.8258) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/27/neuralnet-revisited.html",
            "relUrl": "/fastpages/jupyter/2022/09/27/neuralnet-revisited.html",
            "date": " • Sep 27, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "NLP Transformers model",
            "content": "This notebook runs through Paperspace&#39;s GPU. . from pathlib import Path cred_path = Path(&#39;~/.kaggle/kaggle.json&#39;).expanduser() if not cred_path.exists(): cred_path.parent.mkdir(exist_ok=True) cred_path.write_text(creds) cred_path.chmod(0o600) . path = Path(&#39;us-patent-phrase-to-phrase-matching&#39;) . if not iskaggle and not path.exists(): import zipfile,kaggle kaggle.api.competition_download_cli(str(path)) zipfile.ZipFile(f&#39;{path}.zip&#39;).extractall(path) . Downloading us-patent-phrase-to-phrase-matching.zip to /notebooks . 100%|██████████| 682k/682k [00:00&lt;00:00, 27.8MB/s] . . . !ls {path} . sample_submission.csv test.csv train.csv . import pandas as pd df = pd.read_csv(path/&#39;train.csv&#39;) df[&#39;input&#39;] = &#39;TEXT1: &#39; + df.context + &#39;; TEXT2: &#39; + df.target + &#39;; ANC1: &#39; + df.anchor . df.input.head() . 0 TEXT1: A47; TEXT2: abatement of pollution; ANC... 1 TEXT1: A47; TEXT2: act of abating; ANC1: abate... 2 TEXT1: A47; TEXT2: active catalyst; ANC1: abat... 3 TEXT1: A47; TEXT2: eliminating process; ANC1: ... 4 TEXT1: A47; TEXT2: forest region; ANC1: abatement Name: input, dtype: object . from datasets import Dataset,DatasetDict ds = Dataset.from_pandas(df) . ds . Dataset({ features: [&#39;id&#39;, &#39;anchor&#39;, &#39;target&#39;, &#39;context&#39;, &#39;score&#39;, &#39;input&#39;], num_rows: 36473 }) . model_nm = &#39;microsoft/deberta-v3-small&#39; from transformers import AutoModelForSequenceClassification,AutoTokenizer tokz = AutoTokenizer.from_pretrained(model_nm) . Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. /usr/local/lib/python3.9/dist-packages/transformers/convert_slow_tokenizer.py:434: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text. warnings.warn( Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained. . tokz.tokenize(&quot;Testing out some lil&#39; odd words such as see ya later, hadn&#39;t seen him, Capitalize, ChAngeItUp&quot;) . [&#39;▁Testing&#39;, &#39;▁out&#39;, &#39;▁some&#39;, &#39;▁lil&#39;, &#34;&#39;&#34;, &#39;▁odd&#39;, &#39;▁words&#39;, &#39;▁such&#39;, &#39;▁as&#39;, &#39;▁see&#39;, &#39;▁ya&#39;, &#39;▁later&#39;, &#39;,&#39;, &#39;▁hadn&#39;, &#34;&#39;&#34;, &#39;t&#39;, &#39;▁seen&#39;, &#39;▁him&#39;, &#39;,&#39;, &#39;▁Capital&#39;, &#39;ize&#39;, &#39;,&#39;, &#39;▁Ch&#39;, &#39;A&#39;, &#39;nge&#39;, &#39;It&#39;, &#39;Up&#39;] . def tok_func(x): return tokz(x[&quot;input&quot;]) tok_ds = ds.map(tok_func, batched=True) . Parameter &#39;function&#39;=&lt;function tok_func at 0x7f84b0ecd820&gt; of the transform datasets.arrow_dataset.Dataset._map_single couldn&#39;t be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won&#39;t be showed. . row = tok_ds[0] row[&#39;input&#39;], row[&#39;input_ids&#39;] . (&#39;TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement&#39;, [1, 54453, 435, 294, 336, 5753, 346, 54453, 445, 294, 47284, 265, 6435, 346, 23702, 435, 294, 47284, 2]) . tok_ds = tok_ds.rename_columns({&#39;score&#39;:&#39;labels&#39;}) . eval_df = pd.read_csv(path/&#39;test.csv&#39;) import numpy as np, matplotlib.pyplot as plt from sklearn.linear_model import LinearRegression from sklearn.preprocessing import PolynomialFeatures from sklearn.pipeline import make_pipeline dds = tok_ds.train_test_split(0.25, seed=42) dds . huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either: - Avoid using `tokenizers` before the fork if possible - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false) huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks... To disable this warning, you can either: - Avoid using `tokenizers` before the fork if possible - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false) . DatasetDict({ train: Dataset({ features: [&#39;id&#39;, &#39;anchor&#39;, &#39;target&#39;, &#39;context&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 27354 }) test: Dataset({ features: [&#39;id&#39;, &#39;anchor&#39;, &#39;target&#39;, &#39;context&#39;, &#39;labels&#39;, &#39;input&#39;, &#39;input_ids&#39;, &#39;token_type_ids&#39;, &#39;attention_mask&#39;], num_rows: 9119 }) }) . eval_df[&#39;input&#39;] = &#39;TEXT1: &#39; + eval_df.context + &#39;; TEXT2: &#39; + eval_df.target + &#39;; ANC1: &#39; + eval_df.anchor eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True) . def corr(x,y): return np.corrcoef(x,y)[0][1] . def show_corr(df, a, b): x,y = df[a],df[b] plt.scatter(x,y, alpha=0.5, s=4) plt.title(f&#39;{a} vs {b}; r: {corr(x, y):.2f}&#39;) . from transformers import TrainingArguments,Trainer bs = 128 epochs = 4 . lr = 8e-5 . args = TrainingArguments(&#39;outputs&#39;, learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type=&#39;cosine&#39;, fp16=True, evaluation_strategy=&quot;epoch&quot;, per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2, num_train_epochs=epochs, weight_decay=0.01, report_to=&#39;none&#39;) . def corr_d(eval_pred): return {&#39;pearson&#39;: corr(*eval_pred)} . model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1) trainer = Trainer(model, args, train_dataset=dds[&#39;train&#39;], eval_dataset=dds[&#39;test&#39;], tokenizer=tokz, compute_metrics=corr_d) . Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: [&#39;lm_predictions.lm_head.LayerNorm.weight&#39;, &#39;mask_predictions.classifier.weight&#39;, &#39;mask_predictions.LayerNorm.weight&#39;, &#39;mask_predictions.dense.bias&#39;, &#39;lm_predictions.lm_head.bias&#39;, &#39;mask_predictions.LayerNorm.bias&#39;, &#39;lm_predictions.lm_head.dense.weight&#39;, &#39;lm_predictions.lm_head.dense.bias&#39;, &#39;mask_predictions.dense.weight&#39;, &#39;mask_predictions.classifier.bias&#39;, &#39;lm_predictions.lm_head.LayerNorm.bias&#39;] - This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model). - This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model). Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: [&#39;pooler.dense.bias&#39;, &#39;pooler.dense.weight&#39;, &#39;classifier.weight&#39;, &#39;classifier.bias&#39;] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference. Using cuda_amp half precision backend . trainer.train(); . The following columns in the training set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: anchor, input, id, target, context. If anchor, input, id, target, context are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. /usr/local/lib/python3.9/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning warnings.warn( ***** Running training ***** Num examples = 27354 Num Epochs = 4 Instantaneous batch size per device = 128 Total train batch size (w. parallel, distributed &amp; accumulation) = 128 Gradient Accumulation steps = 1 Total optimization steps = 856 . . [856/856 15:18, Epoch 4/4] Epoch Training Loss Validation Loss Pearson . 1 | No log | 0.026263 | 0.798741 | . 2 | No log | 0.025944 | 0.823264 | . 3 | 0.034800 | 0.022987 | 0.833240 | . 4 | 0.034800 | 0.021924 | 0.833892 | . &lt;/div&gt; &lt;/div&gt; The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: anchor, input, id, target, context. If anchor, input, id, target, context are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: anchor, input, id, target, context. If anchor, input, id, target, context are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 Saving model checkpoint to outputs/checkpoint-500 Configuration saved in outputs/checkpoint-500/config.json Model weights saved in outputs/checkpoint-500/pytorch_model.bin tokenizer config file saved in outputs/checkpoint-500/tokenizer_config.json Special tokens file saved in outputs/checkpoint-500/special_tokens_map.json The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: anchor, input, id, target, context. If anchor, input, id, target, context are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 The following columns in the evaluation set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: anchor, input, id, target, context. If anchor, input, id, target, context are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Evaluation ***** Num examples = 9119 Batch size = 256 Training completed. Do not forget to share your model on huggingface.co/models =) . &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; preds = trainer.predict(eval_ds).predictions.astype(float) preds . The following columns in the test set don&#39;t have a corresponding argument in `DebertaV2ForSequenceClassification.forward` and have been ignored: anchor, id, input, target, context. If anchor, id, input, target, context are not expected by `DebertaV2ForSequenceClassification.forward`, you can safely ignore this message. ***** Running Prediction ***** Num examples = 36 Batch size = 256 . . [1/1 : &lt; :] array([[ 5.74218750e-01], [ 6.59179688e-01], [ 5.41992188e-01], [ 3.12255859e-01], [-3.18908691e-02], [ 5.43945312e-01], [ 5.07324219e-01], [ 7.92694092e-03], [ 2.51464844e-01], [ 1.04882812e+00], [ 3.00537109e-01], [ 2.63671875e-01], [ 7.12402344e-01], [ 8.55957031e-01], [ 7.36816406e-01], [ 4.27490234e-01], [ 2.95166016e-01], [-6.78062439e-04], [ 6.18164062e-01], [ 3.39843750e-01], [ 4.55566406e-01], [ 2.38769531e-01], [ 9.44213867e-02], [ 2.19604492e-01], [ 5.26855469e-01], [-2.81066895e-02], [-4.91638184e-02], [-2.97546387e-02], [-4.06188965e-02], [ 5.79589844e-01], [ 3.13232422e-01], [ 1.97219849e-03], [ 8.07617188e-01], [ 4.92431641e-01], [ 4.26513672e-01], [ 2.25585938e-01]]) . preds = np.clip(preds, 0, 1) preds . array([[0.57421875], [0.65917969], [0.54199219], [0.31225586], [0. ], [0.54394531], [0.50732422], [0.00792694], [0.25146484], [1. ], [0.30053711], [0.26367188], [0.71240234], [0.85595703], [0.73681641], [0.42749023], [0.29516602], [0. ], [0.61816406], [0.33984375], [0.45556641], [0.23876953], [0.09442139], [0.21960449], [0.52685547], [0. ], [0. ], [0. ], [0. ], [0.57958984], [0.31323242], [0.0019722 ], [0.80761719], [0.49243164], [0.42651367], [0.22558594]]) . &lt;/div&gt; .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/26/NLP-transformers-model.html",
            "relUrl": "/fastpages/jupyter/2022/09/26/NLP-transformers-model.html",
            "date": " • Sep 26, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Regression Model - Clean",
            "content": "import pandas as pd import numpy as np # uncomment and import modules # pip install fastbook # import fastbook # fastbook.setup_book() # from fastai.vision.all import * # from fastbook import * # import torch.nn.functional as F # to run from your workstation # download the titanic survival data set : train.csv import os path = os.getcwd() # df = pd.read_csv(f&quot;{path}/YOUR_FILE_LOCATION/train.csv&quot;) . df = pd.read_csv(&#39;/Users/ericvincent/Desktop/train.csv&#39;) def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() def sigmoid(x): return 1/(1+torch.exp(-x)) def survive_loss_updated(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . class FormatDataframe : def __init__(self, df: pd.DataFrame): self.df = df def splitData(self): twenty_percent_df = df twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Sex&#39;] twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:0}) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0, &#39;Q&#39;:0}) twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass1&#39;].replace({2:0, 3:0}) twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;Pclass&#39;]) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].fillna(0) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].fillna(0) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;PassengerId&#39;, &#39;Ticket&#39;]) eighty_percent_df = twenty_percent_df.iloc[180:] twenty_percent_df = twenty_percent_df.iloc[:179] return eighty_percent_df, twenty_percent_df def createTensors(self, dfName): # return labels survived_label_train = dfName[&#39;Survived&#39;] == 1 death_label_train = dfName[&#39;Survived&#39;] == 0 # creating tensors survived_df = dfName.loc[survived_label_train] death_df = dfName.loc[death_label_train] stacked_survived = [tensor(survived_df.iloc[num]) for num in range(len(survived_df))] stacked_death = [tensor(death_df.iloc[num]) for num in range(len(death_df))] survive_tensors_stacked = torch.stack(stacked_survived).float() death_tensors_stacked = torch.stack(stacked_death).float() return survive_tensors_stacked, death_tensors_stacked . if __name__ == &quot;__main__&quot; : model = FormatDataframe(df) train, validation = model.splitData() survive_tensors_stacked_train, death_tensors_stacked_train = model.createTensors(train) survive_tensors_stacked_validation, death_tensors_stacked_validation = model.createTensors(validation) # labels on 80% of data label_df = FormatDataframe(df) eighty_percent_labels,_ = label_df.splitData() survived_label = eighty_percent_labels[&#39;Survived&#39;] == 1 death_label = eighty_percent_labels[&#39;Survived&#39;] == 0 survived = eighty_percent_labels.loc[survived_label] death = eighty_percent_labels.loc[death_label] # create training dl train_x = torch.cat([survive_tensors_stacked_train, death_tensors_stacked_train]).view(-1, 8) train_y = tensor([1]*len(survive_tensors_stacked_train) + [0]*len(death_tensors_stacked_train)).unsqueeze(1) dset = list(zip(train_x,train_y)) dl = DataLoader(dset, batch_size=8) # create validation dl valid_x = torch.cat([survive_tensors_stacked_validation, death_tensors_stacked_validation]).view(-1, 8) valid_y = tensor([1]*len(survive_tensors_stacked_validation) + [0]*len(death_tensors_stacked_validation)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=8) # finally dls = DataLoaders(dl, valid_dl) # neural net simple_net = nn.Sequential( nn.Linear(8,1), nn.ReLU(), nn.Linear(1,8) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(40, 0.1) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/23/Neural-Net-Model-clean.html",
            "relUrl": "/fastpages/jupyter/2022/09/23/Neural-Net-Model-clean.html",
            "date": " • Sep 23, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Regression Model - Draft",
            "content": "import pandas as pd import numpy as np . df = pd.read_csv(&#39;/kaggle/input/titanic-survival-dataset/train.csv&#39;) . df = df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;Ticket&#39;, &#39;PassengerId&#39;]) . df[&#39;Male&#39;] = df[&#39;Sex&#39;] . df[&#39;Male&#39;] = df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) . df[&#39;Embarked_C&#39;] = df[&#39;Embarked&#39;] df[&#39;Embarked_C&#39;] = df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1}) df[&#39;Embarked_S&#39;] = df[&#39;Embarked&#39;] df[&#39;Embarked_S&#39;] = df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0}) df[&#39;Pclass1&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass2&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass3&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass1&#39;] = df[&#39;Pclass1&#39;].replace({2:0, 3:0}) df[&#39;Pclass2&#39;] = df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) df[&#39;Pclass3&#39;] = df[&#39;Pclass3&#39;].replace({1:0, 2:0, 3:1}) . random_parameters = np.random.rand(1,10) . Sibsp, Parch, Age, log_fare, Pclass1, Pclass2, EmbarkS, EmbarkC, Male, Const = [num for num in random_parameters[0]] . maxAge, maxFare = max(df[&#39;Age&#39;]), max(df[&#39;Fare&#39;]) . df[&#39;Age_N&#39;] = df[&#39;Age&#39;] / maxAge . df[&#39;log_Fare&#39;] = df[&#39;Fare&#39;] / maxFare . df[&#39;log_Fare&#39;] = np.log10(df[&#39;log_Fare&#39;] + 1) . parameters = { Sibsp, Parch, Age, log_fare, Pclass1, Pclass2, EmbarkS, EmbarkC, Male, Const } . df[&#39;Ones&#39;] = 1 . parameters = np.array([0.023121551427445874, 0.13402581877095354, 0.1460823666152794, 0.22088891772092012, 0.594795670221624, 0.7961562768917094, 0.8530100056367039, 0.8613248152275507, 0.8901214376590094, 0.9785782868782011]) . . model_df = df . model_df = model_df.drop(columns=[&#39;Survived&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]) . model_df = model_df.drop(columns=[&#39;Pclass&#39;]) . model_df[&#39;Age_N&#39;] = model_df[&#39;Age_N&#39;].fillna(0) . model_df.isnull().values.any() . True . model_df = model_df.drop(columns=[&#39;Pclass3&#39;]) . model_df[&#39;Embarked_S&#39;] = model_df[&#39;Embarked_S&#39;].replace({&#39;Q&#39;: 0}) model_df[&#39;Embarked_C&#39;] = model_df[&#39;Embarked_C&#39;].replace({&#39;Q&#39;: 0}) model_df[&#39;Linear&#39;] = model_df.dot(parameters) . model_df[&#39;Survived&#39;] = df[&#39;Survived&#39;] . model_df[&#39;Loss&#39;] = (model_df[&#39;Linear&#39;] - 1)**2 . model_df[&#39;Linear&#39;].mean() . 2.255390616421957 . survived_label = model_df[&#39;Survived&#39;] == 1 death_label = model_df[&#39;Survived&#39;] == 0 . survived = model_df.loc[survived_label] death = model_df.loc[death_label] . import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . death . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 0.006103 | 1 | 1.984874 | 0 | 0.969977 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 0.006771 | 1 | 2.102313 | 0 | 1.215093 | . 5 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.0000 | 0.007111 | 1 | 1.130991 | 0 | 0.017159 | . 6 0 | 0 | 1 | 0.0 | 1.0 | 1 | 0 | 0.6750 | 0.041878 | 1 | 3.134283 | 0 | 4.555164 | . 7 3 | 1 | 1 | 0.0 | 1.0 | 0 | 0 | 0.0250 | 0.017507 | 1 | 1.959964 | 0 | 0.921530 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 884 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.3125 | 0.005935 | 1 | 1.993904 | 0 | 0.987844 | . 885 0 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0.4875 | 0.024013 | 1 | 2.089977 | 0 | 1.188051 | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 0.010882 | 1 | 2.872850 | 0 | 3.507568 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 0.019437 | 1 | 1.881848 | 0 | 0.777656 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 0.006520 | 1 | 1.474995 | 0 | 0.225620 | . 549 rows × 13 columns . survived . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 8 0 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3375 | 0.009336 | 1 | 2.140433 | 1 | 1.300588 | . 9 1 | 0 | 0 | 1.0 | 0.0 | 0 | 1 | 0.1750 | 0.024771 | 1 | 2.248379 | 1 | 1.558451 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 875 0 | 0 | 0 | 1.0 | 0.0 | 0 | 0 | 0.1875 | 0.006082 | 1 | 1.366379 | 1 | 0.134234 | . 879 0 | 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.7000 | 0.065324 | 1 | 2.790723 | 1 | 3.206687 | . 880 0 | 1 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3125 | 0.021499 | 1 | 2.848710 | 1 | 3.417730 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 342 rows × 13 columns . a = np.array([survived.iloc[0]]) surv = tensor(a) surv . tensor([[1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.4750, 0.0566, 1.0000, 2.4782, 1.0000, 2.1852]]) . b = tensor([survived.iloc[0]]) b # TODO list comprehension for the stacked tensor containing all the rows, the result should be #rows, yaxis, xaxis # do it on the copy df . tensor([[1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.4750, 0.0566, 1.0000, 2.4782, 1.0000, 2.1852]]) . survived . . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 8 0 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3375 | 0.009336 | 1 | 2.140433 | 1 | 1.300588 | . 9 1 | 0 | 0 | 1.0 | 0.0 | 0 | 1 | 0.1750 | 0.024771 | 1 | 2.248379 | 1 | 1.558451 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 875 0 | 0 | 0 | 1.0 | 0.0 | 0 | 0 | 0.1875 | 0.006082 | 1 | 1.366379 | 1 | 0.134234 | . 879 0 | 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.7000 | 0.065324 | 1 | 2.790723 | 1 | 3.206687 | . 880 0 | 1 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3125 | 0.021499 | 1 | 2.848710 | 1 | 3.417730 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 342 rows × 13 columns . survived_tensor . tensor([[1.0000, 0.0000, 0.0000, ..., 2.4782, 1.0000, 2.1852], [0.0000, 0.0000, 0.0000, ..., 1.8592, 1.0000, 0.7383], [1.0000, 0.0000, 0.0000, ..., 2.8076, 1.0000, 3.2674], ..., [0.0000, 1.0000, 0.0000, ..., 2.8487, 1.0000, 3.4177], [0.0000, 0.0000, 0.0000, ..., 2.5961, 1.0000, 2.5475], [0.0000, 0.0000, 1.0000, ..., 2.4436, 1.0000, 2.0841]]) . len(survived_tensor), len(death_tensor) . (342, 549) . num = range(len(survived)) stacked_survived = [tensor(survived.iloc[num]) for num in num] . copydf = model_df . copydf = copydf.drop(columns=[&#39;Age_N&#39;, &#39;log_Fare&#39;, &#39;Ones&#39;, &#39;Linear&#39;, &#39;Loss&#39;]) . copydf[&#39;Embarked_C&#39;].mean(), copydf[&#39;Embarked_S&#39;].mean() . (0.1889763779527559, 0.7244094488188977) . r = [tensor(death.iloc[num]) for num in range(len(death))] rStacked = torch.stack(r).float() rStacked.shape # this makes sense, 549 rows, single vector with (1x13rows) vs they had a rank 3 tensor . torch.Size([549, 13]) . copydf[&#39;Embarked_S&#39;] = copydf[&#39;Embarked_S&#39;].fillna(0) copydf[&#39;Embarked_C&#39;] = copydf[&#39;Embarked_C&#39;].fillna(0) copydf . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Survived . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 1 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 1 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 1 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 1 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 1 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | . 891 rows × 8 columns . survived_label = copydf[&#39;Survived&#39;] == 1 death_label = copydf[&#39;Survived&#39;] == 0 survived = copydf.loc[survived_label] # survived[&#39;Embarked_S&#39;] = survived[&#39;Embarked_S&#39;].fillna(0) # survived[&#39;Embarked_C&#39;] = survived[&#39;Embarked_C&#39;].fillna(0) death = copydf.loc[death_label] # num = range(len(survived)) # nums = range(len(death)) stacked_survived = [tensor(survived.iloc[num]) for num in range(len(survived))] stacked_death = [tensor(death.iloc[num]) for num in range(len(death))] # stacked_survived.shape, stacked_death.shape len(stacked_death), len(stacked_survived) . (549, 342) . # Divide by the number of deaths of survivors in each label? survive_tensors_stacked = torch.stack(stacked_survived).float() death_tensors_stacked = torch.stack(stacked_death).float() # mean_survived = stacked_death.mean(0) # mean_death = stacked_survived.mean(0) . survive_tensors_stacked.shape, death_tensors_stacked.shape # i.e. 2 x rank 2 tensors with each 342 &amp; 549 labels respectively, consisting of 8 rows each . (torch.Size([342, 8]), torch.Size([549, 8])) . mean_survived = survive_tensors_stacked.mean(0) mean_death = death_tensors_stacked.mean(0) . mean_survived,mean_death # embarked rows are Nan - due to fillna? -- no more . (tensor([0.4737, 0.4649, 0.3187, 0.2719, 0.6345, 0.3977, 0.2544, 1.0000]), tensor([0.5537, 0.3297, 0.8525, 0.1366, 0.7778, 0.1457, 0.1767, 0.0000])) . single_survivor = survive_tensors_stacked[1] single_death = death_tensors_stacked[1] single_death, single_survivor . (tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 1.])) . import torch.nn.functional as F . F.l1_loss(single_survivor.float(),mean_death), F.mse_loss(single_survivor,mean_death).sqrt() . (tensor(0.4271), tensor(0.5318)) . F.l1_loss(single_survivor.float(),mean_survived), F.mse_loss(single_survivor,mean_survived).sqrt() # need RElu given that data contains many 0? -- &gt; nan output # Update - both values are smaller than the distance between the survivor and the mean of the death . (tensor(0.3183), tensor(0.3487)) . def survive_distance(a,b): return (a-b).abs().mean() # distance_all_survived = survive_distance(survive_tensors_stacked, mean_survived) # distance_all_survived type(stacked_survived), type(mean_survived) . (list, torch.Tensor) . valid_survivor_dist = survive_distance(survive_tensors_stacked, mean_survived) valid_survivor_dist, valid_survivor_dist.shape . (tensor(0.4210), torch.Size([])) . def is_alive(x): return survive_distance(x,mean_survived) &lt; survive_distance(x,mean_death) . # Update - changed the -1,-2 to simple mean() to fix the index out of range is_alive(single_survivor), is_alive(single_survivor).float # function works, is a valid survivor . (tensor(True), &lt;function Tensor.float&gt;) . is_alive(survive_tensors_stacked) . tensor(True) . accuracy_alive = is_alive(survive_tensors_stacked).float() .mean() accuracy_death = (1 - is_alive(death_tensors_stacked).float()).mean() accuracy_alive,accuracy_death,(accuracy_alive+accuracy_death)/2 . (tensor(1.), tensor(1.), tensor(1.)) . Use Stochastic Gradient Descent to optimize our prediction model . Initialize the weights. | For each row, use these weights to predict whether it appears to be a survivor or not | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until we stop the training process | def f(x): return x**2 # Get a tensor which will requre gradients xt = tensor(3.).requires_grad_() yt = f(xt) yt.backward() xt.grad . tensor(6.) . xt = tensor([3.,4.,10.]).requires_grad_() xt # Add sum to the quadratic function so it can take a vector (rank-1 tensor) and return a scalar (rank-0 tensor) def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad # Implement stepping with a learning rate lr = 1e-5 # w -= gradient(w) * lr . def mse(preds, targets): return ((preds-targets)**2).mean() def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((8,1)) bias = init_params(1) . train_x = torch.cat([survive_tensors_stacked, death_tensors_stacked]).view(-1, 8) (train_x[0]*weights.T).sum() + bias . tensor([-1.1449], grad_fn=&lt;AddBackward0&gt;) . def sigmoid(x): return 1/(1+torch.exp(-x)) def survive_loss_updated(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . train_y = tensor([1]*len(survived) + [0]*len(death)).unsqueeze(1) # dset = dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y # UPDATE DL SO THAT XB SHAPE AND YB SHAPE REFLECT WHAT WE WANT dl = DataLoader(dset, batch_size=8) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([8, 8]), torch.Size([8, 1])) . twenty_percent_df = pd.read_csv(&#39;/kaggle/input/titanic-survival-dataset/train.csv&#39;) twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Sex&#39;] twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:0}) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0, &#39;Q&#39;:0}) twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass1&#39;].replace({2:0, 3:0}) twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;Pclass&#39;]) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].fillna(0) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].fillna(0) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;PassengerId&#39;, &#39;Ticket&#39;]) eighty_percent_df = twenty_percent_df.iloc[180:] twenty_percent_df = twenty_percent_df.iloc[:179] . #training survived_label_train = eighty_percent_df[&#39;Survived&#39;] == 1 death_label_train = eighty_percent_df[&#39;Survived&#39;] == 0 survived_train = eighty_percent_df.loc[survived_label_train] death_train = eighty_percent_df.loc[death_label_train] stacked_survived_train = [tensor(survived_train.iloc[num]) for num in range(len(survived_train))] stacked_death_train = [tensor(death_train.iloc[num]) for num in range(len(death_train))] #stack survive_tensors_stacked_train = torch.stack(stacked_survived_train).float() death_tensors_stacked_train = torch.stack(stacked_death_train).float() #validation survived_label_validation = twenty_percent_df[&#39;Survived&#39;] == 1 death_label_validation = twenty_percent_df[&#39;Survived&#39;] == 0 survived_validation = twenty_percent_df.loc[survived_label_validation] death_validation = twenty_percent_df.loc[death_label_validation] stacked_survived_validation = [tensor(survived_validation.iloc[num]) for num in range(len(survived))] stacked_death_validation = [tensor(death_validation.iloc[num]) for num in range(len(death))] #stack survive_tensors_stacked_validation = torch.stack(stacked_survived_validation).float() death_tensors_stacked_validation = torch.stack(stacked_death_validation).float() . # create training dl train_x = torch.cat([survive_tensors_stacked_train, death_tensors_stacked_train]).view(-1, 8) train_y = tensor([1]*len(survived) + [0]*len(death)).unsqueeze(1) dset = list(zip(train_x,train_y)) dl = DataLoader(dset, batch_size=8) # create validation dl valid_x = torch.cat([survive_tensors_stacked_validation, death_tensors_stacked_validation]).view(-1, 8) valid_y = tensor([1]*len(survive_tensors_stacked_validation) + [0]*len(death_tensors_stacked_validation)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=8) # finally dls = DataLoaders(dl, valid_dl) def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() # neural net simple_net = nn.Sequential( nn.Linear(8,1), nn.ReLU(), nn.Linear(1,8) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.509653 | 0.502687 | 0.473765 | 00:00 | . 1 | 0.508797 | 0.502065 | 0.477132 | 00:00 | . 2 | 0.507841 | 0.501465 | 0.477273 | 00:00 | . 3 | 0.506854 | 0.500884 | 0.480079 | 00:00 | . 4 | 0.505843 | 0.500287 | 0.480780 | 00:00 | . 5 | 0.504807 | 0.499660 | 0.482183 | 00:00 | . 6 | 0.503729 | 0.498991 | 0.482183 | 00:00 | . 7 | 0.502581 | 0.498265 | 0.483025 | 00:00 | . 8 | 0.501378 | 0.497482 | 0.485690 | 00:00 | . 9 | 0.500111 | 0.496636 | 0.488356 | 00:00 | . 10 | 0.498759 | 0.495647 | 0.488917 | 00:00 | . 11 | 0.496819 | 0.493076 | 0.501403 | 00:00 | . 12 | 0.492483 | 0.488248 | 0.542789 | 00:00 | . 13 | 0.485624 | 0.480793 | 0.586700 | 00:00 | . 14 | 0.475594 | 0.470186 | 0.586841 | 00:00 | . 15 | 0.461520 | 0.455530 | 0.628367 | 00:00 | . 16 | 0.443047 | 0.436796 | 0.638749 | 00:00 | . 17 | 0.420336 | 0.414819 | 0.633979 | 00:00 | . 18 | 0.394467 | 0.390664 | 0.637907 | 00:00 | . 19 | 0.366764 | 0.363788 | 0.644220 | 00:00 | . 20 | 0.336472 | 0.332068 | 0.709877 | 00:00 | . 21 | 0.301700 | 0.296407 | 0.799102 | 00:00 | . 22 | 0.266188 | 0.264302 | 0.799383 | 00:00 | . 23 | 0.236447 | 0.239894 | 0.803030 | 00:00 | . 24 | 0.215284 | 0.221003 | 0.808361 | 00:00 | . 25 | 0.199868 | 0.205571 | 0.830107 | 00:00 | . 26 | 0.187974 | 0.192608 | 0.840348 | 00:00 | . 27 | 0.178303 | 0.181214 | 0.883558 | 00:00 | . 28 | 0.170570 | 0.170600 | 0.889450 | 00:00 | . 29 | 0.163774 | 0.160689 | 0.931257 | 00:00 | . 30 | 0.157617 | 0.151713 | 0.937009 | 00:00 | . 31 | 0.152163 | 0.143193 | 0.944865 | 00:00 | . 32 | 0.147052 | 0.135531 | 0.947952 | 00:00 | . 33 | 0.142345 | 0.128379 | 0.949916 | 00:00 | . 34 | 0.138021 | 0.121739 | 0.951459 | 00:00 | . 35 | 0.134111 | 0.115172 | 0.951740 | 00:00 | . 36 | 0.130328 | 0.109340 | 0.951740 | 00:00 | . 37 | 0.126849 | 0.103617 | 0.951880 | 00:00 | . 38 | 0.123680 | 0.098068 | 0.991583 | 00:00 | . 39 | 0.120625 | 0.093022 | 0.996773 | 00:00 | . validdf = pd.read_csv(&#39;/kaggle/input/test-titanic/test.csv&#39;) validdf[&#39;Male&#39;] = validdf[&#39;Sex&#39;] validdf[&#39;Male&#39;] = validdf[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked&#39;] validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:0}) validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked&#39;] validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0, &#39;Q&#39;:0}) validdf[&#39;Pclass1&#39;] = validdf[&#39;Pclass&#39;] validdf[&#39;Pclass2&#39;] = validdf[&#39;Pclass&#39;] validdf[&#39;Pclass1&#39;] = validdf[&#39;Pclass1&#39;].replace({2:0, 3:0}) validdf[&#39;Pclass2&#39;] = validdf[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].fillna(0) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].fillna(0) validdf . PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Male Embarked_C Embarked_S Pclass1 Pclass2 . 0 892 | 3 | Kelly, Mr. James | male | 34.5 | 0 | 0 | 330911 | 7.8292 | NaN | Q | 1 | 0 | 0 | 0 | 0 | . 1 893 | 3 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0 | 1 | 0 | 363272 | 7.0000 | NaN | S | 0 | 0 | 1 | 0 | 0 | . 2 894 | 2 | Myles, Mr. Thomas Francis | male | 62.0 | 0 | 0 | 240276 | 9.6875 | NaN | Q | 1 | 0 | 0 | 0 | 1 | . 3 895 | 3 | Wirz, Mr. Albert | male | 27.0 | 0 | 0 | 315154 | 8.6625 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 4 896 | 3 | Hirvonen, Mrs. Alexander (Helga E Lindqvist) | female | 22.0 | 1 | 1 | 3101298 | 12.2875 | NaN | S | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 413 1305 | 3 | Spector, Mr. Woolf | male | NaN | 0 | 0 | A.5. 3236 | 8.0500 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 414 1306 | 1 | Oliva y Ocana, Dona. Fermina | female | 39.0 | 0 | 0 | PC 17758 | 108.9000 | C105 | C | 0 | 1 | 0 | 1 | 0 | . 415 1307 | 3 | Saether, Mr. Simon Sivertsen | male | 38.5 | 0 | 0 | SOTON/O.Q. 3101262 | 7.2500 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 416 1308 | 3 | Ware, Mr. Frederick | male | NaN | 0 | 0 | 359309 | 8.0500 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 417 1309 | 3 | Peter, Master. Michael J | male | NaN | 1 | 1 | 2668 | 22.3583 | NaN | C | 1 | 1 | 0 | 0 | 0 | . 418 rows × 16 columns . validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].fillna(0) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].fillna(0) validdf = validdf.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;PassengerId&#39;, &#39;Ticket&#39;]) . stacked_pass_valid = [tensor(validdf.iloc[num]) for num in range(len(validdf))] . validdf . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 . 0 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 2 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 3 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 4 1 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 413 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 414 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 415 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 416 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 417 1 | 1 | 1 | 1 | 0 | 0 | 0 | . 418 rows × 7 columns . valid_x = torch.cat([survive_tensors_stacked, death_tensors_stacked]).view(-1, 8) valid_y = tensor([1]*len(survive_tensors_stacked) + [0]*len(death_tensors_stacked)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=418) # need both dl, valid dl and dl # then linear_model = nn.Linear(8,1) opt = SGD(linear_model.parameters(), lr) # train_model(linear_model, 20) #pass in the training and validatin data here dls = DataLoaders(dl, valid_dl) #finally # batch accuracy is optional def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() learn = Learner(dls, nn.Linear(8,1), opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.462106 | 0.474574 | 0.640853 | 00:00 | . 1 | 0.462066 | 0.474545 | 0.640853 | 00:00 | . 2 | 0.462022 | 0.474515 | 0.640853 | 00:00 | . 3 | 0.461978 | 0.474486 | 0.640853 | 00:00 | . 4 | 0.461934 | 0.474457 | 0.640853 | 00:00 | . 5 | 0.461889 | 0.474427 | 0.640853 | 00:00 | . 6 | 0.461845 | 0.474398 | 0.640853 | 00:00 | . 7 | 0.461801 | 0.474368 | 0.640853 | 00:00 | . 8 | 0.461756 | 0.474339 | 0.640853 | 00:00 | . 9 | 0.461712 | 0.474310 | 0.640853 | 00:00 | . simple_net = nn.Sequential( nn.Linear(8,1), nn.ReLU(), nn.Linear(1,8) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(30, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.477024 | 0.488728 | 0.552048 | 00:00 | . 1 | 0.465068 | 0.484222 | 0.579966 | 00:00 | . 2 | 0.452527 | 0.479878 | 0.579265 | 00:00 | . 3 | 0.440095 | 0.475671 | 0.583894 | 00:00 | . 4 | 0.427957 | 0.471588 | 0.584175 | 00:00 | . 5 | 0.416164 | 0.467649 | 0.586420 | 00:00 | . 6 | 0.404775 | 0.463854 | 0.585999 | 00:00 | . 7 | 0.393790 | 0.460202 | 0.586560 | 00:00 | . 8 | 0.383232 | 0.456691 | 0.586700 | 00:00 | . 9 | 0.373102 | 0.453315 | 0.617144 | 00:00 | . 10 | 0.363406 | 0.450072 | 0.621633 | 00:00 | . 11 | 0.354151 | 0.446956 | 0.621352 | 00:00 | . 12 | 0.345341 | 0.443954 | 0.621493 | 00:00 | . 13 | 0.336966 | 0.441063 | 0.620932 | 00:00 | . 14 | 0.329048 | 0.438224 | 0.623317 | 00:00 | . 15 | 0.321520 | 0.435297 | 0.623597 | 00:00 | . 16 | 0.314350 | 0.432160 | 0.624018 | 00:00 | . 17 | 0.307599 | 0.428523 | 0.626263 | 00:00 | . 18 | 0.301146 | 0.423695 | 0.628928 | 00:00 | . 19 | 0.294620 | 0.415390 | 0.643098 | 00:00 | . 20 | 0.286420 | 0.401846 | 0.657828 | 00:00 | . 21 | 0.275414 | 0.380488 | 0.696549 | 00:00 | . 22 | 0.259564 | 0.348015 | 0.740881 | 00:00 | . 23 | 0.237341 | 0.304325 | 0.817059 | 00:00 | . 24 | 0.210966 | 0.255090 | 0.860550 | 00:00 | . 25 | 0.184133 | 0.202349 | 0.902637 | 00:00 | . 26 | 0.156528 | 0.147107 | 0.988636 | 00:00 | . 27 | 0.132545 | 0.112562 | 1.000000 | 00:00 | . 28 | 0.117088 | 0.095011 | 1.000000 | 00:00 | . 29 | 0.106355 | 0.083928 | 1.000000 | 00:00 | . learn.recorder.values[-1][2] . 1.0 .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/16/regression-model-NN-model.html",
            "relUrl": "/fastpages/jupyter/2022/09/16/regression-model-NN-model.html",
            "date": " • Sep 16, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Neural Network Digit Classifier",
            "content": "Neural network image classifier using fast.ai and Pytorch modules . Install required modules and get the training and validation data from MNIST. | pip install fastbook . Collecting fastbook Downloading fastbook-0.0.28-py3-none-any.whl (719 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 719.8/719.8 kB 927.7 kB/s eta 0:00:00a 0:00:01 Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastbook) (22.1.2) Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastbook) (21.3) Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.1.0) Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from fastbook) (4.20.1) Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.8.4) Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastbook) (1.3.5) Requirement already satisfied: fastai&gt;=2.6 in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.7.9) Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.1.97) Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.28.1) Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.7.3) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.5.3) Requirement already satisfied: spacy&lt;4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.3.1) Requirement already satisfied: torch&lt;1.14,&gt;=1.7 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.11.0) Requirement already satisfied: fastcore&lt;1.6,&gt;=1.4.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.21) Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.2) Requirement already satisfied: torchvision&gt;=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.12.0) Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (6.0) Requirement already satisfied: pillow&gt;6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (9.1.1) Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7) Requirement already satisfied: fastprogress&gt;=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: numpy&gt;=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (1.21.6) Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (2022.7.1) Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.0.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.64.0) Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.8.1) Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.70.13) Requirement already satisfied: responses&lt;0.19 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.18.0) Requirement already satisfied: pyarrow&gt;=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (5.0.0) Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.8.1) Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.12.0) Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.3.5.1) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2.1.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2022.6.15) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (1.26.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (3.3) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging-&gt;fastbook) (3.0.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2022.1) Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (2021.11.10) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (0.12.1) Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (3.7.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets-&gt;fastbook) (4.3.0) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;fastbook) (1.15.0) Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.10) Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (59.8.0) Requirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.4.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.6) Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.8) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.17) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.8.2) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.10.1) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.8) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.4) Requirement already satisfied: pathy&gt;=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.2) Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0) Collecting typing-extensions&gt;=3.7.4.3 Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.7) Requirement already satisfied: frozenlist&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2) Requirement already satisfied: aiosignal&gt;=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.2.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.7.2) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (21.4.0) Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (0.13.0) Requirement already satisfied: zipp&gt;=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata-&gt;datasets-&gt;fastbook) (3.8.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.3) Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.33.3) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.1) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (5.2.1) Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.4) Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.1) Installing collected packages: typing-extensions, fastbook Attempting uninstall: typing-extensions Found existing installation: typing_extensions 4.3.0 Uninstalling typing_extensions-4.3.0: Successfully uninstalled typing_extensions-4.3.0 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed. tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible. tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible. tensorflow 2.6.4 requires tensorboard&lt;2.7,&gt;=2.6.0, but you have tensorboard 2.10.0 which is incompatible. tensorflow 2.6.4 requires typing-extensions&lt;3.11,&gt;=3.7, but you have typing-extensions 4.1.1 which is incompatible. tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.6.4 which is incompatible. tensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.6.4 which is incompatible. pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible. flax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible. flake8 4.0.1 requires importlib-metadata&lt;4.3; python_version &lt; &#34;3.8&#34;, but you have importlib-metadata 4.12.0 which is incompatible. apache-beam 2.40.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.5.1 which is incompatible. allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible. aiobotocore 2.3.4 requires botocore&lt;1.24.22,&gt;=1.24.21, but you have botocore 1.27.56 which is incompatible. Successfully installed fastbook-0.0.28 typing-extensions-4.1.1 WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv Note: you may need to restart the kernel to use updated packages. . . import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . . 100.14% [3219456/3214948 00:01&lt;00:00] path.ls() . (#3) [Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;),Path(&#39;train&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . im3_path = threes[1] im3 = Image.open(im3_path) im3 . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . . im3_t = tensor(im3) df = pd.DataFrame(im3_t[:]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 51 | 119 | 253 | 253 | 253 | 76 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 183 | 253 | 253 | 139 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 182 | 253 | 253 | 104 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 249 | 253 | 253 | 36 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 214 | 253 | 253 | 173 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 247 | 253 | 253 | 226 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 150 | 252 | 253 | 253 | 233 | 53 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 42 | 115 | 42 | 60 | 115 | 159 | 240 | 253 | 253 | 250 | 175 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 187 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 197 | 86 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 103 | 253 | 253 | 253 | 253 | 253 | 232 | 67 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . # Check one of the images created show_image(three_tensors[1]); . . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean(0) show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . # Check a random image and see how far its distance is from the ideal three a_3 = stacked_threes[1] show_image(a_3); . . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . Compute Metrics using Broadcasting . Start by getting validation labels from the MNIST dataset for both digits . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1270, 0.1632, 0.1676, ..., 0.1228, 0.1210, 0.1287]), torch.Size([1010])) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . is_3(valid_3_tens) . tensor([ True, False, False, ..., True, True, False]) . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Use Stochastic Gradient Descent to optimize our prediction model . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until we stop the training process | def f(x): return x**2 . xt = tensor(3.).requires_grad_() . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . yt.backward() . xt.grad . tensor(6.) . xt = tensor([3.,4.,10.]).requires_grad_() xt # Add sum to the quadratic function so it can take a vector (rank-1 tensor) and return a scalar (rank-0 tensor) def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad lr = 1e-5 . tensor([ 6., 8., 20.]) . # w -= gradient(w) * lr . def mse(preds, targets): return ((preds-targets)**2).mean() def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . # start by by concatenating all of our images (independant x variable) into a single tensor and change them from a list of matrices (rank-3 tensor) to a list of vectors (a rank-2 tensor) -- using Pytorch&#39;s # view method. train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . bias = init_params(1) . (train_x[0]*weights.T).sum() + bias . tensor([-6.2330], grad_fn=&lt;AddBackward0&gt;) . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ -6.2330], [-10.6388], [-20.8865], ..., [-15.9176], [ -1.6866], [-11.3568]], grad_fn=&lt;AddBackward0&gt;) . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [ True], [ True]]) . corrects.float().mean().item() . 0.5379961133003235 . with torch.no_grad(): weights[0] *= 1.0001 . preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.5379961133003235 . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . # Re-initialize parameters weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . preds = linear1(batch) preds . tensor([[14.0882], [13.9915], [16.0442], [17.7304]], grad_fn=&lt;AddBackward0&gt;) . loss = mnist_loss(preds, train_y[:4]) loss . tensor(4.1723e-07, grad_fn=&lt;MeanBackward0&gt;) . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-5.9512e-08), tensor([-4.1723e-07])) . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-1.1902e-07), tensor([-8.3446e-07])) . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-1.7854e-07), tensor([-1.2517e-06])) . weights.grad.zero_() bias.grad.zero_() . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . (preds&gt;0.0).float() == train_y[:4] . tensor([[True], [True], [True], [True]]) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(batch), train_y[:4]) . tensor(1.) . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5748 . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.7251 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8569 0.9096 0.9296 0.9399 0.9467 0.9545 0.9569 0.9628 0.9647 0.9662 0.9672 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.975 . Creating an optimizer . replace linear1 function with Pytorch&#39;s nn.linear module reminder : nn.linear accomplishes the same thing as init_params and linear together - it contains both the weights and biases in a single class | linear_model = nn.Linear(28*28,1) . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.6381 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.7724 0.8559 0.916 0.935 0.9472 0.9579 0.9628 0.9658 0.9677 0.9697 0.9716 0.9741 0.975 0.976 0.9765 0.9775 0.978 0.978 0.978 . # fast ai SGD class is the same as our BasicOptim class, therefore: linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.831 0.8398 0.9116 0.934 0.9477 0.956 0.9623 0.9658 0.9667 0.9697 0.9726 0.9741 0.975 0.9755 0.9765 0.9775 0.9785 0.9785 0.9785 . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637040 | 0.503638 | 0.495584 | 00:00 | . 1 | 0.596475 | 0.159199 | 0.878312 | 00:00 | . 2 | 0.216541 | 0.197214 | 0.819431 | 00:00 | . 3 | 0.093282 | 0.111199 | 0.908243 | 00:00 | . 4 | 0.047910 | 0.080145 | 0.931305 | 00:00 | . 5 | 0.030276 | 0.063814 | 0.946025 | 00:00 | . 6 | 0.023095 | 0.053701 | 0.955348 | 00:00 | . 7 | 0.019960 | 0.046993 | 0.961727 | 00:00 | . 8 | 0.018413 | 0.042308 | 0.965162 | 00:00 | . 9 | 0.017511 | 0.038881 | 0.967125 | 00:00 | . Adding Nonlinearity . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.385122 | 0.388649 | 0.520118 | 00:00 | . 1 | 0.170687 | 0.256767 | 0.771835 | 00:00 | . 2 | 0.090929 | 0.123868 | 0.908734 | 00:00 | . 3 | 0.057405 | 0.081251 | 0.938665 | 00:00 | . 4 | 0.042229 | 0.062670 | 0.952895 | 00:00 | . 5 | 0.034708 | 0.052418 | 0.963690 | 00:00 | . 6 | 0.030526 | 0.046020 | 0.965653 | 00:00 | . 7 | 0.027888 | 0.041687 | 0.966634 | 00:00 | . 8 | 0.026028 | 0.038563 | 0.968106 | 00:00 | . 9 | 0.024608 | 0.036192 | 0.968597 | 00:00 | . 10 | 0.023467 | 0.034323 | 0.971050 | 00:00 | . 11 | 0.022520 | 0.032800 | 0.973013 | 00:00 | . 12 | 0.021717 | 0.031524 | 0.973503 | 00:00 | . 13 | 0.021023 | 0.030433 | 0.974975 | 00:00 | . 14 | 0.020417 | 0.029485 | 0.974975 | 00:00 | . 15 | 0.019881 | 0.028649 | 0.975466 | 00:00 | . 16 | 0.019402 | 0.027905 | 0.975957 | 00:00 | . 17 | 0.018971 | 0.027236 | 0.976938 | 00:00 | . 18 | 0.018580 | 0.026633 | 0.977429 | 00:00 | . 19 | 0.018223 | 0.026085 | 0.978410 | 00:00 | . 20 | 0.017895 | 0.025584 | 0.978410 | 00:00 | . 21 | 0.017592 | 0.025126 | 0.978901 | 00:00 | . 22 | 0.017310 | 0.024705 | 0.978901 | 00:00 | . 23 | 0.017048 | 0.024316 | 0.979882 | 00:00 | . 24 | 0.016803 | 0.023957 | 0.980373 | 00:00 | . 25 | 0.016572 | 0.023623 | 0.980373 | 00:00 | . 26 | 0.016355 | 0.023313 | 0.980864 | 00:00 | . 27 | 0.016150 | 0.023025 | 0.980864 | 00:00 | . 28 | 0.015955 | 0.022756 | 0.981354 | 00:00 | . 29 | 0.015771 | 0.022505 | 0.981354 | 00:00 | . 30 | 0.015595 | 0.022270 | 0.981354 | 00:00 | . 31 | 0.015427 | 0.022051 | 0.981845 | 00:00 | . 32 | 0.015267 | 0.021844 | 0.981845 | 00:00 | . 33 | 0.015114 | 0.021651 | 0.982826 | 00:00 | . 34 | 0.014967 | 0.021469 | 0.982826 | 00:00 | . 35 | 0.014827 | 0.021297 | 0.982826 | 00:00 | . 36 | 0.014692 | 0.021135 | 0.982826 | 00:00 | . 37 | 0.014562 | 0.020982 | 0.982826 | 00:00 | . 38 | 0.014436 | 0.020837 | 0.982826 | 00:00 | . 39 | 0.014315 | 0.020700 | 0.982826 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)); . learn.recorder.values[-1][2] . 0.982826292514801 . dls = ImageDataLoaders.from_folder(path) learn = vision_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.074170 | 0.031738 | 0.994112 | 00:23 | .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/15/digit-classifier-ipynb.html",
            "relUrl": "/fastpages/jupyter/2022/09/15/digit-classifier-ipynb.html",
            "date": " • Sep 15, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Aspiring SWE/data scientist with a strong background in sports management. . Current tech stack : . Python | SQL | Postgres | Docker | Kubernetes | RabbitMQ | . linkedin: https://www.linkedin.com/in/eric-vincent-706078137/ .",
          "url": "https://ericvincent18.github.io/fastaiMLmodel/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ericvincent18.github.io/fastaiMLmodel/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}