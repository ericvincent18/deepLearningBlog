{
  
    
        "post0": {
            "title": "Regression Model - Clean",
            "content": "import pandas as pd import numpy as np # uncomment and import modules # pip install fastbook # import fastbook # fastbook.setup_book() # from fastai.vision.all import * # from fastbook import * # import torch.nn.functional as F # to run from your workstation # download the titanic survival data set : train.csv import os path = os.getcwd() # df = pd.read_csv(f&quot;{path}/YOUR_FILE_LOCATION/train.csv&quot;) . df = pd.read_csv(&#39;/Users/ericvincent/Desktop/train.csv&#39;) def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() def sigmoid(x): return 1/(1+torch.exp(-x)) def survive_loss_updated(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . class FormatDataframe : def __init__(self, df: pd.DataFrame): self.df = df def splitData(self): twenty_percent_df = df twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Sex&#39;] twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:0}) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0, &#39;Q&#39;:0}) twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass1&#39;].replace({2:0, 3:0}) twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;Pclass&#39;]) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].fillna(0) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].fillna(0) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;PassengerId&#39;, &#39;Ticket&#39;]) eighty_percent_df = twenty_percent_df.iloc[180:] twenty_percent_df = twenty_percent_df.iloc[:179] return eighty_percent_df, twenty_percent_df def createTensors(self, dfName): # return labels survived_label_train = dfName[&#39;Survived&#39;] == 1 death_label_train = dfName[&#39;Survived&#39;] == 0 # creating tensors survived_df = dfName.loc[survived_label_train] death_df = dfName.loc[death_label_train] stacked_survived = [tensor(survived_df.iloc[num]) for num in range(len(survived_df))] stacked_death = [tensor(death_df.iloc[num]) for num in range(len(death_df))] survive_tensors_stacked = torch.stack(stacked_survived).float() death_tensors_stacked = torch.stack(stacked_death).float() return survive_tensors_stacked, death_tensors_stacked . if __name__ == &quot;__main__&quot; : model = FormatDataframe(df) train, validation = model.splitData() survive_tensors_stacked_train, death_tensors_stacked_train = model.createTensors(train) survive_tensors_stacked_validation, death_tensors_stacked_validation = model.createTensors(validation) # labels on 80% of data label_df = FormatDataframe(df) eighty_percent_labels,_ = label_df.splitData() survived_label = eighty_percent_labels[&#39;Survived&#39;] == 1 death_label = eighty_percent_labels[&#39;Survived&#39;] == 0 survived = eighty_percent_labels.loc[survived_label] death = eighty_percent_labels.loc[death_label] # create training dl train_x = torch.cat([survive_tensors_stacked_train, death_tensors_stacked_train]).view(-1, 8) train_y = tensor([1]*len(survive_tensors_stacked_train) + [0]*len(death_tensors_stacked_train)).unsqueeze(1) dset = list(zip(train_x,train_y)) dl = DataLoader(dset, batch_size=8) # create validation dl valid_x = torch.cat([survive_tensors_stacked_validation, death_tensors_stacked_validation]).view(-1, 8) valid_y = tensor([1]*len(survive_tensors_stacked_validation) + [0]*len(death_tensors_stacked_validation)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=8) # finally dls = DataLoaders(dl, valid_dl) # neural net simple_net = nn.Sequential( nn.Linear(8,1), nn.ReLU(), nn.Linear(1,8) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(40, 0.1) .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/23/Neural-Net-Model-clean.html",
            "relUrl": "/fastpages/jupyter/2022/09/23/Neural-Net-Model-clean.html",
            "date": " • Sep 23, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Regression Model - Draft",
            "content": "import pandas as pd import numpy as np . df = pd.read_csv(&#39;/kaggle/input/titanic-survival-dataset/train.csv&#39;) . df = df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;Ticket&#39;, &#39;PassengerId&#39;]) . df[&#39;Male&#39;] = df[&#39;Sex&#39;] . df[&#39;Male&#39;] = df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) . df[&#39;Embarked_C&#39;] = df[&#39;Embarked&#39;] df[&#39;Embarked_C&#39;] = df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1}) df[&#39;Embarked_S&#39;] = df[&#39;Embarked&#39;] df[&#39;Embarked_S&#39;] = df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0}) df[&#39;Pclass1&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass2&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass3&#39;] = df[&#39;Pclass&#39;] df[&#39;Pclass1&#39;] = df[&#39;Pclass1&#39;].replace({2:0, 3:0}) df[&#39;Pclass2&#39;] = df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) df[&#39;Pclass3&#39;] = df[&#39;Pclass3&#39;].replace({1:0, 2:0, 3:1}) . random_parameters = np.random.rand(1,10) . Sibsp, Parch, Age, log_fare, Pclass1, Pclass2, EmbarkS, EmbarkC, Male, Const = [num for num in random_parameters[0]] . maxAge, maxFare = max(df[&#39;Age&#39;]), max(df[&#39;Fare&#39;]) . df[&#39;Age_N&#39;] = df[&#39;Age&#39;] / maxAge . df[&#39;log_Fare&#39;] = df[&#39;Fare&#39;] / maxFare . df[&#39;log_Fare&#39;] = np.log10(df[&#39;log_Fare&#39;] + 1) . parameters = { Sibsp, Parch, Age, log_fare, Pclass1, Pclass2, EmbarkS, EmbarkC, Male, Const } . df[&#39;Ones&#39;] = 1 . parameters = np.array([0.023121551427445874, 0.13402581877095354, 0.1460823666152794, 0.22088891772092012, 0.594795670221624, 0.7961562768917094, 0.8530100056367039, 0.8613248152275507, 0.8901214376590094, 0.9785782868782011]) . . model_df = df . model_df = model_df.drop(columns=[&#39;Survived&#39;, &#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;]) . model_df = model_df.drop(columns=[&#39;Pclass&#39;]) . model_df[&#39;Age_N&#39;] = model_df[&#39;Age_N&#39;].fillna(0) . model_df.isnull().values.any() . True . model_df = model_df.drop(columns=[&#39;Pclass3&#39;]) . model_df[&#39;Embarked_S&#39;] = model_df[&#39;Embarked_S&#39;].replace({&#39;Q&#39;: 0}) model_df[&#39;Embarked_C&#39;] = model_df[&#39;Embarked_C&#39;].replace({&#39;Q&#39;: 0}) model_df[&#39;Linear&#39;] = model_df.dot(parameters) . model_df[&#39;Survived&#39;] = df[&#39;Survived&#39;] . model_df[&#39;Loss&#39;] = (model_df[&#39;Linear&#39;] - 1)**2 . model_df[&#39;Linear&#39;].mean() . 2.255390616421957 . survived_label = model_df[&#39;Survived&#39;] == 1 death_label = model_df[&#39;Survived&#39;] == 0 . survived = model_df.loc[survived_label] death = model_df.loc[death_label] . pip install fastbook . . Requirement already satisfied: fastbook in /opt/conda/lib/python3.7/site-packages (0.0.28) Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastbook) (22.1.2) Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.8.4) Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.28.1) Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.1.97) Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastbook) (1.3.5) Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastbook) (21.3) Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.1.0) Requirement already satisfied: fastai&gt;=2.6 in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.7.9) Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from fastbook) (4.20.1) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.5.3) Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.2) Requirement already satisfied: pillow&gt;6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (9.1.1) Requirement already satisfied: torchvision&gt;=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.12.0) Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (6.0) Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7) Requirement already satisfied: fastcore&lt;1.6,&gt;=1.4.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.26) Requirement already satisfied: spacy&lt;4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.3.1) Requirement already satisfied: fastprogress&gt;=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.7.3) Requirement already satisfied: torch&lt;1.14,&gt;=1.7 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.11.0) Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (2022.8.2) Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.70.13) Requirement already satisfied: responses&lt;0.19 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.18.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.64.0) Requirement already satisfied: numpy&gt;=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (1.21.6) Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.3.5.1) Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.8.1) Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.12.0) Requirement already satisfied: pyarrow&gt;=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (5.0.0) Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.8.1) Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.0.0) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2.1.0) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (1.26.12) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2022.6.15.2) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (3.3) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging-&gt;fastbook) (3.0.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2022.1) Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (2021.11.10) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (0.12.1) Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (3.7.1) Requirement already satisfied: attrs&gt;=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (21.4.0) Requirement already satisfied: aiosignal&gt;=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.2.0) Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (0.13.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.7.2) Requirement already satisfied: frozenlist&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.2) Requirement already satisfied: typing-extensions&gt;=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.1.1) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;fastbook) (1.15.0) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.8) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.4) Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.10.1) Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0) Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (59.8.0) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.8) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.8.2) Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.7) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.10) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.6) Requirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.4.2) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.17) Requirement already satisfied: pathy&gt;=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.2) Requirement already satisfied: zipp&gt;=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata-&gt;datasets-&gt;fastbook) (3.8.0) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0) Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.33.3) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.3) Requirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.1) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.0) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (5.2.1) Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.4) Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.1) WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv Note: you may need to restart the kernel to use updated packages. . import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . death . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.2750 | 0.006103 | 1 | 1.984874 | 0 | 0.969977 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.4375 | 0.006771 | 1 | 2.102313 | 0 | 1.215093 | . 5 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.0000 | 0.007111 | 1 | 1.130991 | 0 | 0.017159 | . 6 0 | 0 | 1 | 0.0 | 1.0 | 1 | 0 | 0.6750 | 0.041878 | 1 | 3.134283 | 0 | 4.555164 | . 7 3 | 1 | 1 | 0.0 | 1.0 | 0 | 0 | 0.0250 | 0.017507 | 1 | 1.959964 | 0 | 0.921530 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 884 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0.3125 | 0.005935 | 1 | 1.993904 | 0 | 0.987844 | . 885 0 | 5 | 0 | 0.0 | 0.0 | 0 | 0 | 0.4875 | 0.024013 | 1 | 2.089977 | 0 | 1.188051 | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0.3375 | 0.010882 | 1 | 2.872850 | 0 | 3.507568 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.0000 | 0.019437 | 1 | 1.881848 | 0 | 0.777656 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0.4000 | 0.006520 | 1 | 1.474995 | 0 | 0.225620 | . 549 rows × 13 columns . survived . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 8 0 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3375 | 0.009336 | 1 | 2.140433 | 1 | 1.300588 | . 9 1 | 0 | 0 | 1.0 | 0.0 | 0 | 1 | 0.1750 | 0.024771 | 1 | 2.248379 | 1 | 1.558451 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 875 0 | 0 | 0 | 1.0 | 0.0 | 0 | 0 | 0.1875 | 0.006082 | 1 | 1.366379 | 1 | 0.134234 | . 879 0 | 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.7000 | 0.065324 | 1 | 2.790723 | 1 | 3.206687 | . 880 0 | 1 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3125 | 0.021499 | 1 | 2.848710 | 1 | 3.417730 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 342 rows × 13 columns . a = np.array([survived.iloc[0]]) surv = tensor(a) surv . tensor([[1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.4750, 0.0566, 1.0000, 2.4782, 1.0000, 2.1852]]) . b = tensor([survived.iloc[0]]) b # TODO list comprehension for the stacked tensor containing all the rows, the result should be #rows, yaxis, xaxis # do it on the copy df . tensor([[1.0000, 0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.4750, 0.0566, 1.0000, 2.4782, 1.0000, 2.1852]]) . survived . . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Age_N log_Fare Ones Linear Survived Loss . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 0.4750 | 0.056575 | 1 | 2.478233 | 1 | 2.185174 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3250 | 0.006666 | 1 | 1.859239 | 1 | 0.738291 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.4375 | 0.042829 | 1 | 2.807605 | 1 | 3.267434 | . 8 0 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0.3375 | 0.009336 | 1 | 2.140433 | 1 | 1.300588 | . 9 1 | 0 | 0 | 1.0 | 0.0 | 0 | 1 | 0.1750 | 0.024771 | 1 | 2.248379 | 1 | 1.558451 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 875 0 | 0 | 0 | 1.0 | 0.0 | 0 | 0 | 0.1875 | 0.006082 | 1 | 1.366379 | 1 | 0.134234 | . 879 0 | 1 | 0 | 1.0 | 0.0 | 1 | 0 | 0.7000 | 0.065324 | 1 | 2.790723 | 1 | 3.206687 | . 880 0 | 1 | 0 | 0.0 | 1.0 | 0 | 1 | 0.3125 | 0.021499 | 1 | 2.848710 | 1 | 3.417730 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 0.2375 | 0.024714 | 1 | 2.596093 | 1 | 2.547514 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 0.3250 | 0.024714 | 1 | 2.443635 | 1 | 2.084081 | . 342 rows × 13 columns . survived_tensor . tensor([[1.0000, 0.0000, 0.0000, ..., 2.4782, 1.0000, 2.1852], [0.0000, 0.0000, 0.0000, ..., 1.8592, 1.0000, 0.7383], [1.0000, 0.0000, 0.0000, ..., 2.8076, 1.0000, 3.2674], ..., [0.0000, 1.0000, 0.0000, ..., 2.8487, 1.0000, 3.4177], [0.0000, 0.0000, 0.0000, ..., 2.5961, 1.0000, 2.5475], [0.0000, 0.0000, 1.0000, ..., 2.4436, 1.0000, 2.0841]]) . len(survived_tensor), len(death_tensor) . (342, 549) . num = range(len(survived)) stacked_survived = [tensor(survived.iloc[num]) for num in num] . copydf = model_df . copydf = copydf.drop(columns=[&#39;Age_N&#39;, &#39;log_Fare&#39;, &#39;Ones&#39;, &#39;Linear&#39;, &#39;Loss&#39;]) . copydf[&#39;Embarked_C&#39;].mean(), copydf[&#39;Embarked_S&#39;].mean() . (0.1889763779527559, 0.7244094488188977) . r = [tensor(death.iloc[num]) for num in range(len(death))] rStacked = torch.stack(r).float() rStacked.shape # this makes sense, 549 rows, single vector with (1x13rows) vs they had a rank 3 tensor . torch.Size([549, 13]) . copydf[&#39;Embarked_S&#39;] = copydf[&#39;Embarked_S&#39;].fillna(0) copydf[&#39;Embarked_C&#39;] = copydf[&#39;Embarked_C&#39;].fillna(0) copydf . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 Survived . 0 1 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 1.0 | 0.0 | 1 | 0 | 1 | . 2 0 | 0 | 0 | 0.0 | 1.0 | 0 | 0 | 1 | . 3 1 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 1 | . 4 0 | 0 | 1 | 0.0 | 1.0 | 0 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | . 886 0 | 0 | 1 | 0.0 | 1.0 | 0 | 1 | 0 | . 887 0 | 0 | 0 | 0.0 | 1.0 | 1 | 0 | 1 | . 888 1 | 2 | 0 | 0.0 | 1.0 | 0 | 0 | 0 | . 889 0 | 0 | 1 | 1.0 | 0.0 | 1 | 0 | 1 | . 890 0 | 0 | 1 | 0.0 | 0.0 | 0 | 0 | 0 | . 891 rows × 8 columns . survived_label = copydf[&#39;Survived&#39;] == 1 death_label = copydf[&#39;Survived&#39;] == 0 survived = copydf.loc[survived_label] # survived[&#39;Embarked_S&#39;] = survived[&#39;Embarked_S&#39;].fillna(0) # survived[&#39;Embarked_C&#39;] = survived[&#39;Embarked_C&#39;].fillna(0) death = copydf.loc[death_label] # num = range(len(survived)) # nums = range(len(death)) stacked_survived = [tensor(survived.iloc[num]) for num in range(len(survived))] stacked_death = [tensor(death.iloc[num]) for num in range(len(death))] # stacked_survived.shape, stacked_death.shape len(stacked_death), len(stacked_survived) . (549, 342) . # Divide by the number of deaths of survivors in each label? survive_tensors_stacked = torch.stack(stacked_survived).float() death_tensors_stacked = torch.stack(stacked_death).float() # mean_survived = stacked_death.mean(0) # mean_death = stacked_survived.mean(0) . survive_tensors_stacked.shape, death_tensors_stacked.shape # i.e. 2 x rank 2 tensors with each 342 &amp; 549 labels respectively, consisting of 8 rows each . (torch.Size([342, 8]), torch.Size([549, 8])) . mean_survived = survive_tensors_stacked.mean(0) mean_death = death_tensors_stacked.mean(0) . mean_survived,mean_death # embarked rows are Nan - due to fillna? -- no more . (tensor([0.4737, 0.4649, 0.3187, 0.2719, 0.6345, 0.3977, 0.2544, 1.0000]), tensor([0.5537, 0.3297, 0.8525, 0.1366, 0.7778, 0.1457, 0.1767, 0.0000])) . single_survivor = survive_tensors_stacked[1] single_death = death_tensors_stacked[1] single_death, single_survivor . (tensor([0., 0., 1., 0., 1., 0., 0., 0.]), tensor([0., 0., 0., 0., 1., 0., 0., 1.])) . import torch.nn.functional as F . F.l1_loss(single_survivor.float(),mean_death), F.mse_loss(single_survivor,mean_death).sqrt() . (tensor(0.4271), tensor(0.5318)) . F.l1_loss(single_survivor.float(),mean_survived), F.mse_loss(single_survivor,mean_survived).sqrt() # need RElu given that data contains many 0? -- &gt; nan output # Update - both values are smaller than the distance between the survivor and the mean of the death . (tensor(0.3183), tensor(0.3487)) . def survive_distance(a,b): return (a-b).abs().mean() # distance_all_survived = survive_distance(survive_tensors_stacked, mean_survived) # distance_all_survived type(stacked_survived), type(mean_survived) . (list, torch.Tensor) . valid_survivor_dist = survive_distance(survive_tensors_stacked, mean_survived) valid_survivor_dist, valid_survivor_dist.shape . (tensor(0.4210), torch.Size([])) . def is_alive(x): return survive_distance(x,mean_survived) &lt; survive_distance(x,mean_death) . # Update - changed the -1,-2 to simple mean() to fix the index out of range is_alive(single_survivor), is_alive(single_survivor).float # function works, is a valid survivor . (tensor(True), &lt;function Tensor.float&gt;) . is_alive(survive_tensors_stacked) . tensor(True) . accuracy_alive = is_alive(survive_tensors_stacked).float() .mean() accuracy_death = (1 - is_alive(death_tensors_stacked).float()).mean() accuracy_alive,accuracy_death,(accuracy_alive+accuracy_death)/2 . (tensor(1.), tensor(1.), tensor(1.)) . Use Stochastic Gradient Descent to optimize our prediction model . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until we stop the training process | def f(x): return x**2 # Get a tensor which will requre gradients xt = tensor(3.).requires_grad_() yt = f(xt) yt.backward() xt.grad . tensor(6.) . xt = tensor([3.,4.,10.]).requires_grad_() xt # Add sum to the quadratic function so it can take a vector (rank-1 tensor) and return a scalar (rank-0 tensor) def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad # Implement stepping with a learning rate lr = 1e-5 # w -= gradient(w) * lr . def mse(preds, targets): return ((preds-targets)**2).mean() def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((8,1)) bias = init_params(1) . train_x = torch.cat([survive_tensors_stacked, death_tensors_stacked]).view(-1, 8) (train_x[0]*weights.T).sum() + bias . tensor([-1.1449], grad_fn=&lt;AddBackward0&gt;) . def sigmoid(x): return 1/(1+torch.exp(-x)) def survive_loss_updated(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . train_y = tensor([1]*len(survived) + [0]*len(death)).unsqueeze(1) # dset = dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y # UPDATE DL SO THAT XB SHAPE AND YB SHAPE REFLECT WHAT WE WANT dl = DataLoader(dset, batch_size=8) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([8, 8]), torch.Size([8, 1])) . twenty_percent_df = pd.read_csv(&#39;/kaggle/input/titanic-survival-dataset/train.csv&#39;) twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Sex&#39;] twenty_percent_df[&#39;Male&#39;] = twenty_percent_df[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:0}) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked&#39;] twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0, &#39;Q&#39;:0}) twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass&#39;] twenty_percent_df[&#39;Pclass1&#39;] = twenty_percent_df[&#39;Pclass1&#39;].replace({2:0, 3:0}) twenty_percent_df[&#39;Pclass2&#39;] = twenty_percent_df[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;Pclass&#39;]) twenty_percent_df[&#39;Embarked_S&#39;] = twenty_percent_df[&#39;Embarked_S&#39;].fillna(0) twenty_percent_df[&#39;Embarked_C&#39;] = twenty_percent_df[&#39;Embarked_C&#39;].fillna(0) twenty_percent_df = twenty_percent_df.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;PassengerId&#39;, &#39;Ticket&#39;]) eighty_percent_df = twenty_percent_df.iloc[180:] twenty_percent_df = twenty_percent_df.iloc[:179] . #training survived_label_train = eighty_percent_df[&#39;Survived&#39;] == 1 death_label_train = eighty_percent_df[&#39;Survived&#39;] == 0 survived_train = eighty_percent_df.loc[survived_label_train] death_train = eighty_percent_df.loc[death_label_train] stacked_survived_train = [tensor(survived_train.iloc[num]) for num in range(len(survived_train))] stacked_death_train = [tensor(death_train.iloc[num]) for num in range(len(death_train))] #stack survive_tensors_stacked_train = torch.stack(stacked_survived_train).float() death_tensors_stacked_train = torch.stack(stacked_death_train).float() #validation survived_label_validation = twenty_percent_df[&#39;Survived&#39;] == 1 death_label_validation = twenty_percent_df[&#39;Survived&#39;] == 0 survived_validation = twenty_percent_df.loc[survived_label_validation] death_validation = twenty_percent_df.loc[death_label_validation] stacked_survived_validation = [tensor(survived_validation.iloc[num]) for num in range(len(survived))] stacked_death_validation = [tensor(death_validation.iloc[num]) for num in range(len(death))] #stack survive_tensors_stacked_validation = torch.stack(stacked_survived_validation).float() death_tensors_stacked_validation = torch.stack(stacked_death_validation).float() . # create training dl train_x = torch.cat([survive_tensors_stacked_train, death_tensors_stacked_train]).view(-1, 8) train_y = tensor([1]*len(survived) + [0]*len(death)).unsqueeze(1) dset = list(zip(train_x,train_y)) dl = DataLoader(dset, batch_size=8) # create validation dl valid_x = torch.cat([survive_tensors_stacked_validation, death_tensors_stacked_validation]).view(-1, 8) valid_y = tensor([1]*len(survive_tensors_stacked_validation) + [0]*len(death_tensors_stacked_validation)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=8) # finally dls = DataLoaders(dl, valid_dl) def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() # neural net simple_net = nn.Sequential( nn.Linear(8,1), nn.ReLU(), nn.Linear(1,8) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.509653 | 0.502687 | 0.473765 | 00:00 | . 1 | 0.508797 | 0.502065 | 0.477132 | 00:00 | . 2 | 0.507841 | 0.501465 | 0.477273 | 00:00 | . 3 | 0.506854 | 0.500884 | 0.480079 | 00:00 | . 4 | 0.505843 | 0.500287 | 0.480780 | 00:00 | . 5 | 0.504807 | 0.499660 | 0.482183 | 00:00 | . 6 | 0.503729 | 0.498991 | 0.482183 | 00:00 | . 7 | 0.502581 | 0.498265 | 0.483025 | 00:00 | . 8 | 0.501378 | 0.497482 | 0.485690 | 00:00 | . 9 | 0.500111 | 0.496636 | 0.488356 | 00:00 | . 10 | 0.498759 | 0.495647 | 0.488917 | 00:00 | . 11 | 0.496819 | 0.493076 | 0.501403 | 00:00 | . 12 | 0.492483 | 0.488248 | 0.542789 | 00:00 | . 13 | 0.485624 | 0.480793 | 0.586700 | 00:00 | . 14 | 0.475594 | 0.470186 | 0.586841 | 00:00 | . 15 | 0.461520 | 0.455530 | 0.628367 | 00:00 | . 16 | 0.443047 | 0.436796 | 0.638749 | 00:00 | . 17 | 0.420336 | 0.414819 | 0.633979 | 00:00 | . 18 | 0.394467 | 0.390664 | 0.637907 | 00:00 | . 19 | 0.366764 | 0.363788 | 0.644220 | 00:00 | . 20 | 0.336472 | 0.332068 | 0.709877 | 00:00 | . 21 | 0.301700 | 0.296407 | 0.799102 | 00:00 | . 22 | 0.266188 | 0.264302 | 0.799383 | 00:00 | . 23 | 0.236447 | 0.239894 | 0.803030 | 00:00 | . 24 | 0.215284 | 0.221003 | 0.808361 | 00:00 | . 25 | 0.199868 | 0.205571 | 0.830107 | 00:00 | . 26 | 0.187974 | 0.192608 | 0.840348 | 00:00 | . 27 | 0.178303 | 0.181214 | 0.883558 | 00:00 | . 28 | 0.170570 | 0.170600 | 0.889450 | 00:00 | . 29 | 0.163774 | 0.160689 | 0.931257 | 00:00 | . 30 | 0.157617 | 0.151713 | 0.937009 | 00:00 | . 31 | 0.152163 | 0.143193 | 0.944865 | 00:00 | . 32 | 0.147052 | 0.135531 | 0.947952 | 00:00 | . 33 | 0.142345 | 0.128379 | 0.949916 | 00:00 | . 34 | 0.138021 | 0.121739 | 0.951459 | 00:00 | . 35 | 0.134111 | 0.115172 | 0.951740 | 00:00 | . 36 | 0.130328 | 0.109340 | 0.951740 | 00:00 | . 37 | 0.126849 | 0.103617 | 0.951880 | 00:00 | . 38 | 0.123680 | 0.098068 | 0.991583 | 00:00 | . 39 | 0.120625 | 0.093022 | 0.996773 | 00:00 | . validdf = pd.read_csv(&#39;/kaggle/input/test-titanic/test.csv&#39;) validdf[&#39;Male&#39;] = validdf[&#39;Sex&#39;] validdf[&#39;Male&#39;] = validdf[&#39;Male&#39;].replace({&#39;male&#39;: 1, &#39;female&#39; : 0}) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked&#39;] validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].replace({&#39;S&#39;:0, &#39;C&#39;:1, &#39;Q&#39;:0}) validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked&#39;] validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].replace({&#39;S&#39;:1, &#39;C&#39;:0, &#39;Q&#39;:0}) validdf[&#39;Pclass1&#39;] = validdf[&#39;Pclass&#39;] validdf[&#39;Pclass2&#39;] = validdf[&#39;Pclass&#39;] validdf[&#39;Pclass1&#39;] = validdf[&#39;Pclass1&#39;].replace({2:0, 3:0}) validdf[&#39;Pclass2&#39;] = validdf[&#39;Pclass2&#39;].replace({1:0, 3:0, 2:1}) validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].fillna(0) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].fillna(0) validdf . PassengerId Pclass Name Sex Age SibSp Parch Ticket Fare Cabin Embarked Male Embarked_C Embarked_S Pclass1 Pclass2 . 0 892 | 3 | Kelly, Mr. James | male | 34.5 | 0 | 0 | 330911 | 7.8292 | NaN | Q | 1 | 0 | 0 | 0 | 0 | . 1 893 | 3 | Wilkes, Mrs. James (Ellen Needs) | female | 47.0 | 1 | 0 | 363272 | 7.0000 | NaN | S | 0 | 0 | 1 | 0 | 0 | . 2 894 | 2 | Myles, Mr. Thomas Francis | male | 62.0 | 0 | 0 | 240276 | 9.6875 | NaN | Q | 1 | 0 | 0 | 0 | 1 | . 3 895 | 3 | Wirz, Mr. Albert | male | 27.0 | 0 | 0 | 315154 | 8.6625 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 4 896 | 3 | Hirvonen, Mrs. Alexander (Helga E Lindqvist) | female | 22.0 | 1 | 1 | 3101298 | 12.2875 | NaN | S | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 413 1305 | 3 | Spector, Mr. Woolf | male | NaN | 0 | 0 | A.5. 3236 | 8.0500 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 414 1306 | 1 | Oliva y Ocana, Dona. Fermina | female | 39.0 | 0 | 0 | PC 17758 | 108.9000 | C105 | C | 0 | 1 | 0 | 1 | 0 | . 415 1307 | 3 | Saether, Mr. Simon Sivertsen | male | 38.5 | 0 | 0 | SOTON/O.Q. 3101262 | 7.2500 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 416 1308 | 3 | Ware, Mr. Frederick | male | NaN | 0 | 0 | 359309 | 8.0500 | NaN | S | 1 | 0 | 1 | 0 | 0 | . 417 1309 | 3 | Peter, Master. Michael J | male | NaN | 1 | 1 | 2668 | 22.3583 | NaN | C | 1 | 1 | 0 | 0 | 0 | . 418 rows × 16 columns . # maxAge, maxFare = max(validdf[&#39;Age&#39;]), max(validdf[&#39;Fare&#39;]) # validdf[&#39;Age_N&#39;] = validdf[&#39;Age&#39;] / maxAge # validdf[&#39;log_Fare&#39;] = validdf[&#39;Fare&#39;] / maxFare # validdf[&#39;log_Fare&#39;] = np.log10(df[&#39;log_Fare&#39;] + 1) # model_df[&#39;Survived&#39;] = df[&#39;Survived&#39;] validdf = validdf.drop(columns=[&#39;Sex&#39;, &#39;Age&#39;, &#39;Fare&#39;, &#39;Embarked&#39;, &#39;Pclass&#39;]) # model_df[&#39;Age_N&#39;] = model_df[&#39;Age_N&#39;].fillna(0) # copydf = model_df # copydf = copydf.drop(columns=[&#39;Age_N&#39;, &#39;log_Fare&#39;, &#39;Ones&#39;, &#39;Linear&#39;, &#39;Loss&#39;]) validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].fillna(0) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].fillna(0) # we now have a single tensor that was created from calling tensor over all of our labelled rows of data using tensor() # survived_label_valid = validdf[&#39;Survived&#39;] == 1 # death_label_valid = validdf[&#39;Survived&#39;] == 0 # survived_valid = validdf.loc[survived_label] # survived[&#39;Embarked_S&#39;] = survived[&#39;Embarked_S&#39;].fillna(0) # survived[&#39;Embarked_C&#39;] = survived[&#39;Embarked_C&#39;].fillna(0) # death_valid = validdf.loc[death_label] # num = range(len(survived)) # nums = range(len(death)) # stacked_pass_valid = [tensor(validdf.iloc[num]) for num in range(len(validdf))] # stacked_death_valid = [tensor(death_valid.iloc[num]) for num in range(len(death_valid))] # stacked_survived.shape, stacked_death.shape # len(stacked_death_valid), len(stacked_survived_valid) . . TypeError Traceback (most recent call last) /tmp/ipykernel_17/612449954.py in &lt;module&gt; 23 # num = range(len(survived)) 24 # nums = range(len(death)) &gt; 25 stacked_pass_valid = [tensor(validdf.iloc[num]) for num in range(len(validdf))] 26 # stacked_death_valid = [tensor(death_valid.iloc[num]) for num in range(len(death_valid))] 27 # stacked_survived.shape, stacked_death.shape /tmp/ipykernel_17/612449954.py in &lt;listcomp&gt;(.0) 23 # num = range(len(survived)) 24 # nums = range(len(death)) &gt; 25 stacked_pass_valid = [tensor(validdf.iloc[num]) for num in range(len(validdf))] 26 # stacked_death_valid = [tensor(death_valid.iloc[num]) for num in range(len(death_valid))] 27 # stacked_survived.shape, stacked_death.shape /opt/conda/lib/python3.7/site-packages/fastai/torch_core.py in tensor(x, *rest, **kwargs) 148 else torch.tensor(x, **kwargs) if isinstance(x, (tuple,list)) 149 else _array2tensor(x) if isinstance(x, ndarray) --&gt; 150 else as_tensor(x.values, **kwargs) if isinstance(x, (pd.Series, pd.DataFrame)) 151 # else as_tensor(array(x, **kwargs)) if hasattr(x, &#39;__array__&#39;) or is_iter(x) 152 else _array2tensor(array(x), **kwargs)) TypeError: can&#39;t convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool. . validdf[&#39;Embarked_S&#39;] = validdf[&#39;Embarked_S&#39;].fillna(0) validdf[&#39;Embarked_C&#39;] = validdf[&#39;Embarked_C&#39;].fillna(0) validdf = validdf.drop(columns=[&#39;Name&#39;, &#39;Cabin&#39;, &#39;PassengerId&#39;, &#39;Ticket&#39;]) . stacked_pass_valid = [tensor(validdf.iloc[num]) for num in range(len(validdf))] . validdf . SibSp Parch Male Embarked_C Embarked_S Pclass1 Pclass2 . 0 0 | 0 | 1 | 0 | 0 | 0 | 0 | . 1 1 | 0 | 0 | 0 | 1 | 0 | 0 | . 2 0 | 0 | 1 | 0 | 0 | 0 | 1 | . 3 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 4 1 | 1 | 0 | 0 | 1 | 0 | 0 | . ... ... | ... | ... | ... | ... | ... | ... | . 413 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 414 0 | 0 | 0 | 1 | 0 | 1 | 0 | . 415 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 416 0 | 0 | 1 | 0 | 1 | 0 | 0 | . 417 1 | 1 | 1 | 1 | 0 | 0 | 0 | . 418 rows × 7 columns . valid_x = torch.cat([survive_tensors_stacked, death_tensors_stacked]).view(-1, 8) valid_y = tensor([1]*len(survive_tensors_stacked) + [0]*len(death_tensors_stacked)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) valid_dl = DataLoader(valid_dset, batch_size=418) # need both dl, valid dl and dl # then linear_model = nn.Linear(8,1) opt = SGD(linear_model.parameters(), lr) # train_model(linear_model, 20) #pass in the training and validatin data here dls = DataLoaders(dl, valid_dl) #finally # batch accuracy is optional def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() learn = Learner(dls, nn.Linear(8,1), opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.462106 | 0.474574 | 0.640853 | 00:00 | . 1 | 0.462066 | 0.474545 | 0.640853 | 00:00 | . 2 | 0.462022 | 0.474515 | 0.640853 | 00:00 | . 3 | 0.461978 | 0.474486 | 0.640853 | 00:00 | . 4 | 0.461934 | 0.474457 | 0.640853 | 00:00 | . 5 | 0.461889 | 0.474427 | 0.640853 | 00:00 | . 6 | 0.461845 | 0.474398 | 0.640853 | 00:00 | . 7 | 0.461801 | 0.474368 | 0.640853 | 00:00 | . 8 | 0.461756 | 0.474339 | 0.640853 | 00:00 | . 9 | 0.461712 | 0.474310 | 0.640853 | 00:00 | . simple_net = nn.Sequential( nn.Linear(8,1), nn.ReLU(), nn.Linear(1,8) ) learn = Learner(dls, simple_net, opt_func=SGD, loss_func=survive_loss_updated, metrics=batch_accuracy) learn.fit(30, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.477024 | 0.488728 | 0.552048 | 00:00 | . 1 | 0.465068 | 0.484222 | 0.579966 | 00:00 | . 2 | 0.452527 | 0.479878 | 0.579265 | 00:00 | . 3 | 0.440095 | 0.475671 | 0.583894 | 00:00 | . 4 | 0.427957 | 0.471588 | 0.584175 | 00:00 | . 5 | 0.416164 | 0.467649 | 0.586420 | 00:00 | . 6 | 0.404775 | 0.463854 | 0.585999 | 00:00 | . 7 | 0.393790 | 0.460202 | 0.586560 | 00:00 | . 8 | 0.383232 | 0.456691 | 0.586700 | 00:00 | . 9 | 0.373102 | 0.453315 | 0.617144 | 00:00 | . 10 | 0.363406 | 0.450072 | 0.621633 | 00:00 | . 11 | 0.354151 | 0.446956 | 0.621352 | 00:00 | . 12 | 0.345341 | 0.443954 | 0.621493 | 00:00 | . 13 | 0.336966 | 0.441063 | 0.620932 | 00:00 | . 14 | 0.329048 | 0.438224 | 0.623317 | 00:00 | . 15 | 0.321520 | 0.435297 | 0.623597 | 00:00 | . 16 | 0.314350 | 0.432160 | 0.624018 | 00:00 | . 17 | 0.307599 | 0.428523 | 0.626263 | 00:00 | . 18 | 0.301146 | 0.423695 | 0.628928 | 00:00 | . 19 | 0.294620 | 0.415390 | 0.643098 | 00:00 | . 20 | 0.286420 | 0.401846 | 0.657828 | 00:00 | . 21 | 0.275414 | 0.380488 | 0.696549 | 00:00 | . 22 | 0.259564 | 0.348015 | 0.740881 | 00:00 | . 23 | 0.237341 | 0.304325 | 0.817059 | 00:00 | . 24 | 0.210966 | 0.255090 | 0.860550 | 00:00 | . 25 | 0.184133 | 0.202349 | 0.902637 | 00:00 | . 26 | 0.156528 | 0.147107 | 0.988636 | 00:00 | . 27 | 0.132545 | 0.112562 | 1.000000 | 00:00 | . 28 | 0.117088 | 0.095011 | 1.000000 | 00:00 | . 29 | 0.106355 | 0.083928 | 1.000000 | 00:00 | . learn.recorder.values[-1][2] . 1.0 .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/16/regression-model-NN-model.html",
            "relUrl": "/fastpages/jupyter/2022/09/16/regression-model-NN-model.html",
            "date": " • Sep 16, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Neural Network Digit Classifier",
            "content": "Neural network image classifier using fast.ai and Pytorch modules . Install required modules and get the training and validation data from MNIST. | pip install fastbook . Collecting fastbook Downloading fastbook-0.0.28-py3-none-any.whl (719 kB) ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 719.8/719.8 kB 927.7 kB/s eta 0:00:00a 0:00:01 Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (from fastbook) (22.1.2) Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from fastbook) (21.3) Requirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.1.0) Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (from fastbook) (4.20.1) Requirement already satisfied: graphviz in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.8.4) Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from fastbook) (1.3.5) Requirement already satisfied: fastai&gt;=2.6 in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.7.9) Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (from fastbook) (0.1.97) Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fastbook) (2.28.1) Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.7.3) Requirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.5.3) Requirement already satisfied: spacy&lt;4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (3.3.1) Requirement already satisfied: torch&lt;1.14,&gt;=1.7 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.11.0) Requirement already satisfied: fastcore&lt;1.6,&gt;=1.4.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.5.21) Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.2) Requirement already satisfied: torchvision&gt;=0.8.2 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.12.0) Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (6.0) Requirement already satisfied: pillow&gt;6.0.0 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (9.1.1) Requirement already satisfied: fastdownload&lt;2,&gt;=0.0.5 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (0.0.7) Requirement already satisfied: fastprogress&gt;=0.2.4 in /opt/conda/lib/python3.7/site-packages (from fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: numpy&gt;=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (1.21.6) Requirement already satisfied: fsspec[http]&gt;=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (2022.7.1) Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.0.0) Requirement already satisfied: tqdm&gt;=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.64.0) Requirement already satisfied: huggingface-hub&lt;1.0.0,&gt;=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.8.1) Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.70.13) Requirement already satisfied: responses&lt;0.19 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.18.0) Requirement already satisfied: pyarrow&gt;=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (5.0.0) Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (3.8.1) Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (4.12.0) Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets-&gt;fastbook) (0.3.5.1) Requirement already satisfied: charset-normalizer&lt;3,&gt;=2 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2.1.0) Requirement already satisfied: certifi&gt;=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (2022.6.15) Requirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (1.26.12) Requirement already satisfied: idna&lt;4,&gt;=2.5 in /opt/conda/lib/python3.7/site-packages (from requests-&gt;fastbook) (3.3) Requirement already satisfied: pyparsing!=3.0.5,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging-&gt;fastbook) (3.0.9) Requirement already satisfied: python-dateutil&gt;=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2.8.2) Requirement already satisfied: pytz&gt;=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas-&gt;fastbook) (2022.1) Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (2021.11.10) Requirement already satisfied: tokenizers!=0.11.3,&lt;0.13,&gt;=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (0.12.1) Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers-&gt;fastbook) (3.7.1) Requirement already satisfied: typing-extensions&gt;=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub&lt;1.0.0,&gt;=0.1.0-&gt;datasets-&gt;fastbook) (4.3.0) Requirement already satisfied: six&gt;=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas-&gt;fastbook) (1.15.0) Requirement already satisfied: jinja2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.2) Requirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.9 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.10) Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (59.8.0) Requirement already satisfied: typer&lt;0.5.0,&gt;=0.3.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.4.2) Requirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.6) Requirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.3) Requirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.8) Requirement already satisfied: thinc&lt;8.1.0,&gt;=8.0.14 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.17) Requirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.0.8) Requirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.9.0,&gt;=1.7.4 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (1.8.2) Requirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.10.1) Requirement already satisfied: blis&lt;0.8.0,&gt;=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.7.8) Requirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.4.4) Requirement already satisfied: pathy&gt;=0.3.5 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (0.6.2) Requirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.3.0) Collecting typing-extensions&gt;=3.7.4.3 Downloading typing_extensions-4.1.1-py3-none-any.whl (26 kB) Requirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (3.0.7) Requirement already satisfied: frozenlist&gt;=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.3.0) Requirement already satisfied: async-timeout&lt;5.0,&gt;=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (4.0.2) Requirement already satisfied: aiosignal&gt;=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.2.0) Requirement already satisfied: yarl&lt;2.0,&gt;=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (1.7.2) Requirement already satisfied: multidict&lt;7.0,&gt;=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (6.0.2) Requirement already satisfied: attrs&gt;=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (21.4.0) Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp-&gt;datasets-&gt;fastbook) (0.13.0) Requirement already satisfied: zipp&gt;=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata-&gt;datasets-&gt;fastbook) (3.8.0) Requirement already satisfied: kiwisolver&gt;=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (1.4.3) Requirement already satisfied: fonttools&gt;=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (4.33.3) Requirement already satisfied: cycler&gt;=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib-&gt;fastai&gt;=2.6-&gt;fastbook) (0.11.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (3.1.0) Requirement already satisfied: joblib&gt;=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn-&gt;fastai&gt;=2.6-&gt;fastbook) (1.0.1) Requirement already satisfied: smart-open&lt;6.0.0,&gt;=5.2.1 in /opt/conda/lib/python3.7/site-packages (from pathy&gt;=0.3.5-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (5.2.1) Requirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /opt/conda/lib/python3.7/site-packages (from typer&lt;0.5.0,&gt;=0.3.0-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (8.0.4) Requirement already satisfied: MarkupSafe&gt;=2.0 in /opt/conda/lib/python3.7/site-packages (from jinja2-&gt;spacy&lt;4-&gt;fastai&gt;=2.6-&gt;fastbook) (2.1.1) Installing collected packages: typing-extensions, fastbook Attempting uninstall: typing-extensions Found existing installation: typing_extensions 4.3.0 Uninstalling typing_extensions-4.3.0: Successfully uninstalled typing_extensions-4.3.0 ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. tensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed. tensorflow 2.6.4 requires h5py~=3.1.0, but you have h5py 3.7.0 which is incompatible. tensorflow 2.6.4 requires numpy~=1.19.2, but you have numpy 1.21.6 which is incompatible. tensorflow 2.6.4 requires tensorboard&lt;2.7,&gt;=2.6.0, but you have tensorboard 2.10.0 which is incompatible. tensorflow 2.6.4 requires typing-extensions&lt;3.11,&gt;=3.7, but you have typing-extensions 4.1.1 which is incompatible. tensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,&lt;2.10,&gt;=1.15.5, but you have tensorflow 2.6.4 which is incompatible. tensorflow-serving-api 2.9.0 requires tensorflow&lt;3,&gt;=2.9.0, but you have tensorflow 2.6.4 which is incompatible. pandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible. flax 0.6.0 requires rich~=11.1, but you have rich 12.1.0 which is incompatible. flake8 4.0.1 requires importlib-metadata&lt;4.3; python_version &lt; &#34;3.8&#34;, but you have importlib-metadata 4.12.0 which is incompatible. apache-beam 2.40.0 requires dill&lt;0.3.2,&gt;=0.3.1.1, but you have dill 0.3.5.1 which is incompatible. allennlp 2.10.0 requires protobuf==3.20.0, but you have protobuf 3.19.4 which is incompatible. aiobotocore 2.3.4 requires botocore&lt;1.24.22,&gt;=1.24.21, but you have botocore 1.27.56 which is incompatible. Successfully installed fastbook-0.0.28 typing-extensions-4.1.1 WARNING: Running pip as the &#39;root&#39; user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv Note: you may need to restart the kernel to use updated packages. . . import fastbook fastbook.setup_book() . from fastai.vision.all import * from fastbook import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path . . 100.14% [3219456/3214948 00:01&lt;00:00] path.ls() . (#3) [Path(&#39;valid&#39;),Path(&#39;labels.csv&#39;),Path(&#39;train&#39;)] . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;train/7&#39;),Path(&#39;train/3&#39;)] . threes = (path/&#39;train&#39;/&#39;3&#39;).ls().sorted() sevens = (path/&#39;train&#39;/&#39;7&#39;).ls().sorted() threes . (#6131) [Path(&#39;train/3/10.png&#39;),Path(&#39;train/3/10000.png&#39;),Path(&#39;train/3/10011.png&#39;),Path(&#39;train/3/10031.png&#39;),Path(&#39;train/3/10034.png&#39;),Path(&#39;train/3/10042.png&#39;),Path(&#39;train/3/10052.png&#39;),Path(&#39;train/3/1007.png&#39;),Path(&#39;train/3/10074.png&#39;),Path(&#39;train/3/10091.png&#39;)...] . im3_path = threes[1] im3 = Image.open(im3_path) im3 . array(im3)[4:10,4:10] . array([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=uint8) . tensor(im3)[4:10,4:10] . tensor([[ 0, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 29], [ 0, 0, 0, 48, 166, 224], [ 0, 93, 244, 249, 253, 187], [ 0, 107, 253, 253, 230, 48], [ 0, 3, 20, 20, 15, 0]], dtype=torch.uint8) . . im3_t = tensor(im3) df = pd.DataFrame(im3_t[:]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . &nbsp; 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 29 | 150 | 195 | 254 | 255 | 254 | 176 | 193 | 150 | 96 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 48 | 166 | 224 | 253 | 253 | 234 | 196 | 253 | 253 | 253 | 253 | 233 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 93 | 244 | 249 | 253 | 187 | 46 | 10 | 8 | 4 | 10 | 194 | 253 | 253 | 233 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 107 | 253 | 253 | 230 | 48 | 0 | 0 | 0 | 0 | 0 | 192 | 253 | 253 | 156 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 3 | 20 | 20 | 15 | 0 | 0 | 0 | 0 | 0 | 43 | 224 | 253 | 245 | 74 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 249 | 253 | 245 | 126 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 14 | 101 | 223 | 253 | 248 | 124 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 11 | 166 | 239 | 253 | 253 | 253 | 187 | 30 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 16 | 248 | 250 | 253 | 253 | 253 | 253 | 232 | 213 | 111 | 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 43 | 98 | 98 | 208 | 253 | 253 | 253 | 253 | 187 | 22 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 9 | 51 | 119 | 253 | 253 | 253 | 76 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 183 | 253 | 253 | 139 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 182 | 253 | 253 | 104 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 85 | 249 | 253 | 253 | 36 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 60 | 214 | 253 | 253 | 173 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 98 | 247 | 253 | 253 | 226 | 9 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 150 | 252 | 253 | 253 | 233 | 53 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 42 | 115 | 42 | 60 | 115 | 159 | 240 | 253 | 253 | 250 | 175 | 25 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 187 | 253 | 253 | 253 | 253 | 253 | 253 | 253 | 197 | 86 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 103 | 253 | 253 | 253 | 253 | 253 | 232 | 67 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . seven_tensors = [tensor(Image.open(o)) for o in sevens] three_tensors = [tensor(Image.open(o)) for o in threes] len(three_tensors),len(seven_tensors) . (6131, 6265) . # Check one of the images created show_image(three_tensors[1]); . . stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes = torch.stack(three_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . stacked_threes.ndim . 3 . mean3 = stacked_threes.mean(0) show_image(mean3); . mean7 = stacked_sevens.mean(0) show_image(mean7); . # Check a random image and see how far its distance is from the ideal three a_3 = stacked_threes[1] show_image(a_3); . . dist_3_abs = (a_3 - mean3).abs().mean() dist_3_sqr = ((a_3 - mean3)**2).mean().sqrt() dist_3_abs,dist_3_sqr . (tensor(0.1114), tensor(0.2021)) . dist_7_abs = (a_3 - mean7).abs().mean() dist_7_sqr = ((a_3 - mean7)**2).mean().sqrt() dist_7_abs,dist_7_sqr . (tensor(0.1586), tensor(0.3021)) . F.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt() . (tensor(0.1586), tensor(0.3021)) . Compute Metrics using Broadcasting . Start by getting validation labels from the MNIST dataset for both digits . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;3&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid&#39;/&#39;7&#39;).ls()]) valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape,valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . def mnist_distance(a,b): return (a-b).abs().mean((-1,-2)) mnist_distance(a_3, mean3) . tensor(0.1114) . valid_3_dist = mnist_distance(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1270, 0.1632, 0.1676, ..., 0.1228, 0.1210, 0.1287]), torch.Size([1010])) . def is_3(x): return mnist_distance(x,mean3) &lt; mnist_distance(x,mean7) . is_3(a_3), is_3(a_3).float() . (tensor(True), tensor(1.)) . is_3(valid_3_tens) . tensor([ True, False, False, ..., True, True, False]) . accuracy_3s = is_3(valid_3_tens).float() .mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s,accuracy_7s,(accuracy_3s+accuracy_7s)/2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Use Stochastic Gradient Descent to optimize our prediction model . Initialize the weights. | For each image, use these weights to predict whether it appears to be a 3 or a 7. | Based on these predictions, calculate how good the model is (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss | Step (that is, change) all the weights based on that calculation. | Go back to the step 2, and repeat the process. | Iterate until we stop the training process | def f(x): return x**2 . xt = tensor(3.).requires_grad_() . yt = f(xt) yt . tensor(9., grad_fn=&lt;PowBackward0&gt;) . yt.backward() . xt.grad . tensor(6.) . xt = tensor([3.,4.,10.]).requires_grad_() xt # Add sum to the quadratic function so it can take a vector (rank-1 tensor) and return a scalar (rank-0 tensor) def f(x): return (x**2).sum() yt = f(xt) yt . tensor(125., grad_fn=&lt;SumBackward0&gt;) . yt.backward() xt.grad lr = 1e-5 . tensor([ 6., 8., 20.]) . # w -= gradient(w) * lr . def mse(preds, targets): return ((preds-targets)**2).mean() def f(t, params): a,b,c = params return a*(t**2) + (b*t) + c . def apply_step(params, prn=True): preds = f(time, params) loss = mse(preds, speed) loss.backward() params.data -= lr * params.grad.data params.grad = None if prn: print(loss.item()) return preds . # start by by concatenating all of our images (independant x variable) into a single tensor and change them from a list of matrices (rank-3 tensor) to a list of vectors (a rank-2 tensor) -- using Pytorch&#39;s # view method. train_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28) . train_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1) train_x.shape,train_y.shape . (torch.Size([12396, 784]), torch.Size([12396, 1])) . dset = list(zip(train_x,train_y)) x,y = dset[0] x.shape,y . (torch.Size([784]), tensor([1])) . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1]*len(valid_3_tens) + [0]*len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x,valid_y)) . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . weights = init_params((28*28,1)) . bias = init_params(1) . (train_x[0]*weights.T).sum() + bias . tensor([-6.2330], grad_fn=&lt;AddBackward0&gt;) . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[ -6.2330], [-10.6388], [-20.8865], ..., [-15.9176], [ -1.6866], [-11.3568]], grad_fn=&lt;AddBackward0&gt;) . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[False], [False], [False], ..., [ True], [ True], [ True]]) . corrects.float().mean().item() . 0.5379961133003235 . with torch.no_grad(): weights[0] *= 1.0001 . preds = linear1(train_x) ((preds&gt;0.0).float() == train_y).float().mean().item() . 0.5379961133003235 . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1-predictions, predictions).mean() . ds = L(enumerate(string.ascii_lowercase)) ds . (#26) [(0, &#39;a&#39;),(1, &#39;b&#39;),(2, &#39;c&#39;),(3, &#39;d&#39;),(4, &#39;e&#39;),(5, &#39;f&#39;),(6, &#39;g&#39;),(7, &#39;h&#39;),(8, &#39;i&#39;),(9, &#39;j&#39;)...] . # Re-initialize parameters weights = init_params((28*28,1)) bias = init_params(1) . dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape . (torch.Size([256, 784]), torch.Size([256, 1])) . valid_dl = DataLoader(valid_dset, batch_size=256) . batch = train_x[:4] batch.shape . torch.Size([4, 784]) . preds = linear1(batch) preds . tensor([[14.0882], [13.9915], [16.0442], [17.7304]], grad_fn=&lt;AddBackward0&gt;) . loss = mnist_loss(preds, train_y[:4]) loss . tensor(4.1723e-07, grad_fn=&lt;MeanBackward0&gt;) . loss.backward() weights.grad.shape,weights.grad.mean(),bias.grad . (torch.Size([784, 1]), tensor(-5.9512e-08), tensor([-4.1723e-07])) . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-1.1902e-07), tensor([-8.3446e-07])) . calc_grad(batch, train_y[:4], linear1) weights.grad.mean(),bias.grad . (tensor(-1.7854e-07), tensor([-1.2517e-06])) . weights.grad.zero_() bias.grad.zero_() . def train_epoch(model, lr, params): for xb,yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad*lr p.grad.zero_() . (preds&gt;0.0).float() == train_y[:4] . tensor([[True], [True], [True], [True]]) . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds&gt;0.5) == yb return correct.float().mean() . batch_accuracy(linear1(batch), train_y[:4]) . tensor(1.) . def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl] return round(torch.stack(accs).mean().item(), 4) . validate_epoch(linear1) . 0.5748 . lr = 1. params = weights,bias train_epoch(linear1, lr, params) validate_epoch(linear1) . 0.7251 . for i in range(20): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.8569 0.9096 0.9296 0.9399 0.9467 0.9545 0.9569 0.9628 0.9647 0.9662 0.9672 0.9681 0.9725 0.9725 0.9725 0.973 0.9735 0.974 0.974 0.975 . Creating an optimizer . replace linear1 function with Pytorch&#39;s nn.linear module reminder : nn.linear accomplishes the same thing as init_params and linear together - it contains both the weights and biases in a single class | linear_model = nn.Linear(28*28,1) . w,b = linear_model.parameters() w.shape,b.shape . (torch.Size([1, 784]), torch.Size([1])) . class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . opt = BasicOptim(linear_model.parameters(), lr) . def train_epoch(model): for xb,yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . validate_epoch(linear_model) . 0.6381 . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . train_model(linear_model, 20) . 0.4932 0.7724 0.8559 0.916 0.935 0.9472 0.9579 0.9628 0.9658 0.9677 0.9697 0.9716 0.9741 0.975 0.976 0.9765 0.9775 0.978 0.978 0.978 . # fast ai SGD class is the same as our BasicOptim class, therefore: linear_model = nn.Linear(28*28,1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.4932 0.831 0.8398 0.9116 0.934 0.9477 0.956 0.9623 0.9658 0.9667 0.9697 0.9726 0.9741 0.975 0.9755 0.9765 0.9775 0.9785 0.9785 0.9785 . dls = DataLoaders(dl, valid_dl) . learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(10, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.637040 | 0.503638 | 0.495584 | 00:00 | . 1 | 0.596475 | 0.159199 | 0.878312 | 00:00 | . 2 | 0.216541 | 0.197214 | 0.819431 | 00:00 | . 3 | 0.093282 | 0.111199 | 0.908243 | 00:00 | . 4 | 0.047910 | 0.080145 | 0.931305 | 00:00 | . 5 | 0.030276 | 0.063814 | 0.946025 | 00:00 | . 6 | 0.023095 | 0.053701 | 0.955348 | 00:00 | . 7 | 0.019960 | 0.046993 | 0.961727 | 00:00 | . 8 | 0.018413 | 0.042308 | 0.965162 | 00:00 | . 9 | 0.017511 | 0.038881 | 0.967125 | 00:00 | . Adding Nonlinearity . def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res . w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) . simple_net = nn.Sequential( nn.Linear(28*28,30), nn.ReLU(), nn.Linear(30,1) ) . learn = Learner(dls, simple_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) . learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.385122 | 0.388649 | 0.520118 | 00:00 | . 1 | 0.170687 | 0.256767 | 0.771835 | 00:00 | . 2 | 0.090929 | 0.123868 | 0.908734 | 00:00 | . 3 | 0.057405 | 0.081251 | 0.938665 | 00:00 | . 4 | 0.042229 | 0.062670 | 0.952895 | 00:00 | . 5 | 0.034708 | 0.052418 | 0.963690 | 00:00 | . 6 | 0.030526 | 0.046020 | 0.965653 | 00:00 | . 7 | 0.027888 | 0.041687 | 0.966634 | 00:00 | . 8 | 0.026028 | 0.038563 | 0.968106 | 00:00 | . 9 | 0.024608 | 0.036192 | 0.968597 | 00:00 | . 10 | 0.023467 | 0.034323 | 0.971050 | 00:00 | . 11 | 0.022520 | 0.032800 | 0.973013 | 00:00 | . 12 | 0.021717 | 0.031524 | 0.973503 | 00:00 | . 13 | 0.021023 | 0.030433 | 0.974975 | 00:00 | . 14 | 0.020417 | 0.029485 | 0.974975 | 00:00 | . 15 | 0.019881 | 0.028649 | 0.975466 | 00:00 | . 16 | 0.019402 | 0.027905 | 0.975957 | 00:00 | . 17 | 0.018971 | 0.027236 | 0.976938 | 00:00 | . 18 | 0.018580 | 0.026633 | 0.977429 | 00:00 | . 19 | 0.018223 | 0.026085 | 0.978410 | 00:00 | . 20 | 0.017895 | 0.025584 | 0.978410 | 00:00 | . 21 | 0.017592 | 0.025126 | 0.978901 | 00:00 | . 22 | 0.017310 | 0.024705 | 0.978901 | 00:00 | . 23 | 0.017048 | 0.024316 | 0.979882 | 00:00 | . 24 | 0.016803 | 0.023957 | 0.980373 | 00:00 | . 25 | 0.016572 | 0.023623 | 0.980373 | 00:00 | . 26 | 0.016355 | 0.023313 | 0.980864 | 00:00 | . 27 | 0.016150 | 0.023025 | 0.980864 | 00:00 | . 28 | 0.015955 | 0.022756 | 0.981354 | 00:00 | . 29 | 0.015771 | 0.022505 | 0.981354 | 00:00 | . 30 | 0.015595 | 0.022270 | 0.981354 | 00:00 | . 31 | 0.015427 | 0.022051 | 0.981845 | 00:00 | . 32 | 0.015267 | 0.021844 | 0.981845 | 00:00 | . 33 | 0.015114 | 0.021651 | 0.982826 | 00:00 | . 34 | 0.014967 | 0.021469 | 0.982826 | 00:00 | . 35 | 0.014827 | 0.021297 | 0.982826 | 00:00 | . 36 | 0.014692 | 0.021135 | 0.982826 | 00:00 | . 37 | 0.014562 | 0.020982 | 0.982826 | 00:00 | . 38 | 0.014436 | 0.020837 | 0.982826 | 00:00 | . 39 | 0.014315 | 0.020700 | 0.982826 | 00:00 | . plt.plot(L(learn.recorder.values).itemgot(2)); . learn.recorder.values[-1][2] . 0.982826292514801 . dls = ImageDataLoaders.from_folder(path) learn = vision_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.074170 | 0.031738 | 0.994112 | 00:23 | .",
            "url": "https://ericvincent18.github.io/fastaiMLmodel/fastpages/jupyter/2022/09/15/digit-classifier-ipynb.html",
            "relUrl": "/fastpages/jupyter/2022/09/15/digit-classifier-ipynb.html",
            "date": " • Sep 15, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Aspiring SWE/data scientist with a strong background in sports management. . Current tech stack : . Python | SQL | Postgres | Docker | Kubernetes | RabbitMQ | . linkedin: https://www.linkedin.com/in/eric-vincent-706078137/ .",
          "url": "https://ericvincent18.github.io/fastaiMLmodel/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ericvincent18.github.io/fastaiMLmodel/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}